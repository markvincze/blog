{
  "db": [{
    "meta": {
      "exported_on": 1555278940613,
      "version": "2.1.4"
    },
    "data": {
      "tags": [
        {
        "id": "59a19a20bacc574fd0de292e",
        "name": "Getting Started",
        "slug": "getting-started",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2015-04-22 21:32:50",
        "created_by": "1",
        "updated_at": "2015-04-22 21:32:50",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de292f",
        "name": "visual studio",
        "slug": "visual-studio",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2015-04-26 12:11:49",
        "created_by": "1",
        "updated_at": "2015-04-26 12:11:49",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2930",
        "name": "debug",
        "slug": "debug",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2015-04-26 12:11:49",
        "created_by": "1",
        "updated_at": "2015-04-26 12:11:49",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2931",
        "name": "tooling",
        "slug": "tooling",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2015-04-26 12:11:49",
        "created_by": "1",
        "updated_at": "2015-04-26 12:11:49",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2932",
        "name": "visual commander",
        "slug": "visual-commander",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2015-04-26 12:11:49",
        "created_by": "1",
        "updated_at": "2015-04-26 12:11:49",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2933",
        "name": "asp.net",
        "slug": "asp-net",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2015-05-14 11:28:11",
        "created_by": "1",
        "updated_at": "2015-05-14 11:28:11",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2934",
        "name": "mvc",
        "slug": "mvc",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2015-05-14 11:28:11",
        "created_by": "1",
        "updated_at": "2015-05-14 11:28:11",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2935",
        "name": "web api",
        "slug": "web-api",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2015-05-14 11:28:11",
        "created_by": "1",
        "updated_at": "2015-05-14 11:28:11",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2936",
        "name": "glimpse",
        "slug": "glimpse",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2015-05-14 11:28:11",
        "created_by": "1",
        "updated_at": "2015-05-14 11:28:11",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2937",
        "name": "specflow",
        "slug": "specflow",
        "description": "SpecFlow is an implementation of the Gherkin language for the .NET Framework. SpecFlow is to .NET what Cucumber is for the JavaScript ecosystem.",
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": "SpecFlow",
        "meta_description": "SpecFlow is an implementation of the Gherkin language for the .NET Framework. SpecFlow is to .NET what Cucumber is for the JavaScript ecosystem.",
        "created_at": "2015-08-15 17:15:39",
        "created_by": "1",
        "updated_at": "2015-08-15 17:16:15",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2938",
        "name": "c#",
        "slug": "c",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2015-08-15 17:17:10",
        "created_by": "1",
        "updated_at": "2015-08-15 17:17:10",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2939",
        "name": ".net",
        "slug": "net",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2015-08-15 17:17:10",
        "created_by": "1",
        "updated_at": "2015-08-15 17:17:10",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de293a",
        "name": "basics",
        "slug": "basics",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2015-08-15 17:18:56",
        "created_by": "1",
        "updated_at": "2015-08-15 17:18:56",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de293b",
        "name": "computer science",
        "slug": "computer-science",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2015-08-15 17:18:56",
        "created_by": "1",
        "updated_at": "2015-08-15 17:18:56",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de293c",
        "name": "couchbase",
        "slug": "couchbase",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-01-10 15:53:37",
        "created_by": "1",
        "updated_at": "2016-01-10 15:53:37",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de293d",
        "name": ".net-core",
        "slug": "net-core",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-02-14 16:22:37",
        "created_by": "1",
        "updated_at": "2016-02-14 16:22:37",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de293e",
        "name": "asp.net-core",
        "slug": "asp-net-core",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-02-14 16:22:37",
        "created_by": "1",
        "updated_at": "2016-02-14 16:22:37",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de293f",
        "name": "dnx",
        "slug": "dnx",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-02-14 16:22:38",
        "created_by": "1",
        "updated_at": "2016-02-14 16:22:38",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2940",
        "name": "ghost",
        "slug": "ghost-tag",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-04-10 16:47:59",
        "created_by": "1",
        "updated_at": "2016-04-10 16:47:59",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2941",
        "name": "postgre",
        "slug": "postgre",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-04-10 16:47:59",
        "created_by": "1",
        "updated_at": "2016-04-10 16:47:59",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2942",
        "name": "sqlite",
        "slug": "sqlite",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-04-10 16:47:59",
        "created_by": "1",
        "updated_at": "2016-04-10 16:47:59",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2943",
        "name": "openshift",
        "slug": "openshift",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-04-10 16:47:59",
        "created_by": "1",
        "updated_at": "2016-04-10 16:47:59",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2944",
        "name": "testing",
        "slug": "testing",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-06-11 23:10:54",
        "created_by": "1",
        "updated_at": "2016-06-11 23:10:54",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2945",
        "name": "integration-testing",
        "slug": "integration-testing",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-06-11 23:10:54",
        "created_by": "1",
        "updated_at": "2016-06-11 23:10:54",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2946",
        "name": "golang",
        "slug": "golang",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-10-30 14:37:51",
        "created_by": "1",
        "updated_at": "2016-10-30 14:37:51",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2947",
        "name": "websocket",
        "slug": "websocket",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-10-30 14:37:51",
        "created_by": "1",
        "updated_at": "2016-10-30 14:37:51",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2948",
        "name": "livereload",
        "slug": "livereload",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-10-30 14:37:52",
        "created_by": "1",
        "updated_at": "2016-10-30 14:37:52",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2949",
        "name": "travis",
        "slug": "travis",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-10-30 14:55:55",
        "created_by": "1",
        "updated_at": "2016-10-30 14:55:55",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de294a",
        "name": "appveyor",
        "slug": "appveyor",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-10-30 14:56:10",
        "created_by": "1",
        "updated_at": "2016-10-30 14:56:10",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de294b",
        "name": "github",
        "slug": "github",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-10-30 14:56:35",
        "created_by": "1",
        "updated_at": "2016-10-30 14:56:35",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de294c",
        "name": "bash",
        "slug": "bash",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-10-30 14:56:35",
        "created_by": "1",
        "updated_at": "2016-10-30 14:56:35",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de294d",
        "name": "powershell",
        "slug": "powershell",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-10-30 14:56:35",
        "created_by": "1",
        "updated_at": "2016-10-30 14:56:35",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de294e",
        "name": "ndc",
        "slug": "ndc",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-10-30 14:56:55",
        "created_by": "1",
        "updated_at": "2016-10-30 14:56:55",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de294f",
        "name": "conference",
        "slug": "conference",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-10-30 14:56:55",
        "created_by": "1",
        "updated_at": "2016-10-30 14:56:55",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2950",
        "name": "opencover",
        "slug": "opencover",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-11-06 00:46:08",
        "created_by": "1",
        "updated_at": "2016-11-06 00:46:08",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2951",
        "name": "coveralls",
        "slug": "coveralls",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-11-06 00:46:08",
        "created_by": "1",
        "updated_at": "2016-11-06 00:46:08",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2952",
        "name": "security",
        "slug": "security",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-12-04 18:36:58",
        "created_by": "1",
        "updated_at": "2016-12-04 18:36:58",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2953",
        "name": "firebase",
        "slug": "firebase",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2016-12-04 18:36:58",
        "created_by": "1",
        "updated_at": "2016-12-04 18:36:58",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2954",
        "name": "f#",
        "slug": "f",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2017-02-05 17:25:23",
        "created_by": "1",
        "updated_at": "2017-02-05 17:25:23",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2955",
        "name": "suave",
        "slug": "suave",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2017-02-05 17:25:23",
        "created_by": "1",
        "updated_at": "2017-02-05 17:25:23",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2956",
        "name": "dependency-injection",
        "slug": "dependency-injection",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2017-04-17 14:31:22",
        "created_by": "1",
        "updated_at": "2017-04-17 14:31:22",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2957",
        "name": "category-theory",
        "slug": "category-theory",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2017-04-19 21:27:12",
        "created_by": "1",
        "updated_at": "2017-04-19 21:27:12",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2958",
        "name": "quartz.net",
        "slug": "quartz-net",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2017-06-21 20:31:50",
        "created_by": "1",
        "updated_at": "2017-06-21 20:31:50",
        "updated_by": "1"
      }, {
        "id": "59a19a20bacc574fd0de2959",
        "name": "kubernetes",
        "slug": "kubernetes",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2017-07-29 18:27:38",
        "created_by": "1",
        "updated_at": "2017-07-29 18:27:38",
        "updated_by": "1"
      }, {
        "id": "5a0e263a19ac3b1e3f6dd28f",
        "name": "docfx",
        "slug": "docfx",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2017-11-16 23:58:50",
        "created_by": "1",
        "updated_at": "2017-11-16 23:58:50",
        "updated_by": "1"
      }, {
        "id": "5a1b3dfb19ac3b1e3f6dd298",
        "name": "visual studio code",
        "slug": "visual-studio-code",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2017-11-26 22:19:39",
        "created_by": "1",
        "updated_at": "2017-11-26 22:19:39",
        "updated_by": "1"
      }, {
        "id": "5a1b3dfb19ac3b1e3f6dd29b",
        "name": "presenting",
        "slug": "presenting",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2017-11-26 22:19:39",
        "created_by": "1",
        "updated_at": "2017-11-26 22:19:39",
        "updated_by": "1"
      }, {
        "id": "5b428aa6e67a0c1020872889",
        "name": "linting",
        "slug": "linting",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2018-07-08 22:05:26",
        "created_by": "1",
        "updated_at": "2018-07-08 22:05:26",
        "updated_by": "1"
      }, {
        "id": "5b5cc007e67a0c1020872891",
        "name": "varnish",
        "slug": "varnish",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2018-07-28 19:12:07",
        "created_by": "1",
        "updated_at": "2018-07-28 19:12:07",
        "updated_by": "1"
      }, {
        "id": "5bb7eeb063bf8a66478d78e1",
        "name": "envoy",
        "slug": "envoy",
        "description": null,
        "feature_image": null,
        "parent_id": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "created_at": "2018-10-05 23:07:28",
        "created_by": "1",
        "updated_at": "2018-10-05 23:07:28",
        "updated_by": "1"
      }],
      "posts_tags": [
        {
        "id": "59a19a21bacc574fd0de2981",
        "post_id": "59a19a20bacc574fd0de295b",
        "tag_id": "59a19a20bacc574fd0de294b",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de2982",
        "post_id": "59a19a20bacc574fd0de295c",
        "tag_id": "59a19a20bacc574fd0de292e",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de2983",
        "post_id": "59a19a20bacc574fd0de295d",
        "tag_id": "59a19a20bacc574fd0de292f",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de2984",
        "post_id": "59a19a20bacc574fd0de295e",
        "tag_id": "59a19a20bacc574fd0de2933",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de2985",
        "post_id": "59a19a20bacc574fd0de295f",
        "tag_id": "59a19a20bacc574fd0de292f",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de2986",
        "post_id": "59a19a20bacc574fd0de2960",
        "tag_id": "59a19a20bacc574fd0de2938",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de2987",
        "post_id": "59a19a20bacc574fd0de2961",
        "tag_id": "59a19a20bacc574fd0de2938",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de2988",
        "post_id": "59a19a20bacc574fd0de2962",
        "tag_id": "59a19a20bacc574fd0de2938",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de2989",
        "post_id": "59a19a20bacc574fd0de2963",
        "tag_id": "59a19a20bacc574fd0de2938",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de298a",
        "post_id": "59a19a20bacc574fd0de2964",
        "tag_id": "59a19a20bacc574fd0de2938",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de298b",
        "post_id": "59a19a20bacc574fd0de2965",
        "tag_id": "59a19a20bacc574fd0de2933",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de298c",
        "post_id": "59a19a20bacc574fd0de2966",
        "tag_id": "59a19a20bacc574fd0de2938",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de298d",
        "post_id": "59a19a20bacc574fd0de2967",
        "tag_id": "59a19a20bacc574fd0de2933",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de298e",
        "post_id": "59a19a20bacc574fd0de2969",
        "tag_id": "59a19a20bacc574fd0de2946",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de298f",
        "post_id": "59a19a20bacc574fd0de296a",
        "tag_id": "59a19a20bacc574fd0de2940",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de2990",
        "post_id": "59a19a20bacc574fd0de296b",
        "tag_id": "59a19a20bacc574fd0de2939",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de2991",
        "post_id": "59a19a20bacc574fd0de296c",
        "tag_id": "59a19a20bacc574fd0de294e",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de2992",
        "post_id": "59a19a20bacc574fd0de296d",
        "tag_id": "59a19a20bacc574fd0de2937",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de2993",
        "post_id": "59a19a20bacc574fd0de296e",
        "tag_id": "59a19a20bacc574fd0de2933",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de2994",
        "post_id": "59a19a20bacc574fd0de296f",
        "tag_id": "59a19a20bacc574fd0de2946",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de2995",
        "post_id": "59a19a20bacc574fd0de2970",
        "tag_id": "59a19a20bacc574fd0de2946",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de2996",
        "post_id": "59a19a20bacc574fd0de2971",
        "tag_id": "59a19a20bacc574fd0de293e",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de2997",
        "post_id": "59a19a20bacc574fd0de2972",
        "tag_id": "59a19a20bacc574fd0de294a",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de2998",
        "post_id": "59a19a20bacc574fd0de2973",
        "tag_id": "59a19a20bacc574fd0de2954",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de2999",
        "post_id": "59a19a20bacc574fd0de2974",
        "tag_id": "59a19a20bacc574fd0de2954",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de299a",
        "post_id": "59a19a20bacc574fd0de2975",
        "tag_id": "59a19a20bacc574fd0de2933",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de299b",
        "post_id": "59a19a20bacc574fd0de2976",
        "tag_id": "59a19a20bacc574fd0de2954",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de299c",
        "post_id": "59a19a20bacc574fd0de2977",
        "tag_id": "59a19a20bacc574fd0de2938",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de299d",
        "post_id": "59a19a20bacc574fd0de2978",
        "tag_id": "59a19a20bacc574fd0de2954",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de299e",
        "post_id": "59a19a20bacc574fd0de2979",
        "tag_id": "59a19a20bacc574fd0de2938",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de299f",
        "post_id": "59a19a20bacc574fd0de297a",
        "tag_id": "59a19a20bacc574fd0de293e",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de29a0",
        "post_id": "59a19a20bacc574fd0de297b",
        "tag_id": "59a19a20bacc574fd0de293c",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de29a1",
        "post_id": "59a19a20bacc574fd0de297c",
        "tag_id": "59a19a20bacc574fd0de2933",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de29a2",
        "post_id": "59a19a20bacc574fd0de297e",
        "tag_id": "59a19a20bacc574fd0de2938",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de29a3",
        "post_id": "59a19a20bacc574fd0de297f",
        "tag_id": "59a19a20bacc574fd0de2959",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de29a4",
        "post_id": "59a19a20bacc574fd0de2980",
        "tag_id": "59a19a20bacc574fd0de293e",
        "sort_order": 0
      }, {
        "id": "59a19a21bacc574fd0de29a5",
        "post_id": "59a19a20bacc574fd0de295b",
        "tag_id": "59a19a20bacc574fd0de294c",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29a6",
        "post_id": "59a19a20bacc574fd0de295d",
        "tag_id": "59a19a20bacc574fd0de2930",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29a7",
        "post_id": "59a19a20bacc574fd0de295e",
        "tag_id": "59a19a20bacc574fd0de2934",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29a8",
        "post_id": "59a19a20bacc574fd0de295f",
        "tag_id": "59a19a20bacc574fd0de2937",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29a9",
        "post_id": "59a19a20bacc574fd0de2960",
        "tag_id": "59a19a20bacc574fd0de2939",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29aa",
        "post_id": "59a19a20bacc574fd0de2961",
        "tag_id": "59a19a20bacc574fd0de2939",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29ab",
        "post_id": "59a19a20bacc574fd0de2962",
        "tag_id": "59a19a20bacc574fd0de2939",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29ac",
        "post_id": "59a19a20bacc574fd0de2963",
        "tag_id": "59a19a20bacc574fd0de2939",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29ad",
        "post_id": "59a19a20bacc574fd0de2964",
        "tag_id": "59a19a20bacc574fd0de2939",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29ae",
        "post_id": "59a19a20bacc574fd0de2965",
        "tag_id": "59a19a20bacc574fd0de2938",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29af",
        "post_id": "59a19a20bacc574fd0de2966",
        "tag_id": "59a19a20bacc574fd0de2939",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29b0",
        "post_id": "59a19a20bacc574fd0de2967",
        "tag_id": "59a19a20bacc574fd0de2938",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29b1",
        "post_id": "59a19a20bacc574fd0de2969",
        "tag_id": "59a19a20bacc574fd0de294a",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29b2",
        "post_id": "59a19a20bacc574fd0de296a",
        "tag_id": "59a19a20bacc574fd0de2941",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29b3",
        "post_id": "59a19a20bacc574fd0de296b",
        "tag_id": "59a19a20bacc574fd0de293d",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29b4",
        "post_id": "59a19a20bacc574fd0de296c",
        "tag_id": "59a19a20bacc574fd0de294f",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29b5",
        "post_id": "59a19a20bacc574fd0de296e",
        "tag_id": "59a19a20bacc574fd0de2934",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29b6",
        "post_id": "59a19a20bacc574fd0de296f",
        "tag_id": "59a19a20bacc574fd0de2949",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29b7",
        "post_id": "59a19a20bacc574fd0de2970",
        "tag_id": "59a19a20bacc574fd0de2947",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29b8",
        "post_id": "59a19a20bacc574fd0de2971",
        "tag_id": "59a19a20bacc574fd0de2952",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29b9",
        "post_id": "59a19a20bacc574fd0de2972",
        "tag_id": "59a19a20bacc574fd0de2944",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29ba",
        "post_id": "59a19a20bacc574fd0de2973",
        "tag_id": "59a19a20bacc574fd0de293e",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29bb",
        "post_id": "59a19a20bacc574fd0de2974",
        "tag_id": "59a19a20bacc574fd0de2955",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29bc",
        "post_id": "59a19a20bacc574fd0de2975",
        "tag_id": "59a19a20bacc574fd0de2954",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29bd",
        "post_id": "59a19a20bacc574fd0de2976",
        "tag_id": "59a19a20bacc574fd0de293b",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29be",
        "post_id": "59a19a20bacc574fd0de2977",
        "tag_id": "59a19a20bacc574fd0de2954",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29bf",
        "post_id": "59a19a20bacc574fd0de2978",
        "tag_id": "59a19a20bacc574fd0de2955",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29c0",
        "post_id": "59a19a20bacc574fd0de2979",
        "tag_id": "59a19a20bacc574fd0de2954",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29c1",
        "post_id": "59a19a20bacc574fd0de297a",
        "tag_id": "59a19a20bacc574fd0de2938",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29c2",
        "post_id": "59a19a20bacc574fd0de297c",
        "tag_id": "59a19a20bacc574fd0de2954",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29c3",
        "post_id": "59a19a20bacc574fd0de297e",
        "tag_id": "59a19a20bacc574fd0de293e",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29c4",
        "post_id": "59a19a20bacc574fd0de297f",
        "tag_id": "59a19a20bacc574fd0de293d",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29c5",
        "post_id": "59a19a20bacc574fd0de2980",
        "tag_id": "59a19a20bacc574fd0de2935",
        "sort_order": 1
      }, {
        "id": "59a19a21bacc574fd0de29c6",
        "post_id": "59a19a20bacc574fd0de295b",
        "tag_id": "59a19a20bacc574fd0de294d",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29c7",
        "post_id": "59a19a20bacc574fd0de295d",
        "tag_id": "59a19a20bacc574fd0de2931",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29c8",
        "post_id": "59a19a20bacc574fd0de295e",
        "tag_id": "59a19a20bacc574fd0de2935",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29c9",
        "post_id": "59a19a20bacc574fd0de295f",
        "tag_id": "59a19a20bacc574fd0de2938",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29ca",
        "post_id": "59a19a20bacc574fd0de2960",
        "tag_id": "59a19a20bacc574fd0de293a",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29cb",
        "post_id": "59a19a20bacc574fd0de2961",
        "tag_id": "59a19a20bacc574fd0de293a",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29cc",
        "post_id": "59a19a20bacc574fd0de2962",
        "tag_id": "59a19a20bacc574fd0de293a",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29cd",
        "post_id": "59a19a20bacc574fd0de2963",
        "tag_id": "59a19a20bacc574fd0de293a",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29ce",
        "post_id": "59a19a20bacc574fd0de2964",
        "tag_id": "59a19a20bacc574fd0de293c",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29cf",
        "post_id": "59a19a20bacc574fd0de2965",
        "tag_id": "59a19a20bacc574fd0de2939",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29d0",
        "post_id": "59a19a20bacc574fd0de2966",
        "tag_id": "59a19a20bacc574fd0de293c",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29d1",
        "post_id": "59a19a20bacc574fd0de2967",
        "tag_id": "59a19a20bacc574fd0de293d",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29d2",
        "post_id": "59a19a20bacc574fd0de296a",
        "tag_id": "59a19a20bacc574fd0de2942",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29d3",
        "post_id": "59a19a20bacc574fd0de296b",
        "tag_id": "59a19a20bacc574fd0de2933",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29d4",
        "post_id": "59a19a20bacc574fd0de2970",
        "tag_id": "59a19a20bacc574fd0de2948",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29d5",
        "post_id": "59a19a20bacc574fd0de2971",
        "tag_id": "59a19a20bacc574fd0de2953",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29d6",
        "post_id": "59a19a20bacc574fd0de2972",
        "tag_id": "59a19a20bacc574fd0de2950",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29d7",
        "post_id": "59a19a20bacc574fd0de2974",
        "tag_id": "59a19a20bacc574fd0de2939",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29d8",
        "post_id": "59a19a20bacc574fd0de2975",
        "tag_id": "59a19a20bacc574fd0de2935",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29d9",
        "post_id": "59a19a20bacc574fd0de2976",
        "tag_id": "59a19a20bacc574fd0de2938",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29da",
        "post_id": "59a19a20bacc574fd0de2977",
        "tag_id": "59a19a20bacc574fd0de2957",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29db",
        "post_id": "59a19a20bacc574fd0de2978",
        "tag_id": "59a19a20bacc574fd0de293d",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29dc",
        "post_id": "59a19a20bacc574fd0de2979",
        "tag_id": "59a19a20bacc574fd0de293b",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29dd",
        "post_id": "59a19a20bacc574fd0de297a",
        "tag_id": "59a19a20bacc574fd0de2956",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29de",
        "post_id": "59a19a20bacc574fd0de297c",
        "tag_id": "59a19a20bacc574fd0de2955",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29df",
        "post_id": "59a19a20bacc574fd0de297e",
        "tag_id": "59a19a20bacc574fd0de2945",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29e0",
        "post_id": "59a19a20bacc574fd0de297f",
        "tag_id": "59a19a20bacc574fd0de293e",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29e1",
        "post_id": "59a19a20bacc574fd0de2980",
        "tag_id": "59a19a20bacc574fd0de2959",
        "sort_order": 2
      }, {
        "id": "59a19a21bacc574fd0de29e2",
        "post_id": "59a19a20bacc574fd0de295d",
        "tag_id": "59a19a20bacc574fd0de2932",
        "sort_order": 3
      }, {
        "id": "59a19a21bacc574fd0de29e3",
        "post_id": "59a19a20bacc574fd0de295e",
        "tag_id": "59a19a20bacc574fd0de2936",
        "sort_order": 3
      }, {
        "id": "59a19a21bacc574fd0de29e4",
        "post_id": "59a19a20bacc574fd0de295f",
        "tag_id": "59a19a20bacc574fd0de2939",
        "sort_order": 3
      }, {
        "id": "59a19a21bacc574fd0de29e5",
        "post_id": "59a19a20bacc574fd0de2960",
        "tag_id": "59a19a20bacc574fd0de293b",
        "sort_order": 3
      }, {
        "id": "59a19a21bacc574fd0de29e6",
        "post_id": "59a19a20bacc574fd0de2961",
        "tag_id": "59a19a20bacc574fd0de293b",
        "sort_order": 3
      }, {
        "id": "59a19a21bacc574fd0de29e7",
        "post_id": "59a19a20bacc574fd0de2962",
        "tag_id": "59a19a20bacc574fd0de293b",
        "sort_order": 3
      }, {
        "id": "59a19a21bacc574fd0de29e8",
        "post_id": "59a19a20bacc574fd0de2963",
        "tag_id": "59a19a20bacc574fd0de293b",
        "sort_order": 3
      }, {
        "id": "59a19a21bacc574fd0de29e9",
        "post_id": "59a19a20bacc574fd0de2965",
        "tag_id": "59a19a20bacc574fd0de293d",
        "sort_order": 3
      }, {
        "id": "59a19a21bacc574fd0de29ea",
        "post_id": "59a19a20bacc574fd0de2967",
        "tag_id": "59a19a20bacc574fd0de293e",
        "sort_order": 3
      }, {
        "id": "59a19a21bacc574fd0de29eb",
        "post_id": "59a19a20bacc574fd0de296a",
        "tag_id": "59a19a20bacc574fd0de2943",
        "sort_order": 3
      }, {
        "id": "59a19a21bacc574fd0de29ec",
        "post_id": "59a19a20bacc574fd0de296b",
        "tag_id": "59a19a20bacc574fd0de293e",
        "sort_order": 3
      }, {
        "id": "59a19a21bacc574fd0de29ed",
        "post_id": "59a19a20bacc574fd0de2972",
        "tag_id": "59a19a20bacc574fd0de2951",
        "sort_order": 3
      }, {
        "id": "59a19a21bacc574fd0de29ee",
        "post_id": "59a19a20bacc574fd0de297e",
        "tag_id": "59a19a20bacc574fd0de2944",
        "sort_order": 3
      }, {
        "id": "59a19a21bacc574fd0de29ef",
        "post_id": "59a19a20bacc574fd0de2965",
        "tag_id": "59a19a20bacc574fd0de293e",
        "sort_order": 4
      }, {
        "id": "59a19a21bacc574fd0de29f0",
        "post_id": "59a19a20bacc574fd0de296b",
        "tag_id": "59a19a20bacc574fd0de2944",
        "sort_order": 4
      }, {
        "id": "59a19a21bacc574fd0de29f1",
        "post_id": "59a19a20bacc574fd0de2972",
        "tag_id": "59a19a20bacc574fd0de293d",
        "sort_order": 4
      }, {
        "id": "59a19a21bacc574fd0de29f2",
        "post_id": "59a19a20bacc574fd0de297e",
        "tag_id": "59a19a20bacc574fd0de293c",
        "sort_order": 4
      }, {
        "id": "59a19a21bacc574fd0de29f3",
        "post_id": "59a19a20bacc574fd0de2965",
        "tag_id": "59a19a20bacc574fd0de293f",
        "sort_order": 5
      }, {
        "id": "59a19a21bacc574fd0de29f4",
        "post_id": "59a19a20bacc574fd0de296b",
        "tag_id": "59a19a20bacc574fd0de2945",
        "sort_order": 5
      }, {
        "id": "59a19a21bacc574fd0de29f5",
        "post_id": "59a19a20bacc574fd0de297e",
        "tag_id": "59a19a20bacc574fd0de2958",
        "sort_order": 5
      }, {
        "id": "5a0e263a19ac3b1e3f6dd290",
        "post_id": "5a0e206819ac3b1e3f6dd28e",
        "tag_id": "5a0e263a19ac3b1e3f6dd28f",
        "sort_order": 2
      }, {
        "id": "5a0e263a19ac3b1e3f6dd291",
        "post_id": "5a0e206819ac3b1e3f6dd28e",
        "tag_id": "59a19a20bacc574fd0de293d",
        "sort_order": 0
      }, {
        "id": "5a0e263a19ac3b1e3f6dd292",
        "post_id": "5a0e206819ac3b1e3f6dd28e",
        "tag_id": "59a19a20bacc574fd0de294a",
        "sort_order": 1
      }, {
        "id": "5a1b3dfb19ac3b1e3f6dd299",
        "post_id": "5a1b336219ac3b1e3f6dd297",
        "tag_id": "5a1b3dfb19ac3b1e3f6dd298",
        "sort_order": 1
      }, {
        "id": "5a1b3dfb19ac3b1e3f6dd29a",
        "post_id": "5a1b336219ac3b1e3f6dd297",
        "tag_id": "59a19a20bacc574fd0de292f",
        "sort_order": 0
      }, {
        "id": "5a1b3dfb19ac3b1e3f6dd29c",
        "post_id": "5a1b336219ac3b1e3f6dd297",
        "tag_id": "5a1b3dfb19ac3b1e3f6dd29b",
        "sort_order": 2
      }, {
        "id": "5b428aa6e67a0c102087288a",
        "post_id": "5b422ffe19ac3b1e3f6dd2b1",
        "tag_id": "59a19a20bacc574fd0de293d",
        "sort_order": 0
      }, {
        "id": "5b428aa6e67a0c102087288b",
        "post_id": "5b422ffe19ac3b1e3f6dd2b1",
        "tag_id": "59a19a20bacc574fd0de2938",
        "sort_order": 1
      }, {
        "id": "5b428aa6e67a0c102087288c",
        "post_id": "5b422ffe19ac3b1e3f6dd2b1",
        "tag_id": "5b428aa6e67a0c1020872889",
        "sort_order": 2
      }, {
        "id": "5b5cc007e67a0c1020872892",
        "post_id": "5b5c8545e67a0c102087288f",
        "tag_id": "5b5cc007e67a0c1020872891",
        "sort_order": 0
      }, {
        "id": "5bb7eeb063bf8a66478d78e2",
        "post_id": "5bb0a47f63bf8a66478d78d6",
        "tag_id": "5bb7eeb063bf8a66478d78e1",
        "sort_order": 0
      }, {
        "id": "5bb7eeb063bf8a66478d78e3",
        "post_id": "5bb0a47f63bf8a66478d78d6",
        "tag_id": "59a19a20bacc574fd0de2959",
        "sort_order": 1
      }, {
        "id": "5c3232f7a26b5f5ce4856be2",
        "post_id": "5c32318aa26b5f5ce4856be0",
        "tag_id": "59a19a20bacc574fd0de2959",
        "sort_order": 0
      }, {
        "id": "5c3232f7a26b5f5ce4856be3",
        "post_id": "5c32318aa26b5f5ce4856be0",
        "tag_id": "59a19a20bacc574fd0de293e",
        "sort_order": 1
      }, {
        "id": "5cafa70fa26b5f5ce4856bf6",
        "post_id": "5cafa655a26b5f5ce4856bf4",
        "tag_id": "5bb7eeb063bf8a66478d78e1",
        "sort_order": 0
      }],
      "apps": [],
      "app_settings": [],
      "app_fields": [],
      "subscribers": [],
      "invites": [],
      "brute": [{
        "key": "iPRjmLQThqKesNmGzX1WQEz7es5+rtCAv0M+/q2OY14=",
        "firstRequest": 1555278881464,
        "lastRequest": 1555278881464,
        "lifetime": 1555282481471,
        "count": 1
      }],
      "migrations_lock": [{
        "lock_key": "km01",
        "locked": 0,
        "acquired_at": "2018-09-30 10:16:48",
        "released_at": "2018-09-30 10:16:51"
      }],
      "webhooks": [],
      "posts": [{
        "id": "59a19a20bacc574fd0de295b",
        "uuid": "92cf6557-5c07-498f-854d-c052b3df8eac",
        "title": "Download artifacts from a latest GitHub release with bash and PowerShell",
        "slug": "download-artifacts-from-a-latest-github-release-in-sh-and-powershell",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"[Releases](https://help.github.com/articles/about-releases/) is an important feature of GitHub, with which we can publish packaged versions of our project.\\n\\nThe source code of our repository is packaged with every release, and we also have the possibility to upload some artifacts alongside, for example the binaries or executables that we've built.\\n\\nLately I've been working on an [application](https://github.com/Travix-International/Travix.Core.Adk) for which the releases are published on GitHub, and I wanted to create an install script which always downloads the latest release.\\n\\nIt turned out that downloading the artifacts for a specific version is easy, we can simply build an URL with the version number to access the artifact. However, there is no direct URL to download artifacts from the *latest* release, so we need a bit of logic in our scripts to do that.\\n\\n## Link structure\\n\\nWe can access a particular release with a URL like `https://github.com/account/project/releases/tag/project-1.0.5`. (The part after `tag/` is the what we specified when we created the release.)\\n\\nArtifacts from this particular release can be downloaded with the URL `https://github.com/account/project/releases/download/project-1.0.5/myArtifact.zip`.\\n\\nAnd there is this URL, which which always takes us to the latest release of a project: `https://github.com/account/project/releases/latest`.  \\nIt seemed logical that there should also be a way to also download artifacts from the latest release, so I [asked around](http://stackoverflow.com/questions/38283074/is-there-a-way-to-download-an-artifact-from-the-latest-release-on-github), but it turns out, there is no URL available to do that directly.\\n\\n## Getting the latest tag\\n\\nAs it's pointed out in the above SO answer, we can send a GET request to the URL `https://github.com/account/project/releases/latest` and set the `Accept` header to `application/json` (without this we get back the HTML page), we get information about the latest release in the following format.\\n\\n```json\\n{\\\"id\\\":3622206,\\\"tag_name\\\":\\\"project-1.0.5\\\"}\\n```\\n\\nThis is the information we want, now we can use this in our scripts to determine the latest version and build our download links.\\n\\n## Use the version in scripts\\n\\n### Shell script\\n\\nWe can get the information about the latest release with `curl`.\\n\\n```bash\\nLATEST_RELEASE=$(curl -L -s -H 'Accept: application/json' https://github.com/account/project/releases/latest)\\n```\\n\\nThen we have to extract the value of the `tag_name` property, I used a regex for this with `sed`.\\n\\n```bash\\n# The releases are returned in the format {\\\"id\\\":3622206,\\\"tag_name\\\":\\\"hello-1.0.0.11\\\",...}, we have to extract the tag_name.\\nLATEST_VERSION=$(echo $LATEST_RELEASE | sed -e 's/.*\\\"tag_name\\\":\\\"\\\\([^\\\"]*\\\\)\\\".*/\\\\1/')\\n```\\n\\nThen we can build our download URL for a certain artifact.\\n\\n```bash\\nARTIFACT_URL=\\\"https://github.com/account/project/releases/download/$LATEST_VERSION/myArtifact.zip\\\"\\n```\\n\\n### PowerShell\\n\\nThe latest release can be retrieved with `Invoke-WebRequest`.\\n\\n```powershell\\n$latestRelease = Invoke-WebRequest https://github.com/account/project/releases/latest -Headers @{\\\"Accept\\\"=\\\"application/json\\\"}\\n```\\n\\nThen we have to extract the value of the `tag_name` property. Since PowerShell has built-in support for parsing Json, we don't have to use a regex.\\n\\n```powershell\\n# The releases are returned in the format {\\\"id\\\":3622206,\\\"tag_name\\\":\\\"hello-1.0.0.11\\\",...}, we have to extract the tag_name.\\n$json = $latestRelease.Content | ConvertFrom-Json\\n$latestVersion = $json.tag_name\\n```\\n\\nThen we can build our download URL for a certain artifact.\\n\\n```powershell\\n$url = \\\"https://github.com/account/project/releases/download/$latestVersion/myArtifact.zip\\\"\\n```\"}]],\"sections\":[[10,0]]}",
        "html": "<p><a href=\"https://help.github.com/articles/about-releases/\">Releases</a> is an important feature of GitHub, with which we can publish packaged versions of our project.</p>\n<p>The source code of our repository is packaged with every release, and we also have the possibility to upload some artifacts alongside, for example the binaries or executables that we've built.</p>\n<p>Lately I've been working on an <a href=\"https://github.com/Travix-International/Travix.Core.Adk\">application</a> for which the releases are published on GitHub, and I wanted to create an install script which always downloads the latest release.</p>\n<p>It turned out that downloading the artifacts for a specific version is easy, we can simply build an URL with the version number to access the artifact. However, there is no direct URL to download artifacts from the <em>latest</em> release, so we need a bit of logic in our scripts to do that.</p>\n<h2 id=\"linkstructure\">Link structure</h2>\n<p>We can access a particular release with a URL like <code>https://github.com/account/project/releases/tag/project-1.0.5</code>. (The part after <code>tag/</code> is the what we specified when we created the release.)</p>\n<p>Artifacts from this particular release can be downloaded with the URL <code>https://github.com/account/project/releases/download/project-1.0.5/myArtifact.zip</code>.</p>\n<p>And there is this URL, which which always takes us to the latest release of a project: <code>https://github.com/account/project/releases/latest</code>.<br>\nIt seemed logical that there should also be a way to also download artifacts from the latest release, so I <a href=\"http://stackoverflow.com/questions/38283074/is-there-a-way-to-download-an-artifact-from-the-latest-release-on-github\">asked around</a>, but it turns out, there is no URL available to do that directly.</p>\n<h2 id=\"gettingthelatesttag\">Getting the latest tag</h2>\n<p>As it's pointed out in the above SO answer, we can send a GET request to the URL <code>https://github.com/account/project/releases/latest</code> and set the <code>Accept</code> header to <code>application/json</code> (without this we get back the HTML page), we get information about the latest release in the following format.</p>\n<pre><code class=\"language-json\">{&quot;id&quot;:3622206,&quot;tag_name&quot;:&quot;project-1.0.5&quot;}\n</code></pre>\n<p>This is the information we want, now we can use this in our scripts to determine the latest version and build our download links.</p>\n<h2 id=\"usetheversioninscripts\">Use the version in scripts</h2>\n<h3 id=\"shellscript\">Shell script</h3>\n<p>We can get the information about the latest release with <code>curl</code>.</p>\n<pre><code class=\"language-bash\">LATEST_RELEASE=$(curl -L -s -H 'Accept: application/json' https://github.com/account/project/releases/latest)\n</code></pre>\n<p>Then we have to extract the value of the <code>tag_name</code> property, I used a regex for this with <code>sed</code>.</p>\n<pre><code class=\"language-bash\"># The releases are returned in the format {&quot;id&quot;:3622206,&quot;tag_name&quot;:&quot;hello-1.0.0.11&quot;,...}, we have to extract the tag_name.\nLATEST_VERSION=$(echo $LATEST_RELEASE | sed -e 's/.*&quot;tag_name&quot;:&quot;\\([^&quot;]*\\)&quot;.*/\\1/')\n</code></pre>\n<p>Then we can build our download URL for a certain artifact.</p>\n<pre><code class=\"language-bash\">ARTIFACT_URL=&quot;https://github.com/account/project/releases/download/$LATEST_VERSION/myArtifact.zip&quot;\n</code></pre>\n<h3 id=\"powershell\">PowerShell</h3>\n<p>The latest release can be retrieved with <code>Invoke-WebRequest</code>.</p>\n<pre><code class=\"language-powershell\">$latestRelease = Invoke-WebRequest https://github.com/account/project/releases/latest -Headers @{&quot;Accept&quot;=&quot;application/json&quot;}\n</code></pre>\n<p>Then we have to extract the value of the <code>tag_name</code> property. Since PowerShell has built-in support for parsing Json, we don't have to use a regex.</p>\n<pre><code class=\"language-powershell\"># The releases are returned in the format {&quot;id&quot;:3622206,&quot;tag_name&quot;:&quot;hello-1.0.0.11&quot;,...}, we have to extract the tag_name.\n$json = $latestRelease.Content | ConvertFrom-Json\n$latestVersion = $json.tag_name\n</code></pre>\n<p>Then we can build our download URL for a certain artifact.</p>\n<pre><code class=\"language-powershell\">$url = &quot;https://github.com/account/project/releases/download/$latestVersion/myArtifact.zip&quot;\n</code></pre>\n",
        "comment_id": "23",
        "plaintext": "Releases [https://help.github.com/articles/about-releases/]  is an important\nfeature of GitHub, with which we can publish packaged versions of our project.\n\nThe source code of our repository is packaged with every release, and we also\nhave the possibility to upload some artifacts alongside, for example the\nbinaries or executables that we've built.\n\nLately I've been working on an application\n[https://github.com/Travix-International/Travix.Core.Adk]  for which the\nreleases are published on GitHub, and I wanted to create an install script which\nalways downloads the latest release.\n\nIt turned out that downloading the artifacts for a specific version is easy, we\ncan simply build an URL with the version number to access the artifact. However,\nthere is no direct URL to download artifacts from the latest  release, so we\nneed a bit of logic in our scripts to do that.\n\nLink structure\nWe can access a particular release with a URL like \nhttps://github.com/account/project/releases/tag/project-1.0.5. (The part after \ntag/  is the what we specified when we created the release.)\n\nArtifacts from this particular release can be downloaded with the URL \nhttps://github.com/account/project/releases/download/project-1.0.5/myArtifact.zip\n.\n\nAnd there is this URL, which which always takes us to the latest release of a\nproject: https://github.com/account/project/releases/latest.\nIt seemed logical that there should also be a way to also download artifacts\nfrom the latest release, so I asked around\n[http://stackoverflow.com/questions/38283074/is-there-a-way-to-download-an-artifact-from-the-latest-release-on-github]\n, but it turns out, there is no URL available to do that directly.\n\nGetting the latest tag\nAs it's pointed out in the above SO answer, we can send a GET request to the URL\n https://github.com/account/project/releases/latest  and set the Accept  header\nto application/json  (without this we get back the HTML page), we get\ninformation about the latest release in the following format.\n\n{\"id\":3622206,\"tag_name\":\"project-1.0.5\"}\n\n\nThis is the information we want, now we can use this in our scripts to determine\nthe latest version and build our download links.\n\nUse the version in scripts\nShell script\nWe can get the information about the latest release with curl.\n\nLATEST_RELEASE=$(curl -L -s -H 'Accept: application/json' https://github.com/account/project/releases/latest)\n\n\nThen we have to extract the value of the tag_name  property, I used a regex for\nthis with sed.\n\n# The releases are returned in the format {\"id\":3622206,\"tag_name\":\"hello-1.0.0.11\",...}, we have to extract the tag_name.\nLATEST_VERSION=$(echo $LATEST_RELEASE | sed -e 's/.*\"tag_name\":\"\\([^\"]*\\)\".*/\\1/')\n\n\nThen we can build our download URL for a certain artifact.\n\nARTIFACT_URL=\"https://github.com/account/project/releases/download/$LATEST_VERSION/myArtifact.zip\"\n\n\nPowerShell\nThe latest release can be retrieved with Invoke-WebRequest.\n\n$latestRelease = Invoke-WebRequest https://github.com/account/project/releases/latest -Headers @{\"Accept\"=\"application/json\"}\n\n\nThen we have to extract the value of the tag_name  property. Since PowerShell\nhas built-in support for parsing Json, we don't have to use a regex.\n\n# The releases are returned in the format {\"id\":3622206,\"tag_name\":\"hello-1.0.0.11\",...}, we have to extract the tag_name.\n$json = $latestRelease.Content | ConvertFrom-Json\n$latestVersion = $json.tag_name\n\n\nThen we can build our download URL for a certain artifact.\n\n$url = \"https://github.com/account/project/releases/download/$latestVersion/myArtifact.zip\"",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "Downloading artifacts for a particular GitHub release is easy, but to download artifacts from the latest release we need some extra steps in our scripts.",
        "author_id": "1",
        "created_at": "2016-07-09 21:41:18",
        "created_by": "1",
        "updated_at": "2017-02-26 20:38:04",
        "updated_by": "1",
        "published_at": "2016-07-09 22:45:38",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de295d",
        "uuid": "9257a3a5-a17d-4a18-af48-2972656c4aa7",
        "title": "Attach to specific Process shortcut in Visual Studio",
        "slug": "attach-to-process-shortcut-in-visual-studio",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"It's a very useful feature of Visual Studio that besides starting an application for debugging, we can attach the debugger to already running processes as well.\\r\\nThis can be done with the **Debug->Attach to Process...** option, where we have to select the desired one from a list of all running processes.\\r\\n![Attach to Process window in Visual Studio](/content/images/2015/04/attach_window.png)\\r\\nThis method of attaching to a process is OK if you have to do it only once in a while, but if you have to debug applications this way regularly, it becomes time-consuming to search for the process in the list every time.\\r\\n\\r\\nThe principle of automation has been written in many forms, this is one of the renditions:\\r\\n>Anything that you do more than twice has to be automated.\\r\\n\\r\\nThis applies not just to integration, deployment, testing, etc., but  to tooling as well.\\r\\nSo I started looking for a more convenient way to attach to a specific process quickly.\\r\\n#Visual Studio Macros\\r\\nThe first approach I found is to write a Visual Studio macro that looks for a specific process and attaches VS to it. This approach is described in this [Stack Overflow answer](http://stackoverflow.com/a/6696813/974733), it seemed very straightforward and promising.  \\r\\nHowever, the first comment on the answer destroys our happiness:\\r\\n>\\\"Macros are no longer available in Visual Studio 2012.\\\"\\r\\n\\r\\nOk, so this is a dead end. Or is it?\\r\\n#Visual Commander to the rescue!\\r\\nI really like it when a popular feature of a product is removed, but the community brings it back. The same thing happened to VS macros with [Visual Commander](https://vlasovstudio.com/visual-commander/), a freemium VS extension that lets us reuse existing VS macros, and create commands and extensions using C# or VB.NET.  \\r\\nAfter installing the extension, you can open the Commands windows with the VCMD->Commands option.\\r\\n![Commands option in the VCMD menu](/content/images/2015/04/VCMD_menu.png)\\r\\nIn the Commands window we can manage our existing commands, and create a new one with the Add button.\\r\\n![Commands window for managing all our existing commands](/content/images/2015/04/vc_commands.png)\\r\\nIn the command editor window can pick a name, select the language we'd like to use (C# or VB), and implement the code which will run when we execute the command.\\r\\n![Command editor window of VCMD](/content/images/2015/04/new_command.png)\\r\\n**One gotcha**: pick the desired language before starting to type the code, because if you switch to another language, all your edits are erased.  \\r\\nThe editor window is a bit plain, it does not support IntelliSense or automatic code formatting, so you'll might be better off writing the code in a normal Visual Studio window and insert it into the VCMD command afterwards.\\r\\n\\r\\nThe code for attaching to a process with a specific name is quite simple, as described in the above SO-answer. This is the same code translated to C#.\\r\\n\\r\\n```csharp\\r\\nusing EnvDTE;\\r\\nusing EnvDTE80;\\r\\nusing System.Management;\\r\\nusing System;\\r\\n\\r\\npublic class C : VisualCommanderExt.ICommand\\r\\n{\\r\\n    public void Run(EnvDTE80.DTE2 DTE, Microsoft.VisualStudio.Shell.Package package) \\r\\n    {\\r\\n        foreach(Process proc in DTE.Debugger.LocalProcesses)\\r\\n        {\\r\\n            if(proc.Name.ToString().EndsWith(\\\"MyApp.exe\\\"))\\r\\n            {\\r\\n                proc.Attach();\\r\\n                return;\\r\\n            }\\r\\n        }\\r\\n\\r\\n        System.Windows.MessageBox.Show(\\\"Process running the MyApp was not found.\\\");\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nThe above code tries to attach to a process that has a name ending with \\\"MyApp.exe\\\". The command in this form is already ready to use. Existing commands can be found with their specified name in the VCMD menu.\\r\\n![Run existing VCMD command](/content/images/2015/04/attachToMyApp.png)\\r\\n\\r\\n#Attach to a process with a specific owner user\\r\\n\\r\\nSometimes we can't identify the process we are looking for by it's name, but we have to use the name of the user running it.  \\r\\nA typical scenario for this is when we have several web applications running in IIS. In this case all the processes have the same name, **w3wp.exe**, they only differ in the name of the user running them. (That is, if we use different application pools for each of the applications.)\\r\\n![IIS processes with the same name, but different user](/content/images/2015/04/attach_iis.png)\\r\\n##Finding out the process owner's user name\\r\\nUnfortunately, the `EnvDTE.Process` [interface](https://msdn.microsoft.com/en-us/library/envdte.process.aspx) does not contain the user name, so we have to do a little additional coding to find it.  \\r\\nOne solution is to use WMI, which can be used in .NET with the System.Management namespace, and implement a little helper method.\\r\\n\\r\\n```csharp\\r\\nprivate string GetProcessOwner(int processId)\\r\\n{\\r\\n    string query = \\\"Select * From Win32_Process Where ProcessID = \\\" + processId;\\r\\n    ManagementObjectSearcher searcher = new ManagementObjectSearcher(query);\\r\\n    ManagementObjectCollection processList = searcher.Get();\\r\\n\\r\\n    foreach (ManagementObject obj in processList)\\r\\n    {\\r\\n        string[] argList = new string[] { string.Empty, string.Empty };\\r\\n        int returnVal = Convert.ToInt32(obj.InvokeMethod(\\\"GetOwner\\\", argList));\\r\\n        if (returnVal == 0)\\r\\n        {\\r\\n            // Return DOMAIN\\\\user.\\r\\n            return argList[1] + \\\"\\\\\\\\\\\" + argList[0];\\r\\n        }\\r\\n    }\\r\\n\\r\\n    return \\\"NO OWNER\\\";\\r\\n}\\r\\n```\\r\\n\\r\\nWith this, we can implement the complete VS command attaching for a specific IIS application.\\r\\n\\r\\n```csharp\\r\\nusing EnvDTE;\\r\\nusing EnvDTE80;\\r\\nusing System.Management;\\r\\nusing System;\\r\\n\\r\\npublic class C : VisualCommanderExt.ICommand\\r\\n{\\r\\n    public void Run(EnvDTE80.DTE2 DTE, Microsoft.VisualStudio.Shell.Package package) \\r\\n    {\\r\\n        foreach(Process proc in DTE.Debugger.LocalProcesses)\\r\\n        {\\r\\n            if(proc.Name.ToString().EndsWith(\\\"w3wp.exe\\\") && GetProcessOwner(proc.ProcessID) == \\\"IIS APPPOOL\\\\\\\\MySite\\\")\\r\\n            {\\r\\n                proc.Attach();\\r\\n                return;\\r\\n            }\\r\\n        }\\r\\n\\r\\n        System.Windows.MessageBox.Show(\\\"Process runing MySite was not found.\\\");\\r\\n    }\\r\\n\\r\\n    private string GetProcessOwner(int processId)\\r\\n    {\\r\\n        string query = \\\"Select * From Win32_Process Where ProcessID = \\\" + processId;\\r\\n        ManagementObjectSearcher searcher = new ManagementObjectSearcher(query);\\r\\n        ManagementObjectCollection processList = searcher.Get();\\r\\n\\r\\n        foreach (ManagementObject obj in processList)\\r\\n        {\\r\\n            string[] argList = new string[] { string.Empty, string.Empty };\\r\\n            int returnVal = Convert.ToInt32(obj.InvokeMethod(\\\"GetOwner\\\", argList));\\r\\n            if (returnVal == 0)\\r\\n            {\\r\\n                // Return DOMAIN\\\\user.\\r\\n                return argList[1] + \\\"\\\\\\\\\\\" + argList[0];\\r\\n            }\\r\\n        }\\r\\n\\r\\n        return \\\"NO OWNER\\\";\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nThe last thing to take care of is making sure the command has a reference to the System.Management assembly. We simply have to add the line `System.Management` in the References window.\\r\\n\\r\\nOne last thing to note is a cool feature of Visual Commander: you can create an **export** of the commands in the VCMD menu, and distribute them in your team, to make everybody debug processes as conveniently as possible!\"}]],\"sections\":[[10,0]]}",
        "html": "<p>It's a very useful feature of Visual Studio that besides starting an application for debugging, we can attach the debugger to already running processes as well.<br>\nThis can be done with the <strong>Debug-&gt;Attach to Process...</strong> option, where we have to select the desired one from a list of all running processes.<br>\n<img src=\"/content/images/2015/04/attach_window.png\" alt=\"Attach to Process window in Visual Studio\"><br>\nThis method of attaching to a process is OK if you have to do it only once in a while, but if you have to debug applications this way regularly, it becomes time-consuming to search for the process in the list every time.</p>\n<p>The principle of automation has been written in many forms, this is one of the renditions:</p>\n<blockquote>\n<p>Anything that you do more than twice has to be automated.</p>\n</blockquote>\n<p>This applies not just to integration, deployment, testing, etc., but  to tooling as well.<br>\nSo I started looking for a more convenient way to attach to a specific process quickly.</p>\n<h1 id=\"visualstudiomacros\">Visual Studio Macros</h1>\n<p>The first approach I found is to write a Visual Studio macro that looks for a specific process and attaches VS to it. This approach is described in this <a href=\"http://stackoverflow.com/a/6696813/974733\">Stack Overflow answer</a>, it seemed very straightforward and promising.<br>\nHowever, the first comment on the answer destroys our happiness:</p>\n<blockquote>\n<p>&quot;Macros are no longer available in Visual Studio 2012.&quot;</p>\n</blockquote>\n<p>Ok, so this is a dead end. Or is it?</p>\n<h1 id=\"visualcommandertotherescue\">Visual Commander to the rescue!</h1>\n<p>I really like it when a popular feature of a product is removed, but the community brings it back. The same thing happened to VS macros with <a href=\"https://vlasovstudio.com/visual-commander/\">Visual Commander</a>, a freemium VS extension that lets us reuse existing VS macros, and create commands and extensions using C# or VB.NET.<br>\nAfter installing the extension, you can open the Commands windows with the VCMD-&gt;Commands option.<br>\n<img src=\"/content/images/2015/04/VCMD_menu.png\" alt=\"Commands option in the VCMD menu\"><br>\nIn the Commands window we can manage our existing commands, and create a new one with the Add button.<br>\n<img src=\"/content/images/2015/04/vc_commands.png\" alt=\"Commands window for managing all our existing commands\"><br>\nIn the command editor window can pick a name, select the language we'd like to use (C# or VB), and implement the code which will run when we execute the command.<br>\n<img src=\"/content/images/2015/04/new_command.png\" alt=\"Command editor window of VCMD\"><br>\n<strong>One gotcha</strong>: pick the desired language before starting to type the code, because if you switch to another language, all your edits are erased.<br>\nThe editor window is a bit plain, it does not support IntelliSense or automatic code formatting, so you'll might be better off writing the code in a normal Visual Studio window and insert it into the VCMD command afterwards.</p>\n<p>The code for attaching to a process with a specific name is quite simple, as described in the above SO-answer. This is the same code translated to C#.</p>\n<pre><code class=\"language-csharp\">using EnvDTE;\nusing EnvDTE80;\nusing System.Management;\nusing System;\n\npublic class C : VisualCommanderExt.ICommand\n{\n    public void Run(EnvDTE80.DTE2 DTE, Microsoft.VisualStudio.Shell.Package package) \n    {\n        foreach(Process proc in DTE.Debugger.LocalProcesses)\n        {\n            if(proc.Name.ToString().EndsWith(&quot;MyApp.exe&quot;))\n            {\n                proc.Attach();\n                return;\n            }\n        }\n\n        System.Windows.MessageBox.Show(&quot;Process running the MyApp was not found.&quot;);\n    }\n}\n</code></pre>\n<p>The above code tries to attach to a process that has a name ending with &quot;MyApp.exe&quot;. The command in this form is already ready to use. Existing commands can be found with their specified name in the VCMD menu.<br>\n<img src=\"/content/images/2015/04/attachToMyApp.png\" alt=\"Run existing VCMD command\"></p>\n<h1 id=\"attachtoaprocesswithaspecificowneruser\">Attach to a process with a specific owner user</h1>\n<p>Sometimes we can't identify the process we are looking for by it's name, but we have to use the name of the user running it.<br>\nA typical scenario for this is when we have several web applications running in IIS. In this case all the processes have the same name, <strong>w3wp.exe</strong>, they only differ in the name of the user running them. (That is, if we use different application pools for each of the applications.)<br>\n<img src=\"/content/images/2015/04/attach_iis.png\" alt=\"IIS processes with the same name, but different user\"></p>\n<h2 id=\"findingouttheprocessownersusername\">Finding out the process owner's user name</h2>\n<p>Unfortunately, the <code>EnvDTE.Process</code> <a href=\"https://msdn.microsoft.com/en-us/library/envdte.process.aspx\">interface</a> does not contain the user name, so we have to do a little additional coding to find it.<br>\nOne solution is to use WMI, which can be used in .NET with the System.Management namespace, and implement a little helper method.</p>\n<pre><code class=\"language-csharp\">private string GetProcessOwner(int processId)\n{\n    string query = &quot;Select * From Win32_Process Where ProcessID = &quot; + processId;\n    ManagementObjectSearcher searcher = new ManagementObjectSearcher(query);\n    ManagementObjectCollection processList = searcher.Get();\n\n    foreach (ManagementObject obj in processList)\n    {\n        string[] argList = new string[] { string.Empty, string.Empty };\n        int returnVal = Convert.ToInt32(obj.InvokeMethod(&quot;GetOwner&quot;, argList));\n        if (returnVal == 0)\n        {\n            // Return DOMAIN\\user.\n            return argList[1] + &quot;\\\\&quot; + argList[0];\n        }\n    }\n\n    return &quot;NO OWNER&quot;;\n}\n</code></pre>\n<p>With this, we can implement the complete VS command attaching for a specific IIS application.</p>\n<pre><code class=\"language-csharp\">using EnvDTE;\nusing EnvDTE80;\nusing System.Management;\nusing System;\n\npublic class C : VisualCommanderExt.ICommand\n{\n    public void Run(EnvDTE80.DTE2 DTE, Microsoft.VisualStudio.Shell.Package package) \n    {\n        foreach(Process proc in DTE.Debugger.LocalProcesses)\n        {\n            if(proc.Name.ToString().EndsWith(&quot;w3wp.exe&quot;) &amp;&amp; GetProcessOwner(proc.ProcessID) == &quot;IIS APPPOOL\\\\MySite&quot;)\n            {\n                proc.Attach();\n                return;\n            }\n        }\n\n        System.Windows.MessageBox.Show(&quot;Process runing MySite was not found.&quot;);\n    }\n\n    private string GetProcessOwner(int processId)\n    {\n        string query = &quot;Select * From Win32_Process Where ProcessID = &quot; + processId;\n        ManagementObjectSearcher searcher = new ManagementObjectSearcher(query);\n        ManagementObjectCollection processList = searcher.Get();\n\n        foreach (ManagementObject obj in processList)\n        {\n            string[] argList = new string[] { string.Empty, string.Empty };\n            int returnVal = Convert.ToInt32(obj.InvokeMethod(&quot;GetOwner&quot;, argList));\n            if (returnVal == 0)\n            {\n                // Return DOMAIN\\user.\n                return argList[1] + &quot;\\\\&quot; + argList[0];\n            }\n        }\n\n        return &quot;NO OWNER&quot;;\n    }\n}\n</code></pre>\n<p>The last thing to take care of is making sure the command has a reference to the System.Management assembly. We simply have to add the line <code>System.Management</code> in the References window.</p>\n<p>One last thing to note is a cool feature of Visual Commander: you can create an <strong>export</strong> of the commands in the VCMD menu, and distribute them in your team, to make everybody debug processes as conveniently as possible!</p>\n",
        "comment_id": "2",
        "plaintext": "It's a very useful feature of Visual Studio that besides starting an application\nfor debugging, we can attach the debugger to already running processes as well.\nThis can be done with the Debug->Attach to Process...  option, where we have to\nselect the desired one from a list of all running processes.\n\nThis method of attaching to a process is OK if you have to do it only once in a\nwhile, but if you have to debug applications this way regularly, it becomes\ntime-consuming to search for the process in the list every time.\n\nThe principle of automation has been written in many forms, this is one of the\nrenditions:\n\nAnything that you do more than twice has to be automated.\n\nThis applies not just to integration, deployment, testing, etc., but to tooling\nas well.\nSo I started looking for a more convenient way to attach to a specific process\nquickly.\n\nVisual Studio Macros\nThe first approach I found is to write a Visual Studio macro that looks for a\nspecific process and attaches VS to it. This approach is described in this \nStack\nOverflow answer [http://stackoverflow.com/a/6696813/974733], it seemed very\nstraightforward and promising.\nHowever, the first comment on the answer destroys our happiness:\n\n\"Macros are no longer available in Visual Studio 2012.\"\n\nOk, so this is a dead end. Or is it?\n\nVisual Commander to the rescue!\nI really like it when a popular feature of a product is removed, but the\ncommunity brings it back. The same thing happened to VS macros with Visual\nCommander [https://vlasovstudio.com/visual-commander/], a freemium VS extension\nthat lets us reuse existing VS macros, and create commands and extensions using\nC# or VB.NET.\nAfter installing the extension, you can open the Commands windows with the\nVCMD->Commands option.\n\nIn the Commands window we can manage our existing commands, and create a new one\nwith the Add button.\n\nIn the command editor window can pick a name, select the language we'd like to\nuse (C# or VB), and implement the code which will run when we execute the\ncommand.\n\nOne gotcha: pick the desired language before starting to type the code, because\nif you switch to another language, all your edits are erased.\nThe editor window is a bit plain, it does not support IntelliSense or automatic\ncode formatting, so you'll might be better off writing the code in a normal\nVisual Studio window and insert it into the VCMD command afterwards.\n\nThe code for attaching to a process with a specific name is quite simple, as\ndescribed in the above SO-answer. This is the same code translated to C#.\n\nusing EnvDTE;\nusing EnvDTE80;\nusing System.Management;\nusing System;\n\npublic class C : VisualCommanderExt.ICommand\n{\n    public void Run(EnvDTE80.DTE2 DTE, Microsoft.VisualStudio.Shell.Package package) \n    {\n        foreach(Process proc in DTE.Debugger.LocalProcesses)\n        {\n            if(proc.Name.ToString().EndsWith(\"MyApp.exe\"))\n            {\n                proc.Attach();\n                return;\n            }\n        }\n\n        System.Windows.MessageBox.Show(\"Process running the MyApp was not found.\");\n    }\n}\n\n\nThe above code tries to attach to a process that has a name ending with\n\"MyApp.exe\". The command in this form is already ready to use. Existing commands\ncan be found with their specified name in the VCMD menu.\n\n\nAttach to a process with a specific owner user\nSometimes we can't identify the process we are looking for by it's name, but we\nhave to use the name of the user running it.\nA typical scenario for this is when we have several web applications running in\nIIS. In this case all the processes have the same name, w3wp.exe, they only\ndiffer in the name of the user running them. (That is, if we use different\napplication pools for each of the applications.)\n\n\nFinding out the process owner's user name\nUnfortunately, the EnvDTE.Process  interface\n[https://msdn.microsoft.com/en-us/library/envdte.process.aspx]  does not contain\nthe user name, so we have to do a little additional coding to find it.\nOne solution is to use WMI, which can be used in .NET with the System.Management\nnamespace, and implement a little helper method.\n\nprivate string GetProcessOwner(int processId)\n{\n    string query = \"Select * From Win32_Process Where ProcessID = \" + processId;\n    ManagementObjectSearcher searcher = new ManagementObjectSearcher(query);\n    ManagementObjectCollection processList = searcher.Get();\n\n    foreach (ManagementObject obj in processList)\n    {\n        string[] argList = new string[] { string.Empty, string.Empty };\n        int returnVal = Convert.ToInt32(obj.InvokeMethod(\"GetOwner\", argList));\n        if (returnVal == 0)\n        {\n            // Return DOMAIN\\user.\n            return argList[1] + \"\\\\\" + argList[0];\n        }\n    }\n\n    return \"NO OWNER\";\n}\n\n\nWith this, we can implement the complete VS command attaching for a specific IIS\napplication.\n\nusing EnvDTE;\nusing EnvDTE80;\nusing System.Management;\nusing System;\n\npublic class C : VisualCommanderExt.ICommand\n{\n    public void Run(EnvDTE80.DTE2 DTE, Microsoft.VisualStudio.Shell.Package package) \n    {\n        foreach(Process proc in DTE.Debugger.LocalProcesses)\n        {\n            if(proc.Name.ToString().EndsWith(\"w3wp.exe\") && GetProcessOwner(proc.ProcessID) == \"IIS APPPOOL\\\\MySite\")\n            {\n                proc.Attach();\n                return;\n            }\n        }\n\n        System.Windows.MessageBox.Show(\"Process runing MySite was not found.\");\n    }\n\n    private string GetProcessOwner(int processId)\n    {\n        string query = \"Select * From Win32_Process Where ProcessID = \" + processId;\n        ManagementObjectSearcher searcher = new ManagementObjectSearcher(query);\n        ManagementObjectCollection processList = searcher.Get();\n\n        foreach (ManagementObject obj in processList)\n        {\n            string[] argList = new string[] { string.Empty, string.Empty };\n            int returnVal = Convert.ToInt32(obj.InvokeMethod(\"GetOwner\", argList));\n            if (returnVal == 0)\n            {\n                // Return DOMAIN\\user.\n                return argList[1] + \"\\\\\" + argList[0];\n            }\n        }\n\n        return \"NO OWNER\";\n    }\n}\n\n\nThe last thing to take care of is making sure the command has a reference to the\nSystem.Management assembly. We simply have to add the line System.Management  in\nthe References window.\n\nOne last thing to note is a cool feature of Visual Commander: you can create an \nexport  of the commands in the VCMD menu, and distribute them in your team, to\nmake everybody debug processes as conveniently as possible!",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": "Attach to specific Process shortcut in Visual Studio",
        "meta_description": "How to create a shortcut in Visual Studio to attach the debugger to a specific process.",
        "author_id": "1",
        "created_at": "2015-04-26 09:03:32",
        "created_by": "1",
        "updated_at": "2015-09-05 08:30:30",
        "updated_by": "1",
        "published_at": "2015-04-26 12:21:19",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de295e",
        "uuid": "a09998a1-497e-4b6b-9a32-891767a8d807",
        "title": "Use Glimpse with ASP.NET Web Api",
        "slug": "use-glimpse-with-asp-net-web-api",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Glimpse is a wonderful tool for getting an insight into the mechanisms happening in an ASP.NET application. It inspects every request processed by our app, and displays its UI either embedded into our web site, or on a standalone page at a different URL.\\n\\n![Example of the Glimpse UI](/content/images/2015/05/1example-1.png)\\n\\nThe current version of Glimpse (1.9.2 at the time of writing this) only has proper support for ASP.NET Web Forms and MVC, and not for the Web Api.\\nIn this post I'm going to look at how can we use Glimpse for the Web Api, what are the features which works for Web Api as well, and what is missing.\\n\\n#Getting Started\\n\\nI am going to get started by creating an ASP.NET web project that contains both MVC and Web Api elements, and see how well Glimpse performs out of the box.\\nI created a new project by checking both MVC and Web Api in the project creation wizard.\\n\\n![Creating a new ASP.NET project with both MVC and Web Api support.](/content/images/2015/05/20createproject.png)\\n\\n##Creating a test application\\n\\nIn order to be able to try out Glimpse, I created a simple web application with both an MVC and a Web Api controller, that can return a list of guitars stored in the database of an instrument shop.\\n\\n###Data model\\n\\nFor the the data model I used Entity Framework Code First with LocalDB to store some sample data. EF is no longer distributed with the .NET Framework, but rather published in a NuGet package. We can install it with the following command.\\n\\n    Install-Package EntityFramework\\n\\nThe following data model will represent manufacturers and models of guitars sold in a guitar shop:\\n\\n```csharp\\nnamespace GlimpseTest.Models\\n{\\n    public class Guitar\\n    {\\n        public int GuitarId { get; set; }\\n\\n        public GuitarType Type { get; set; }\\n\\n        [JsonIgnore]\\n        public virtual Manufacturer Manufacturer { get; set; }\\n\\n        public string Name { get; set; }\\n\\n        public string Material { get; set; }\\n\\n        public decimal Price { get; set; }\\n    }\\n\\n    public class Manufacturer\\n    {\\n        public int ManufacturerId { get; set; }\\n\\n        public string Name { get; set; }\\n\\n        public virtual IList<Guitar> Models { get; set; }\\n    }\\n\\n    public enum GuitarType\\n    {\\n        Classical,\\n        Acoustic,\\n        Electric,\\n        Bass\\n    }\\n}\\n```\\n\\nI created the database context with EF code first and implemented a configuration class to seed the database with some test data.\\n\\n```csharp\\nnamespace GlimpseTest.Models\\n{\\n    public class GuitarShopContext : DbContext\\n    {\\n        public GuitarShopContext() : base(\\\"GuitarShopContext\\\")\\n        {\\n        }\\n\\n        public DbSet<Guitar> Guitars { get; set; }\\n\\n        public DbSet<Manufacturer> Manufacturers { get; set; }\\n    }\\n\\n    public class AlwaysDropAndCreateInitializer : DropCreateDatabaseAlways<GuitarShopContext>\\n    {\\n        protected override void Seed(GuitarShopContext context)\\n        {\\n            base.Seed(context);\\n\\n            var yamaha = new Manufacturer\\n            {\\n                Name = \\\"Yamaha\\\",\\n                Models = new List<Guitar>\\n                {\\n                    new Guitar\\n                    {\\n                        Name = \\\"C40\\\",\\n                        Material = \\\"Spruce\\\",\\n                        Type = GuitarType.Classical\\n                    },\\n                    new Guitar\\n                    {\\n                        Name = \\\"FS700S\\\",\\n                        Material = \\\"Rosewood\\\",\\n                        Type = GuitarType.Acoustic\\n                    }\\n                }\\n            };\\n\\n            var gibson = new Manufacturer\\n            {\\n                Name = \\\"Gibson\\\",\\n                Models = new List<Guitar>\\n                {\\n                    new Guitar\\n                    {\\n                        Name = \\\"ES-175\\\",\\n                        Material = \\\"Maple\\\",\\n                        Type = GuitarType.Electric\\n                    }\\n                }\\n            };\\n\\n            context.Manufacturers.Add(yamaha);\\n            context.Manufacturers.Add(gibson);\\n        }\\n    }\\n}\\n```\\n\\nWe need to configure the database initializer in our Application_Start method class in the Global.asax.cs.\\n\\n```csharp\\nDatabase.SetInitializer(new AlwaysDropAndCreateInitializer());\\n```\\n\\nThe last thing to do in order to make our data model work is to configure a connection string to the database. I used LocalDB for testing.\\n\\n```xml\\n<connectionStrings>\\n  <add name=\\\"GuitarShopContext\\\" providerName=\\\"System.Data.SqlClient\\\" connectionString=\\\"Data Source=(localdb)\\\\v11.0;Initial Catalog=GuitarShop;Integrated Security=True\\\" />\\n</connectionStrings>\\n```\\n\\n###Controllers\\n\\nTo be able to test Glimpse in its merits, we need to create some controllers as well. I implemented basically the same functionality in both an MVC and a Web Api controller.\\n\\n#### MVC\\n\\nThe controller has a single action, that returns the list of manufacturers and guitars stored in the database. The controller action does two things.\\n\\n - Loads the data from the database\\n - Imitates a service call to an external service to fetch the prices of the models\\n\\nThe controller:\\n\\n```csharp\\nnamespace GlimpseTest.Controllers\\n{\\n    public class GuitarController : Controller\\n    {\\n        public async Task<ActionResult> Index()\\n        {\\n            using (var ctx = new GuitarShopContext())\\n            {\\n                var manufacturers = await ctx.Manufacturers.Include(\\\"Models\\\").ToListAsync();\\n\\n                await this.LoadPrices(manufacturers.SelectMany(m => m.Models));\\n\\n                return this.View(manufacturers);\\n            }\\n        }\\n\\n        private async Task LoadPrices(IEnumerable<Guitar> guitars)\\n        {\\n            Random rnd = new Random();\\n            foreach (var guitar in guitars)\\n            {\\n                guitar.Price = rnd.Next(100, 500);\\n            }\\n\\n            await Task.Delay(500);\\n        }\\n    }\\n}\\n```\\n\\nThe views consists of two parts. Views\\\\Shared\\\\DisplayTemplates\\\\Guitars.cshtml contains the view for displaying a list of guitars.\\n\\n```html\\n@model IEnumerable<GlimpseTest.Models.Guitar>\\n\\n<table class=\\\"table\\\">\\n    <tr>\\n        <th>\\n            @Html.DisplayNameFor(model => model.Name)\\n        </th>\\n        <th>\\n            @Html.DisplayNameFor(model => model.Material)\\n        </th>\\n        <th>\\n            @Html.DisplayNameFor(model => model.Price)\\n        </th>\\n    </tr>\\n\\n@foreach (var item in Model) {\\n    <tr>\\n        <td>\\n            @Html.DisplayFor(modelItem => item.Name)\\n        </td>\\n        <td>\\n            @Html.DisplayFor(modelItem => item.Material)\\n        </td>\\n        <td>\\n            @Html.DisplayFor(modelItem => item.Price)\\n        </td>\\n    </tr>\\n}\\n\\n</table>\\n```\\n\\nView\\\\Guitar\\\\Index.cshtml is the main view for this action.\\n\\n```html\\n@model IEnumerable<GlimpseTest.Models.Manufacturer>\\n\\n@{\\n    ViewBag.Title = \\\"Index\\\";\\n}\\n\\n<h2>Guitars</h2>\\n\\n<div>\\n    @foreach (var manufacturer in Model)\\n    {\\n        <h4>\\n            @Html.DisplayFor(modelItem => manufacturer.Name)\\n        </h4>\\n\\n        @Html.Partial(\\\"~/Views/Shared/DisplayTemplates/Guitars.cshtml\\\", manufacturer.Models);\\n    }\\n</div>\\n```\\n\\n####Web Api\\n\\nThe Web Api controller is very similar, but it has a different base class and method signature:\\n\\n```csharp\\nnamespace GlimpseTest.Controllers\\n{\\n    public class GuitarApiController : ApiController\\n    {\\n        public async Task<IHttpActionResult> Get()\\n        {\\n            using (var ctx = new GuitarShopContext())\\n            {\\n                var manufacturers = await ctx.Manufacturers.Include(\\\"Models\\\").ToListAsync();\\n\\n                await this.LoadPrices(manufacturers.SelectMany(m => m.Models));\\n\\n                return this.Ok(manufacturers);\\n            }\\n        }\\n\\n        private async Task LoadPrices(IEnumerable<Guitar> guitars)\\n        {\\n            Random rnd = new Random();\\n            foreach (var guitar in guitars)\\n            {\\n                guitar.Price = rnd.Next(100, 500);\\n            }\\n\\n            await Task.Delay(500);\\n        }\\n    }\\n}\\n```\\n\\nThe Web Api controller returns the data in Json directly, so it does not have a corresponding view.\\nOne convenient thing to do is to make the Json formatter be the only formatter, so we can open endpoints in our browser without having to specify the Accept header. In order to do this, we have to insert the following piece of code in the start of the Register method in our WebApiConfig class:\\n\\n```csharp\\n// Web API configuration and services\\nvar jsonFormatter = config.Formatters.JsonFormatter;\\nconfig.Formatters.Clear();\\nconfig.Formatters.Add(jsonFormatter);\\n```\\nThis code removes every formatter, except the Json one.\\n\\n##Installing Glimpse\\n\\nInstalling Glimpse is merely adding a NuGet package to our ASP.NET project. For using with the latest version of MVC, Glimpse can be added with the following command from the Package Manager Console:\\n\\n    Install-Package Glimpse.Mvc5\\nSadly, there is no official package for Web Api yet. Official Web Api support is supposed to come with version 2 of Glimpse, but no estimated release date has been announce. (However, there is active work going on in this area: https://github.com/Glimpse/Glimpse/issues/715)\\n\\n###Using Glimpse with MVC\\nAfter installing Glimpse this way, we can start using it immediately. To enable Glimpse, go to the url /Glimpse.axd, and click on Turn Glimpse On.\\n\\n![Turn Glimpse on](/content/images/2015/05/25turnonglimpse.png)\\n\\nIf we navigate to the index of our guitar controller, we should see the Glimpse plugin showing up on the bottom of the page, where we can see a summary of the information recorded during the request.\\n\\n![Default view of the Glimpse plugin.](/content/images/2015/05/30glimpseinitial-1.png)\\n\\nIf we click on the g button in the bottom right hand corner, the full view of Glimpse is opened, where we find several tabs showing different kinds of data collected about the request:\\n\\n![Full view of the Glimpse plugin.](/content/images/2015/05/40glimpsefull.png)\\n\\nBecause MVC is officially supported, these tabs all work very well out of the box: we can see the order of different events on the Execution tabs, the Routes tab shows information about the routes taken into consideration, and the Timeline tab displays timing information about the different phases of the request processing.\\nOne thing that's missing but we would expect to see is insight into the SQL commands being executed during the request. This is not recorded out of the box, but luckily, there is a package that does just this. The package for simple ADO.NET is Glimpse.ADO, but we are using Entity Framework, which needs a different package, Glimpse.EF6 (for Entity Framework version six). We can install it with the following command.\\n\\n    Install-Package Glimpse.EF6\\nAfter installing the package, we will have an additional SQL tab, on which we can see all the SQL queries executed during the request.\\n\\n![Glimpse showing SQL queries with the Glimpse.EF6 package.](/content/images/2015/05/50glimpseef.png)\\n\\nSo we can see that with ASP.NET MVC, Glimpse works very well out of the box without any special customization.\\n\\n###Using Glimpse with Web Api\\nLet's try our Web Api controller by opening it in the browser.\\n\\n![Opening a Web Api action in the browser.](/content/images/2015/05/60jsonapi.png)\\n\\nHmm, when we send a request for a Web Api action, instead of a web page, we get back the result in its raw form, serialized to Json. So there is no place for the Glimpse plugin to get embedded into. How can we use Glimpse in this situation then?\\n\\n####Standalone Glimpse page\\nIn order to be able to use Glimpse independently from the web page itself, it also supports a standalone view, that can also be accessed from its /Glimpse.axd configuration page.\\n\\n![Opening the standalone Glimpse page.](/content/images/2015/05/70standaloneglimpse-1.png)\\n\\nOn this page we can see a list of all the requests coming to our application, let them be MVC or Web Api requests.\\n\\n![The standalone Glimpse page.](/content/images/2015/05/80standaloneglimpse.png)\\n\\nWhen we click on the Inspect link, we can view the information collected on the different tabs the same way we did with the embedded plugin. However, when we inspect a Web Api request, we can see that the following tabs are showing no information: Execution, Metadata, Routes, Views. In addition to that, the Timeline tab doesn't show any Web Api-related information, only the events related to Entity Framework.\\n\\nThese are the things that should be supported in the upcoming version of Glimpse, probably the unification of the MVC and Web Api data model in ASP.NET 5 will make this simpler.\\n\\n###How to improve?\\n\\nThere are at least two features which work just as well with Web Api as MVC: Tracing and custom Timeline messages.\\n\\n####Tracing\\n\\nTracing is very easy to use with Glimpse. Any standard .NET trace message will be captured by Glimpse and displayed on the Trace tab.\\nThe big advantage of browsing your trace messages with Glimpse is that it shows the trace messages in the context of a single request, so you don't have to filter your whole log in order to correlate the entries with a specific request.\\nIf we add some trace messages to our application\\n\\n```csharp\\nTrace.Write(\\\"Loading manufacturers from the database.\\\");\\n...\\nTrace.Write(\\\"Getting price information\\\");\\n```\\nThey will show up in the Trace tab.\\n\\n![Custom Trace messages.](/content/images/2015/05/90tracing.png)\\n\\n####Custom timeline messages\\n\\nThe timeline tab is a great tool to see timing information and to analyze how long different parts of the request processing take. With Web Api the information displayed out of the box on the Timeline tab is rather limited.\\nIt is possible to extend it with custom events, however, this is not as trivial as emitting trace messages.\\n\\nThere is no simple way in the public Glimpse API to emit our own Timeline events, we have to implement a couple of helper classes containing the necessary boilerplate code.\\n\\nLet's add a code file called GlimpseTimeline.cs to our project and implement a couple of classes and methods in the Glimpse.Core namespace.\\n\\n```csharp\\nusing System;\\nusing Glimpse.Core.Extensibility;\\nusing Glimpse.Core.Framework;\\nusing Glimpse.Core.Message;\\n\\nnamespace Glimpse.Core\\n{\\n    public class TimelineMessage : ITimelineMessage\\n    {\\n        public TimelineMessage()\\n        {\\n            Id = Guid.NewGuid();\\n        }\\n\\n        public Guid Id { get; private set; }\\n        public TimeSpan Offset { get; set; }\\n        public TimeSpan Duration { get; set; }\\n        public DateTime StartTime { get; set; }\\n        public string EventName { get; set; }\\n        public TimelineCategoryItem EventCategory { get; set; }\\n        public string EventSubText { get; set; }\\n    }\\n\\n    public static class GlimpseTimeline\\n    {\\n        private static readonly TimelineCategoryItem DefaultCategory = new TimelineCategoryItem(\\\"User\\\", \\\"green\\\", \\\"blue\\\");\\n\\n        public static OngoingCapture Capture(string eventName)\\n        {\\n            return Capture(eventName, null, DefaultCategory, new TimelineMessage());\\n        }\\n\\n        public static OngoingCapture Capture(string eventName, string eventSubText)\\n        {\\n            return Capture(eventName, eventSubText, DefaultCategory, new TimelineMessage());\\n        }\\n\\n        internal static OngoingCapture Capture(string eventName, TimelineCategoryItem category)\\n        {\\n            return Capture(eventName, null, category, new TimelineMessage());\\n        }\\n\\n        internal static OngoingCapture Capture(string eventName, TimelineCategoryItem category, ITimelineMessage message)\\n        {\\n            return Capture(eventName, null, category, message);\\n        }\\n\\n        internal static OngoingCapture Capture(string eventName, ITimelineMessage message)\\n        {\\n            return Capture(eventName, null, DefaultCategory, message);\\n        }\\n\\n        internal static OngoingCapture Capture(string eventName, string eventSubText, TimelineCategoryItem category, ITimelineMessage message)\\n        {\\n            if (string.IsNullOrEmpty(eventName))\\n            {\\n                throw new ArgumentNullException(\\\"eventName\\\");\\n            }\\n\\n#pragma warning disable 618\\n            var executionTimer = GlimpseConfiguration.GetConfiguredTimerStrategy()();\\n            var messageBroker = GlimpseConfiguration.GetConfiguredMessageBroker();\\n#pragma warning restore 618\\n\\n            if (executionTimer == null || messageBroker == null)\\n            {\\n                return OngoingCapture.Empty();\\n            }\\n\\n            return new OngoingCapture(executionTimer, messageBroker, eventName, eventSubText, category, message);\\n        }\\n\\n        public static void CaptureMoment(string eventName)\\n        {\\n            CaptureMoment(eventName, null, DefaultCategory, new TimelineMessage());\\n        }\\n\\n        public static void CaptureMoment(string eventName, string eventSubText)\\n        {\\n            CaptureMoment(eventName, eventSubText, DefaultCategory, new TimelineMessage());\\n        }\\n\\n        internal static void CaptureMoment(string eventName, TimelineCategoryItem category)\\n        {\\n            CaptureMoment(eventName, null, category, new TimelineMessage());\\n        }\\n\\n        internal static void CaptureMoment(string eventName, TimelineCategoryItem category, ITimelineMessage message)\\n        {\\n            CaptureMoment(eventName, null, category, message);\\n        }\\n\\n        internal static void CaptureMoment(string eventName, ITimelineMessage message)\\n        {\\n            CaptureMoment(eventName, null, DefaultCategory, message);\\n        }\\n\\n        internal static void CaptureMoment(string eventName, string eventSubText, TimelineCategoryItem category, ITimelineMessage message)\\n        {\\n            if (string.IsNullOrEmpty(eventName))\\n            {\\n                throw new ArgumentNullException(\\\"eventName\\\");\\n            }\\n\\n#pragma warning disable 618\\n            var executionTimer = GlimpseConfiguration.GetConfiguredTimerStrategy()();\\n            var messageBroker = GlimpseConfiguration.GetConfiguredMessageBroker();\\n#pragma warning restore 618\\n\\n            if (executionTimer == null || messageBroker == null)\\n            {\\n                return;\\n            }\\n\\n            message\\n                .AsTimelineMessage(eventName, category, eventSubText)\\n                .AsTimedMessage(executionTimer.Point());\\n\\n            messageBroker.Publish(message);\\n        }\\n\\n        public class OngoingCapture : IDisposable\\n        {\\n            public static OngoingCapture Empty()\\n            {\\n                return new NullOngoingCapture();\\n            }\\n\\n            private OngoingCapture()\\n            {\\n            }\\n\\n            public OngoingCapture(IExecutionTimer executionTimer, IMessageBroker messageBroker, string eventName, string eventSubText, TimelineCategoryItem category, ITimelineMessage message)\\n            {\\n                Offset = executionTimer.Start();\\n                ExecutionTimer = executionTimer;\\n                Message = message.AsTimelineMessage(eventName, category, eventSubText);\\n                MessageBroker = messageBroker;\\n            }\\n\\n            private ITimelineMessage Message { get; set; }\\n\\n            private TimeSpan Offset { get; set; }\\n\\n            private IExecutionTimer ExecutionTimer { get; set; }\\n\\n            private IMessageBroker MessageBroker { get; set; }\\n\\n            public virtual void Stop()\\n            {\\n                var timerResult = ExecutionTimer.Stop(Offset);\\n\\n                MessageBroker.Publish(Message.AsTimedMessage(timerResult));\\n            }\\n\\n            public void Dispose()\\n            {\\n                Stop();\\n            }\\n\\n            private class NullOngoingCapture : OngoingCapture\\n            {\\n                public override void Stop()\\n                {\\n                }\\n            }\\n        }\\n    }\\n}\\n```\\n\\n(There are several slightly different versions of this code around on the internet, I took this version from the following gist: https://gist.github.com/johnjuuljensen/776e61b720c2a7e5b6ef)\\n\\nWith this helper logic we can add our own timing events to our controller.\\n\\n```csharp\\nusing Glimpse.Core;\\n...\\nList<Manufacturer> manufacturers;\\nusing (GlimpseTimeline.Capture(\\\"Loading from DB\\\"))\\n{\\n    manufacturers = await ctx.Manufacturers.Include(\\\"Models\\\").ToListAsync();\\n}\\n\\nusing (GlimpseTimeline.Capture(\\\"Fetching product prices\\\"))\\n{\\n    await this.LoadPrices(manufacturers.SelectMany(m => m.Models));\\n}\\n```\\n\\nOur custom timeline events will show up in the timeline tab.\\n\\n![Custom Timeline events.](/content/images/2015/05/100timeline.png)\\n\\nIf you'd like to take a look at it, you can find the whole source code on [GitHub](https://github.com/markvincze/glimpse-web-api).\\n#Conclusion\\nWe've seen that setting up Glimpse for the Web Api is just as easy as using it with MVC, however, the feature set supported is much more limited. Still, Glimpse can be a very useful tool to analyze what's going on under the hood in a Web Api application as well.\"}]],\"sections\":[[10,0]]}",
        "html": "<p>Glimpse is a wonderful tool for getting an insight into the mechanisms happening in an ASP.NET application. It inspects every request processed by our app, and displays its UI either embedded into our web site, or on a standalone page at a different URL.</p>\n<p><img src=\"/content/images/2015/05/1example-1.png\" alt=\"Example of the Glimpse UI\"></p>\n<p>The current version of Glimpse (1.9.2 at the time of writing this) only has proper support for ASP.NET Web Forms and MVC, and not for the Web Api.<br>\nIn this post I'm going to look at how can we use Glimpse for the Web Api, what are the features which works for Web Api as well, and what is missing.</p>\n<h1 id=\"gettingstarted\">Getting Started</h1>\n<p>I am going to get started by creating an ASP.NET web project that contains both MVC and Web Api elements, and see how well Glimpse performs out of the box.<br>\nI created a new project by checking both MVC and Web Api in the project creation wizard.</p>\n<p><img src=\"/content/images/2015/05/20createproject.png\" alt=\"Creating a new ASP.NET project with both MVC and Web Api support.\"></p>\n<h2 id=\"creatingatestapplication\">Creating a test application</h2>\n<p>In order to be able to try out Glimpse, I created a simple web application with both an MVC and a Web Api controller, that can return a list of guitars stored in the database of an instrument shop.</p>\n<h3 id=\"datamodel\">Data model</h3>\n<p>For the the data model I used Entity Framework Code First with LocalDB to store some sample data. EF is no longer distributed with the .NET Framework, but rather published in a NuGet package. We can install it with the following command.</p>\n<pre><code>Install-Package EntityFramework\n</code></pre>\n<p>The following data model will represent manufacturers and models of guitars sold in a guitar shop:</p>\n<pre><code class=\"language-csharp\">namespace GlimpseTest.Models\n{\n    public class Guitar\n    {\n        public int GuitarId { get; set; }\n\n        public GuitarType Type { get; set; }\n\n        [JsonIgnore]\n        public virtual Manufacturer Manufacturer { get; set; }\n\n        public string Name { get; set; }\n\n        public string Material { get; set; }\n\n        public decimal Price { get; set; }\n    }\n\n    public class Manufacturer\n    {\n        public int ManufacturerId { get; set; }\n\n        public string Name { get; set; }\n\n        public virtual IList&lt;Guitar&gt; Models { get; set; }\n    }\n\n    public enum GuitarType\n    {\n        Classical,\n        Acoustic,\n        Electric,\n        Bass\n    }\n}\n</code></pre>\n<p>I created the database context with EF code first and implemented a configuration class to seed the database with some test data.</p>\n<pre><code class=\"language-csharp\">namespace GlimpseTest.Models\n{\n    public class GuitarShopContext : DbContext\n    {\n        public GuitarShopContext() : base(&quot;GuitarShopContext&quot;)\n        {\n        }\n\n        public DbSet&lt;Guitar&gt; Guitars { get; set; }\n\n        public DbSet&lt;Manufacturer&gt; Manufacturers { get; set; }\n    }\n\n    public class AlwaysDropAndCreateInitializer : DropCreateDatabaseAlways&lt;GuitarShopContext&gt;\n    {\n        protected override void Seed(GuitarShopContext context)\n        {\n            base.Seed(context);\n\n            var yamaha = new Manufacturer\n            {\n                Name = &quot;Yamaha&quot;,\n                Models = new List&lt;Guitar&gt;\n                {\n                    new Guitar\n                    {\n                        Name = &quot;C40&quot;,\n                        Material = &quot;Spruce&quot;,\n                        Type = GuitarType.Classical\n                    },\n                    new Guitar\n                    {\n                        Name = &quot;FS700S&quot;,\n                        Material = &quot;Rosewood&quot;,\n                        Type = GuitarType.Acoustic\n                    }\n                }\n            };\n\n            var gibson = new Manufacturer\n            {\n                Name = &quot;Gibson&quot;,\n                Models = new List&lt;Guitar&gt;\n                {\n                    new Guitar\n                    {\n                        Name = &quot;ES-175&quot;,\n                        Material = &quot;Maple&quot;,\n                        Type = GuitarType.Electric\n                    }\n                }\n            };\n\n            context.Manufacturers.Add(yamaha);\n            context.Manufacturers.Add(gibson);\n        }\n    }\n}\n</code></pre>\n<p>We need to configure the database initializer in our Application_Start method class in the Global.asax.cs.</p>\n<pre><code class=\"language-csharp\">Database.SetInitializer(new AlwaysDropAndCreateInitializer());\n</code></pre>\n<p>The last thing to do in order to make our data model work is to configure a connection string to the database. I used LocalDB for testing.</p>\n<pre><code class=\"language-xml\">&lt;connectionStrings&gt;\n  &lt;add name=&quot;GuitarShopContext&quot; providerName=&quot;System.Data.SqlClient&quot; connectionString=&quot;Data Source=(localdb)\\v11.0;Initial Catalog=GuitarShop;Integrated Security=True&quot; /&gt;\n&lt;/connectionStrings&gt;\n</code></pre>\n<h3 id=\"controllers\">Controllers</h3>\n<p>To be able to test Glimpse in its merits, we need to create some controllers as well. I implemented basically the same functionality in both an MVC and a Web Api controller.</p>\n<h4 id=\"mvc\">MVC</h4>\n<p>The controller has a single action, that returns the list of manufacturers and guitars stored in the database. The controller action does two things.</p>\n<ul>\n<li>Loads the data from the database</li>\n<li>Imitates a service call to an external service to fetch the prices of the models</li>\n</ul>\n<p>The controller:</p>\n<pre><code class=\"language-csharp\">namespace GlimpseTest.Controllers\n{\n    public class GuitarController : Controller\n    {\n        public async Task&lt;ActionResult&gt; Index()\n        {\n            using (var ctx = new GuitarShopContext())\n            {\n                var manufacturers = await ctx.Manufacturers.Include(&quot;Models&quot;).ToListAsync();\n\n                await this.LoadPrices(manufacturers.SelectMany(m =&gt; m.Models));\n\n                return this.View(manufacturers);\n            }\n        }\n\n        private async Task LoadPrices(IEnumerable&lt;Guitar&gt; guitars)\n        {\n            Random rnd = new Random();\n            foreach (var guitar in guitars)\n            {\n                guitar.Price = rnd.Next(100, 500);\n            }\n\n            await Task.Delay(500);\n        }\n    }\n}\n</code></pre>\n<p>The views consists of two parts. Views\\Shared\\DisplayTemplates\\Guitars.cshtml contains the view for displaying a list of guitars.</p>\n<pre><code class=\"language-html\">@model IEnumerable&lt;GlimpseTest.Models.Guitar&gt;\n\n&lt;table class=&quot;table&quot;&gt;\n    &lt;tr&gt;\n        &lt;th&gt;\n            @Html.DisplayNameFor(model =&gt; model.Name)\n        &lt;/th&gt;\n        &lt;th&gt;\n            @Html.DisplayNameFor(model =&gt; model.Material)\n        &lt;/th&gt;\n        &lt;th&gt;\n            @Html.DisplayNameFor(model =&gt; model.Price)\n        &lt;/th&gt;\n    &lt;/tr&gt;\n\n@foreach (var item in Model) {\n    &lt;tr&gt;\n        &lt;td&gt;\n            @Html.DisplayFor(modelItem =&gt; item.Name)\n        &lt;/td&gt;\n        &lt;td&gt;\n            @Html.DisplayFor(modelItem =&gt; item.Material)\n        &lt;/td&gt;\n        &lt;td&gt;\n            @Html.DisplayFor(modelItem =&gt; item.Price)\n        &lt;/td&gt;\n    &lt;/tr&gt;\n}\n\n&lt;/table&gt;\n</code></pre>\n<p>View\\Guitar\\Index.cshtml is the main view for this action.</p>\n<pre><code class=\"language-html\">@model IEnumerable&lt;GlimpseTest.Models.Manufacturer&gt;\n\n@{\n    ViewBag.Title = &quot;Index&quot;;\n}\n\n&lt;h2&gt;Guitars&lt;/h2&gt;\n\n&lt;div&gt;\n    @foreach (var manufacturer in Model)\n    {\n        &lt;h4&gt;\n            @Html.DisplayFor(modelItem =&gt; manufacturer.Name)\n        &lt;/h4&gt;\n\n        @Html.Partial(&quot;~/Views/Shared/DisplayTemplates/Guitars.cshtml&quot;, manufacturer.Models);\n    }\n&lt;/div&gt;\n</code></pre>\n<h4 id=\"webapi\">Web Api</h4>\n<p>The Web Api controller is very similar, but it has a different base class and method signature:</p>\n<pre><code class=\"language-csharp\">namespace GlimpseTest.Controllers\n{\n    public class GuitarApiController : ApiController\n    {\n        public async Task&lt;IHttpActionResult&gt; Get()\n        {\n            using (var ctx = new GuitarShopContext())\n            {\n                var manufacturers = await ctx.Manufacturers.Include(&quot;Models&quot;).ToListAsync();\n\n                await this.LoadPrices(manufacturers.SelectMany(m =&gt; m.Models));\n\n                return this.Ok(manufacturers);\n            }\n        }\n\n        private async Task LoadPrices(IEnumerable&lt;Guitar&gt; guitars)\n        {\n            Random rnd = new Random();\n            foreach (var guitar in guitars)\n            {\n                guitar.Price = rnd.Next(100, 500);\n            }\n\n            await Task.Delay(500);\n        }\n    }\n}\n</code></pre>\n<p>The Web Api controller returns the data in Json directly, so it does not have a corresponding view.<br>\nOne convenient thing to do is to make the Json formatter be the only formatter, so we can open endpoints in our browser without having to specify the Accept header. In order to do this, we have to insert the following piece of code in the start of the Register method in our WebApiConfig class:</p>\n<pre><code class=\"language-csharp\">// Web API configuration and services\nvar jsonFormatter = config.Formatters.JsonFormatter;\nconfig.Formatters.Clear();\nconfig.Formatters.Add(jsonFormatter);\n</code></pre>\n<p>This code removes every formatter, except the Json one.</p>\n<h2 id=\"installingglimpse\">Installing Glimpse</h2>\n<p>Installing Glimpse is merely adding a NuGet package to our ASP.NET project. For using with the latest version of MVC, Glimpse can be added with the following command from the Package Manager Console:</p>\n<pre><code>Install-Package Glimpse.Mvc5\n</code></pre>\n<p>Sadly, there is no official package for Web Api yet. Official Web Api support is supposed to come with version 2 of Glimpse, but no estimated release date has been announce. (However, there is active work going on in this area: <a href=\"https://github.com/Glimpse/Glimpse/issues/715\">https://github.com/Glimpse/Glimpse/issues/715</a>)</p>\n<h3 id=\"usingglimpsewithmvc\">Using Glimpse with MVC</h3>\n<p>After installing Glimpse this way, we can start using it immediately. To enable Glimpse, go to the url /Glimpse.axd, and click on Turn Glimpse On.</p>\n<p><img src=\"/content/images/2015/05/25turnonglimpse.png\" alt=\"Turn Glimpse on\"></p>\n<p>If we navigate to the index of our guitar controller, we should see the Glimpse plugin showing up on the bottom of the page, where we can see a summary of the information recorded during the request.</p>\n<p><img src=\"/content/images/2015/05/30glimpseinitial-1.png\" alt=\"Default view of the Glimpse plugin.\"></p>\n<p>If we click on the g button in the bottom right hand corner, the full view of Glimpse is opened, where we find several tabs showing different kinds of data collected about the request:</p>\n<p><img src=\"/content/images/2015/05/40glimpsefull.png\" alt=\"Full view of the Glimpse plugin.\"></p>\n<p>Because MVC is officially supported, these tabs all work very well out of the box: we can see the order of different events on the Execution tabs, the Routes tab shows information about the routes taken into consideration, and the Timeline tab displays timing information about the different phases of the request processing.<br>\nOne thing that's missing but we would expect to see is insight into the SQL commands being executed during the request. This is not recorded out of the box, but luckily, there is a package that does just this. The package for simple ADO.NET is Glimpse.ADO, but we are using Entity Framework, which needs a different package, Glimpse.EF6 (for Entity Framework version six). We can install it with the following command.</p>\n<pre><code>Install-Package Glimpse.EF6\n</code></pre>\n<p>After installing the package, we will have an additional SQL tab, on which we can see all the SQL queries executed during the request.</p>\n<p><img src=\"/content/images/2015/05/50glimpseef.png\" alt=\"Glimpse showing SQL queries with the Glimpse.EF6 package.\"></p>\n<p>So we can see that with ASP.NET MVC, Glimpse works very well out of the box without any special customization.</p>\n<h3 id=\"usingglimpsewithwebapi\">Using Glimpse with Web Api</h3>\n<p>Let's try our Web Api controller by opening it in the browser.</p>\n<p><img src=\"/content/images/2015/05/60jsonapi.png\" alt=\"Opening a Web Api action in the browser.\"></p>\n<p>Hmm, when we send a request for a Web Api action, instead of a web page, we get back the result in its raw form, serialized to Json. So there is no place for the Glimpse plugin to get embedded into. How can we use Glimpse in this situation then?</p>\n<h4 id=\"standaloneglimpsepage\">Standalone Glimpse page</h4>\n<p>In order to be able to use Glimpse independently from the web page itself, it also supports a standalone view, that can also be accessed from its /Glimpse.axd configuration page.</p>\n<p><img src=\"/content/images/2015/05/70standaloneglimpse-1.png\" alt=\"Opening the standalone Glimpse page.\"></p>\n<p>On this page we can see a list of all the requests coming to our application, let them be MVC or Web Api requests.</p>\n<p><img src=\"/content/images/2015/05/80standaloneglimpse.png\" alt=\"The standalone Glimpse page.\"></p>\n<p>When we click on the Inspect link, we can view the information collected on the different tabs the same way we did with the embedded plugin. However, when we inspect a Web Api request, we can see that the following tabs are showing no information: Execution, Metadata, Routes, Views. In addition to that, the Timeline tab doesn't show any Web Api-related information, only the events related to Entity Framework.</p>\n<p>These are the things that should be supported in the upcoming version of Glimpse, probably the unification of the MVC and Web Api data model in ASP.NET 5 will make this simpler.</p>\n<h3 id=\"howtoimprove\">How to improve?</h3>\n<p>There are at least two features which work just as well with Web Api as MVC: Tracing and custom Timeline messages.</p>\n<h4 id=\"tracing\">Tracing</h4>\n<p>Tracing is very easy to use with Glimpse. Any standard .NET trace message will be captured by Glimpse and displayed on the Trace tab.<br>\nThe big advantage of browsing your trace messages with Glimpse is that it shows the trace messages in the context of a single request, so you don't have to filter your whole log in order to correlate the entries with a specific request.<br>\nIf we add some trace messages to our application</p>\n<pre><code class=\"language-csharp\">Trace.Write(&quot;Loading manufacturers from the database.&quot;);\n...\nTrace.Write(&quot;Getting price information&quot;);\n</code></pre>\n<p>They will show up in the Trace tab.</p>\n<p><img src=\"/content/images/2015/05/90tracing.png\" alt=\"Custom Trace messages.\"></p>\n<h4 id=\"customtimelinemessages\">Custom timeline messages</h4>\n<p>The timeline tab is a great tool to see timing information and to analyze how long different parts of the request processing take. With Web Api the information displayed out of the box on the Timeline tab is rather limited.<br>\nIt is possible to extend it with custom events, however, this is not as trivial as emitting trace messages.</p>\n<p>There is no simple way in the public Glimpse API to emit our own Timeline events, we have to implement a couple of helper classes containing the necessary boilerplate code.</p>\n<p>Let's add a code file called GlimpseTimeline.cs to our project and implement a couple of classes and methods in the Glimpse.Core namespace.</p>\n<pre><code class=\"language-csharp\">using System;\nusing Glimpse.Core.Extensibility;\nusing Glimpse.Core.Framework;\nusing Glimpse.Core.Message;\n\nnamespace Glimpse.Core\n{\n    public class TimelineMessage : ITimelineMessage\n    {\n        public TimelineMessage()\n        {\n            Id = Guid.NewGuid();\n        }\n\n        public Guid Id { get; private set; }\n        public TimeSpan Offset { get; set; }\n        public TimeSpan Duration { get; set; }\n        public DateTime StartTime { get; set; }\n        public string EventName { get; set; }\n        public TimelineCategoryItem EventCategory { get; set; }\n        public string EventSubText { get; set; }\n    }\n\n    public static class GlimpseTimeline\n    {\n        private static readonly TimelineCategoryItem DefaultCategory = new TimelineCategoryItem(&quot;User&quot;, &quot;green&quot;, &quot;blue&quot;);\n\n        public static OngoingCapture Capture(string eventName)\n        {\n            return Capture(eventName, null, DefaultCategory, new TimelineMessage());\n        }\n\n        public static OngoingCapture Capture(string eventName, string eventSubText)\n        {\n            return Capture(eventName, eventSubText, DefaultCategory, new TimelineMessage());\n        }\n\n        internal static OngoingCapture Capture(string eventName, TimelineCategoryItem category)\n        {\n            return Capture(eventName, null, category, new TimelineMessage());\n        }\n\n        internal static OngoingCapture Capture(string eventName, TimelineCategoryItem category, ITimelineMessage message)\n        {\n            return Capture(eventName, null, category, message);\n        }\n\n        internal static OngoingCapture Capture(string eventName, ITimelineMessage message)\n        {\n            return Capture(eventName, null, DefaultCategory, message);\n        }\n\n        internal static OngoingCapture Capture(string eventName, string eventSubText, TimelineCategoryItem category, ITimelineMessage message)\n        {\n            if (string.IsNullOrEmpty(eventName))\n            {\n                throw new ArgumentNullException(&quot;eventName&quot;);\n            }\n\n#pragma warning disable 618\n            var executionTimer = GlimpseConfiguration.GetConfiguredTimerStrategy()();\n            var messageBroker = GlimpseConfiguration.GetConfiguredMessageBroker();\n#pragma warning restore 618\n\n            if (executionTimer == null || messageBroker == null)\n            {\n                return OngoingCapture.Empty();\n            }\n\n            return new OngoingCapture(executionTimer, messageBroker, eventName, eventSubText, category, message);\n        }\n\n        public static void CaptureMoment(string eventName)\n        {\n            CaptureMoment(eventName, null, DefaultCategory, new TimelineMessage());\n        }\n\n        public static void CaptureMoment(string eventName, string eventSubText)\n        {\n            CaptureMoment(eventName, eventSubText, DefaultCategory, new TimelineMessage());\n        }\n\n        internal static void CaptureMoment(string eventName, TimelineCategoryItem category)\n        {\n            CaptureMoment(eventName, null, category, new TimelineMessage());\n        }\n\n        internal static void CaptureMoment(string eventName, TimelineCategoryItem category, ITimelineMessage message)\n        {\n            CaptureMoment(eventName, null, category, message);\n        }\n\n        internal static void CaptureMoment(string eventName, ITimelineMessage message)\n        {\n            CaptureMoment(eventName, null, DefaultCategory, message);\n        }\n\n        internal static void CaptureMoment(string eventName, string eventSubText, TimelineCategoryItem category, ITimelineMessage message)\n        {\n            if (string.IsNullOrEmpty(eventName))\n            {\n                throw new ArgumentNullException(&quot;eventName&quot;);\n            }\n\n#pragma warning disable 618\n            var executionTimer = GlimpseConfiguration.GetConfiguredTimerStrategy()();\n            var messageBroker = GlimpseConfiguration.GetConfiguredMessageBroker();\n#pragma warning restore 618\n\n            if (executionTimer == null || messageBroker == null)\n            {\n                return;\n            }\n\n            message\n                .AsTimelineMessage(eventName, category, eventSubText)\n                .AsTimedMessage(executionTimer.Point());\n\n            messageBroker.Publish(message);\n        }\n\n        public class OngoingCapture : IDisposable\n        {\n            public static OngoingCapture Empty()\n            {\n                return new NullOngoingCapture();\n            }\n\n            private OngoingCapture()\n            {\n            }\n\n            public OngoingCapture(IExecutionTimer executionTimer, IMessageBroker messageBroker, string eventName, string eventSubText, TimelineCategoryItem category, ITimelineMessage message)\n            {\n                Offset = executionTimer.Start();\n                ExecutionTimer = executionTimer;\n                Message = message.AsTimelineMessage(eventName, category, eventSubText);\n                MessageBroker = messageBroker;\n            }\n\n            private ITimelineMessage Message { get; set; }\n\n            private TimeSpan Offset { get; set; }\n\n            private IExecutionTimer ExecutionTimer { get; set; }\n\n            private IMessageBroker MessageBroker { get; set; }\n\n            public virtual void Stop()\n            {\n                var timerResult = ExecutionTimer.Stop(Offset);\n\n                MessageBroker.Publish(Message.AsTimedMessage(timerResult));\n            }\n\n            public void Dispose()\n            {\n                Stop();\n            }\n\n            private class NullOngoingCapture : OngoingCapture\n            {\n                public override void Stop()\n                {\n                }\n            }\n        }\n    }\n}\n</code></pre>\n<p>(There are several slightly different versions of this code around on the internet, I took this version from the following gist: <a href=\"https://gist.github.com/johnjuuljensen/776e61b720c2a7e5b6ef\">https://gist.github.com/johnjuuljensen/776e61b720c2a7e5b6ef</a>)</p>\n<p>With this helper logic we can add our own timing events to our controller.</p>\n<pre><code class=\"language-csharp\">using Glimpse.Core;\n...\nList&lt;Manufacturer&gt; manufacturers;\nusing (GlimpseTimeline.Capture(&quot;Loading from DB&quot;))\n{\n    manufacturers = await ctx.Manufacturers.Include(&quot;Models&quot;).ToListAsync();\n}\n\nusing (GlimpseTimeline.Capture(&quot;Fetching product prices&quot;))\n{\n    await this.LoadPrices(manufacturers.SelectMany(m =&gt; m.Models));\n}\n</code></pre>\n<p>Our custom timeline events will show up in the timeline tab.</p>\n<p><img src=\"/content/images/2015/05/100timeline.png\" alt=\"Custom Timeline events.\"></p>\n<p>If you'd like to take a look at it, you can find the whole source code on <a href=\"https://github.com/markvincze/glimpse-web-api\">GitHub</a>.</p>\n<h1 id=\"conclusion\">Conclusion</h1>\n<p>We've seen that setting up Glimpse for the Web Api is just as easy as using it with MVC, however, the feature set supported is much more limited. Still, Glimpse can be a very useful tool to analyze what's going on under the hood in a Web Api application as well.</p>\n",
        "comment_id": "3",
        "plaintext": "Glimpse is a wonderful tool for getting an insight into the mechanisms happening\nin an ASP.NET application. It inspects every request processed by our app, and\ndisplays its UI either embedded into our web site, or on a standalone page at a\ndifferent URL.\n\n\n\nThe current version of Glimpse (1.9.2 at the time of writing this) only has\nproper support for ASP.NET Web Forms and MVC, and not for the Web Api.\nIn this post I'm going to look at how can we use Glimpse for the Web Api, what\nare the features which works for Web Api as well, and what is missing.\n\nGetting Started\nI am going to get started by creating an ASP.NET web project that contains both\nMVC and Web Api elements, and see how well Glimpse performs out of the box.\nI created a new project by checking both MVC and Web Api in the project creation\nwizard.\n\n\n\nCreating a test application\nIn order to be able to try out Glimpse, I created a simple web application with\nboth an MVC and a Web Api controller, that can return a list of guitars stored\nin the database of an instrument shop.\n\nData model\nFor the the data model I used Entity Framework Code First with LocalDB to store\nsome sample data. EF is no longer distributed with the .NET Framework, but\nrather published in a NuGet package. We can install it with the following\ncommand.\n\nInstall-Package EntityFramework\n\n\nThe following data model will represent manufacturers and models of guitars sold\nin a guitar shop:\n\nnamespace GlimpseTest.Models\n{\n    public class Guitar\n    {\n        public int GuitarId { get; set; }\n\n        public GuitarType Type { get; set; }\n\n        [JsonIgnore]\n        public virtual Manufacturer Manufacturer { get; set; }\n\n        public string Name { get; set; }\n\n        public string Material { get; set; }\n\n        public decimal Price { get; set; }\n    }\n\n    public class Manufacturer\n    {\n        public int ManufacturerId { get; set; }\n\n        public string Name { get; set; }\n\n        public virtual IList<Guitar> Models { get; set; }\n    }\n\n    public enum GuitarType\n    {\n        Classical,\n        Acoustic,\n        Electric,\n        Bass\n    }\n}\n\n\nI created the database context with EF code first and implemented a\nconfiguration class to seed the database with some test data.\n\nnamespace GlimpseTest.Models\n{\n    public class GuitarShopContext : DbContext\n    {\n        public GuitarShopContext() : base(\"GuitarShopContext\")\n        {\n        }\n\n        public DbSet<Guitar> Guitars { get; set; }\n\n        public DbSet<Manufacturer> Manufacturers { get; set; }\n    }\n\n    public class AlwaysDropAndCreateInitializer : DropCreateDatabaseAlways<GuitarShopContext>\n    {\n        protected override void Seed(GuitarShopContext context)\n        {\n            base.Seed(context);\n\n            var yamaha = new Manufacturer\n            {\n                Name = \"Yamaha\",\n                Models = new List<Guitar>\n                {\n                    new Guitar\n                    {\n                        Name = \"C40\",\n                        Material = \"Spruce\",\n                        Type = GuitarType.Classical\n                    },\n                    new Guitar\n                    {\n                        Name = \"FS700S\",\n                        Material = \"Rosewood\",\n                        Type = GuitarType.Acoustic\n                    }\n                }\n            };\n\n            var gibson = new Manufacturer\n            {\n                Name = \"Gibson\",\n                Models = new List<Guitar>\n                {\n                    new Guitar\n                    {\n                        Name = \"ES-175\",\n                        Material = \"Maple\",\n                        Type = GuitarType.Electric\n                    }\n                }\n            };\n\n            context.Manufacturers.Add(yamaha);\n            context.Manufacturers.Add(gibson);\n        }\n    }\n}\n\n\nWe need to configure the database initializer in our Application_Start method\nclass in the Global.asax.cs.\n\nDatabase.SetInitializer(new AlwaysDropAndCreateInitializer());\n\n\nThe last thing to do in order to make our data model work is to configure a\nconnection string to the database. I used LocalDB for testing.\n\n<connectionStrings>\n  <add name=\"GuitarShopContext\" providerName=\"System.Data.SqlClient\" connectionString=\"Data Source=(localdb)\\v11.0;Initial Catalog=GuitarShop;Integrated Security=True\" />\n</connectionStrings>\n\n\nControllers\nTo be able to test Glimpse in its merits, we need to create some controllers as\nwell. I implemented basically the same functionality in both an MVC and a Web\nApi controller.\n\nMVC\nThe controller has a single action, that returns the list of manufacturers and\nguitars stored in the database. The controller action does two things.\n\n * Loads the data from the database\n * Imitates a service call to an external service to fetch the prices of the\n   models\n\nThe controller:\n\nnamespace GlimpseTest.Controllers\n{\n    public class GuitarController : Controller\n    {\n        public async Task<ActionResult> Index()\n        {\n            using (var ctx = new GuitarShopContext())\n            {\n                var manufacturers = await ctx.Manufacturers.Include(\"Models\").ToListAsync();\n\n                await this.LoadPrices(manufacturers.SelectMany(m => m.Models));\n\n                return this.View(manufacturers);\n            }\n        }\n\n        private async Task LoadPrices(IEnumerable<Guitar> guitars)\n        {\n            Random rnd = new Random();\n            foreach (var guitar in guitars)\n            {\n                guitar.Price = rnd.Next(100, 500);\n            }\n\n            await Task.Delay(500);\n        }\n    }\n}\n\n\nThe views consists of two parts. Views\\Shared\\DisplayTemplates\\Guitars.cshtml\ncontains the view for displaying a list of guitars.\n\n@model IEnumerable<GlimpseTest.Models.Guitar>\n\n<table class=\"table\">\n    <tr>\n        <th>\n            @Html.DisplayNameFor(model => model.Name)\n        </th>\n        <th>\n            @Html.DisplayNameFor(model => model.Material)\n        </th>\n        <th>\n            @Html.DisplayNameFor(model => model.Price)\n        </th>\n    </tr>\n\n@foreach (var item in Model) {\n    <tr>\n        <td>\n            @Html.DisplayFor(modelItem => item.Name)\n        </td>\n        <td>\n            @Html.DisplayFor(modelItem => item.Material)\n        </td>\n        <td>\n            @Html.DisplayFor(modelItem => item.Price)\n        </td>\n    </tr>\n}\n\n</table>\n\n\nView\\Guitar\\Index.cshtml is the main view for this action.\n\n@model IEnumerable<GlimpseTest.Models.Manufacturer>\n\n@{\n    ViewBag.Title = \"Index\";\n}\n\n<h2>Guitars</h2>\n\n<div>\n    @foreach (var manufacturer in Model)\n    {\n        <h4>\n            @Html.DisplayFor(modelItem => manufacturer.Name)\n        </h4>\n\n        @Html.Partial(\"~/Views/Shared/DisplayTemplates/Guitars.cshtml\", manufacturer.Models);\n    }\n</div>\n\n\nWeb Api\nThe Web Api controller is very similar, but it has a different base class and\nmethod signature:\n\nnamespace GlimpseTest.Controllers\n{\n    public class GuitarApiController : ApiController\n    {\n        public async Task<IHttpActionResult> Get()\n        {\n            using (var ctx = new GuitarShopContext())\n            {\n                var manufacturers = await ctx.Manufacturers.Include(\"Models\").ToListAsync();\n\n                await this.LoadPrices(manufacturers.SelectMany(m => m.Models));\n\n                return this.Ok(manufacturers);\n            }\n        }\n\n        private async Task LoadPrices(IEnumerable<Guitar> guitars)\n        {\n            Random rnd = new Random();\n            foreach (var guitar in guitars)\n            {\n                guitar.Price = rnd.Next(100, 500);\n            }\n\n            await Task.Delay(500);\n        }\n    }\n}\n\n\nThe Web Api controller returns the data in Json directly, so it does not have a\ncorresponding view.\nOne convenient thing to do is to make the Json formatter be the only formatter,\nso we can open endpoints in our browser without having to specify the Accept\nheader. In order to do this, we have to insert the following piece of code in\nthe start of the Register method in our WebApiConfig class:\n\n// Web API configuration and services\nvar jsonFormatter = config.Formatters.JsonFormatter;\nconfig.Formatters.Clear();\nconfig.Formatters.Add(jsonFormatter);\n\n\nThis code removes every formatter, except the Json one.\n\nInstalling Glimpse\nInstalling Glimpse is merely adding a NuGet package to our ASP.NET project. For\nusing with the latest version of MVC, Glimpse can be added with the following\ncommand from the Package Manager Console:\n\nInstall-Package Glimpse.Mvc5\n\n\nSadly, there is no official package for Web Api yet. Official Web Api support is\nsupposed to come with version 2 of Glimpse, but no estimated release date has\nbeen announce. (However, there is active work going on in this area: \nhttps://github.com/Glimpse/Glimpse/issues/715)\n\nUsing Glimpse with MVC\nAfter installing Glimpse this way, we can start using it immediately. To enable\nGlimpse, go to the url /Glimpse.axd, and click on Turn Glimpse On.\n\n\n\nIf we navigate to the index of our guitar controller, we should see the Glimpse\nplugin showing up on the bottom of the page, where we can see a summary of the\ninformation recorded during the request.\n\n\n\nIf we click on the g button in the bottom right hand corner, the full view of\nGlimpse is opened, where we find several tabs showing different kinds of data\ncollected about the request:\n\n\n\nBecause MVC is officially supported, these tabs all work very well out of the\nbox: we can see the order of different events on the Execution tabs, the Routes\ntab shows information about the routes taken into consideration, and the\nTimeline tab displays timing information about the different phases of the\nrequest processing.\nOne thing that's missing but we would expect to see is insight into the SQL\ncommands being executed during the request. This is not recorded out of the box,\nbut luckily, there is a package that does just this. The package for simple\nADO.NET is Glimpse.ADO, but we are using Entity Framework, which needs a\ndifferent package, Glimpse.EF6 (for Entity Framework version six). We can\ninstall it with the following command.\n\nInstall-Package Glimpse.EF6\n\n\nAfter installing the package, we will have an additional SQL tab, on which we\ncan see all the SQL queries executed during the request.\n\n\n\nSo we can see that with ASP.NET MVC, Glimpse works very well out of the box\nwithout any special customization.\n\nUsing Glimpse with Web Api\nLet's try our Web Api controller by opening it in the browser.\n\n\n\nHmm, when we send a request for a Web Api action, instead of a web page, we get\nback the result in its raw form, serialized to Json. So there is no place for\nthe Glimpse plugin to get embedded into. How can we use Glimpse in this\nsituation then?\n\nStandalone Glimpse page\nIn order to be able to use Glimpse independently from the web page itself, it\nalso supports a standalone view, that can also be accessed from its /Glimpse.axd\nconfiguration page.\n\n\n\nOn this page we can see a list of all the requests coming to our application,\nlet them be MVC or Web Api requests.\n\n\n\nWhen we click on the Inspect link, we can view the information collected on the\ndifferent tabs the same way we did with the embedded plugin. However, when we\ninspect a Web Api request, we can see that the following tabs are showing no\ninformation: Execution, Metadata, Routes, Views. In addition to that, the\nTimeline tab doesn't show any Web Api-related information, only the events\nrelated to Entity Framework.\n\nThese are the things that should be supported in the upcoming version of\nGlimpse, probably the unification of the MVC and Web Api data model in ASP.NET 5\nwill make this simpler.\n\nHow to improve?\nThere are at least two features which work just as well with Web Api as MVC:\nTracing and custom Timeline messages.\n\nTracing\nTracing is very easy to use with Glimpse. Any standard .NET trace message will\nbe captured by Glimpse and displayed on the Trace tab.\nThe big advantage of browsing your trace messages with Glimpse is that it shows\nthe trace messages in the context of a single request, so you don't have to\nfilter your whole log in order to correlate the entries with a specific request.\nIf we add some trace messages to our application\n\nTrace.Write(\"Loading manufacturers from the database.\");\n...\nTrace.Write(\"Getting price information\");\n\n\nThey will show up in the Trace tab.\n\n\n\nCustom timeline messages\nThe timeline tab is a great tool to see timing information and to analyze how\nlong different parts of the request processing take. With Web Api the\ninformation displayed out of the box on the Timeline tab is rather limited.\nIt is possible to extend it with custom events, however, this is not as trivial\nas emitting trace messages.\n\nThere is no simple way in the public Glimpse API to emit our own Timeline\nevents, we have to implement a couple of helper classes containing the necessary\nboilerplate code.\n\nLet's add a code file called GlimpseTimeline.cs to our project and implement a\ncouple of classes and methods in the Glimpse.Core namespace.\n\nusing System;\nusing Glimpse.Core.Extensibility;\nusing Glimpse.Core.Framework;\nusing Glimpse.Core.Message;\n\nnamespace Glimpse.Core\n{\n    public class TimelineMessage : ITimelineMessage\n    {\n        public TimelineMessage()\n        {\n            Id = Guid.NewGuid();\n        }\n\n        public Guid Id { get; private set; }\n        public TimeSpan Offset { get; set; }\n        public TimeSpan Duration { get; set; }\n        public DateTime StartTime { get; set; }\n        public string EventName { get; set; }\n        public TimelineCategoryItem EventCategory { get; set; }\n        public string EventSubText { get; set; }\n    }\n\n    public static class GlimpseTimeline\n    {\n        private static readonly TimelineCategoryItem DefaultCategory = new TimelineCategoryItem(\"User\", \"green\", \"blue\");\n\n        public static OngoingCapture Capture(string eventName)\n        {\n            return Capture(eventName, null, DefaultCategory, new TimelineMessage());\n        }\n\n        public static OngoingCapture Capture(string eventName, string eventSubText)\n        {\n            return Capture(eventName, eventSubText, DefaultCategory, new TimelineMessage());\n        }\n\n        internal static OngoingCapture Capture(string eventName, TimelineCategoryItem category)\n        {\n            return Capture(eventName, null, category, new TimelineMessage());\n        }\n\n        internal static OngoingCapture Capture(string eventName, TimelineCategoryItem category, ITimelineMessage message)\n        {\n            return Capture(eventName, null, category, message);\n        }\n\n        internal static OngoingCapture Capture(string eventName, ITimelineMessage message)\n        {\n            return Capture(eventName, null, DefaultCategory, message);\n        }\n\n        internal static OngoingCapture Capture(string eventName, string eventSubText, TimelineCategoryItem category, ITimelineMessage message)\n        {\n            if (string.IsNullOrEmpty(eventName))\n            {\n                throw new ArgumentNullException(\"eventName\");\n            }\n\n#pragma warning disable 618\n            var executionTimer = GlimpseConfiguration.GetConfiguredTimerStrategy()();\n            var messageBroker = GlimpseConfiguration.GetConfiguredMessageBroker();\n#pragma warning restore 618\n\n            if (executionTimer == null || messageBroker == null)\n            {\n                return OngoingCapture.Empty();\n            }\n\n            return new OngoingCapture(executionTimer, messageBroker, eventName, eventSubText, category, message);\n        }\n\n        public static void CaptureMoment(string eventName)\n        {\n            CaptureMoment(eventName, null, DefaultCategory, new TimelineMessage());\n        }\n\n        public static void CaptureMoment(string eventName, string eventSubText)\n        {\n            CaptureMoment(eventName, eventSubText, DefaultCategory, new TimelineMessage());\n        }\n\n        internal static void CaptureMoment(string eventName, TimelineCategoryItem category)\n        {\n            CaptureMoment(eventName, null, category, new TimelineMessage());\n        }\n\n        internal static void CaptureMoment(string eventName, TimelineCategoryItem category, ITimelineMessage message)\n        {\n            CaptureMoment(eventName, null, category, message);\n        }\n\n        internal static void CaptureMoment(string eventName, ITimelineMessage message)\n        {\n            CaptureMoment(eventName, null, DefaultCategory, message);\n        }\n\n        internal static void CaptureMoment(string eventName, string eventSubText, TimelineCategoryItem category, ITimelineMessage message)\n        {\n            if (string.IsNullOrEmpty(eventName))\n            {\n                throw new ArgumentNullException(\"eventName\");\n            }\n\n#pragma warning disable 618\n            var executionTimer = GlimpseConfiguration.GetConfiguredTimerStrategy()();\n            var messageBroker = GlimpseConfiguration.GetConfiguredMessageBroker();\n#pragma warning restore 618\n\n            if (executionTimer == null || messageBroker == null)\n            {\n                return;\n            }\n\n            message\n                .AsTimelineMessage(eventName, category, eventSubText)\n                .AsTimedMessage(executionTimer.Point());\n\n            messageBroker.Publish(message);\n        }\n\n        public class OngoingCapture : IDisposable\n        {\n            public static OngoingCapture Empty()\n            {\n                return new NullOngoingCapture();\n            }\n\n            private OngoingCapture()\n            {\n            }\n\n            public OngoingCapture(IExecutionTimer executionTimer, IMessageBroker messageBroker, string eventName, string eventSubText, TimelineCategoryItem category, ITimelineMessage message)\n            {\n                Offset = executionTimer.Start();\n                ExecutionTimer = executionTimer;\n                Message = message.AsTimelineMessage(eventName, category, eventSubText);\n                MessageBroker = messageBroker;\n            }\n\n            private ITimelineMessage Message { get; set; }\n\n            private TimeSpan Offset { get; set; }\n\n            private IExecutionTimer ExecutionTimer { get; set; }\n\n            private IMessageBroker MessageBroker { get; set; }\n\n            public virtual void Stop()\n            {\n                var timerResult = ExecutionTimer.Stop(Offset);\n\n                MessageBroker.Publish(Message.AsTimedMessage(timerResult));\n            }\n\n            public void Dispose()\n            {\n                Stop();\n            }\n\n            private class NullOngoingCapture : OngoingCapture\n            {\n                public override void Stop()\n                {\n                }\n            }\n        }\n    }\n}\n\n\n(There are several slightly different versions of this code around on the\ninternet, I took this version from the following gist: \nhttps://gist.github.com/johnjuuljensen/776e61b720c2a7e5b6ef)\n\nWith this helper logic we can add our own timing events to our controller.\n\nusing Glimpse.Core;\n...\nList<Manufacturer> manufacturers;\nusing (GlimpseTimeline.Capture(\"Loading from DB\"))\n{\n    manufacturers = await ctx.Manufacturers.Include(\"Models\").ToListAsync();\n}\n\nusing (GlimpseTimeline.Capture(\"Fetching product prices\"))\n{\n    await this.LoadPrices(manufacturers.SelectMany(m => m.Models));\n}\n\n\nOur custom timeline events will show up in the timeline tab.\n\n\n\nIf you'd like to take a look at it, you can find the whole source code on GitHub\n[https://github.com/markvincze/glimpse-web-api].\n\nConclusion\nWe've seen that setting up Glimpse for the Web Api is just as easy as using it\nwith MVC, however, the feature set supported is much more limited. Still,\nGlimpse can be a very useful tool to analyze what's going on under the hood in a\nWeb Api application as well.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": "Using Glimpse with ASP.NET Web Api",
        "meta_description": "Glimpse is not fully supported for the Web Api, but it can still be a valuable tool.",
        "author_id": "1",
        "created_at": "2015-05-04 19:43:42",
        "created_by": "1",
        "updated_at": "2018-02-17 11:57:39",
        "updated_by": "1",
        "published_at": "2015-05-14 12:56:50",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de295f",
        "uuid": "b578904e-6a9c-47f5-910b-eea2b5a69a02",
        "title": "How to store state during SpecFlow tests?",
        "slug": "how-to-store-state-during-specflow-tests",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"#Introduction\\r\\n[SpecFlow](http://www.specflow.org/) is an implementation of the [Gherkin](https://github.com/cucumber/cucumber/wiki/Gherkin) language for the .NET Framework. SpecFlow is to .NET what [Cucumber](https://cucumber.io/) is for the JavaScript ecosystem.\\r\\nIt is a way to write tests in a DSL that is easily readable (and maybe writable) by not just developers, but also the business. A simple example from the Cucumber web site (which is also generated when a new SpecFlow feature is added in Visual Studio) is the following:\\r\\n\\r\\n```gherkin\\r\\nFeature: Addition\\r\\n\\r\\nScenario Outline: Add two numbers\\r\\n    Given I have entered 50 into the calculator\\r\\n    And I have entered 70 into the calculator\\r\\n    When I press add\\r\\n    Then the result should be 120 on the screen\\r\\n```\\r\\n\\r\\nWhat SpecFlow does is that it generates some C# code based on the Gherkin test descriptions, therefore implementing unit tests which will try to execute the steps in the feature description.\\r\\nAll the steps must have a corresponding method implementing the step in a binding class.\\r\\nThe binding class implementing the above steps could look like this:\\r\\n\\r\\n```csharp\\r\\n[Binding]\\r\\npublic class CalculatorTestSteps\\r\\n{\\r\\n    private readonly Calculator calculator = new Calculator();\\r\\n\\r\\n    [Given(@\\\"I have entered (\\\\d+) into the calculator\\\")]\\r\\n\\r\\n    public void GivenIHaveEnteredIntoTheCalculator(int number)\\r\\n    {\\r\\n        this.calculator.Push(number);\\r\\n    }\\r\\n\\r\\n    [When(@\\\"I press add\\\")]\\r\\n    public void GivenIPressAdd()\\r\\n    {\\r\\n        this.calculator.Add();\\r\\n    }\\r\\n\\r\\n    [Then(@\\\"the result should be (\\\\d+) on the screen\\\")]\\r\\n    public void ThenTheResultShouldBeOnTheScreen(int result)\\r\\n    {\\r\\n        Assert.AreEqual(result, this.calculator.result);\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nSpecFlow generates a unit test based on the feature definition, which basically instantiates the above class, and executes the instance methods corresponding to the feature steps.\\r\\nThe unit tests themselves can be run with the test runner of our choice (for instance the built-in VS test runner or ReSharper), and assertions can be implemented with the usual Assert mechanism of the test framework (which can be either MSTest or NUnit).\\r\\n\\r\\n#Maintaining state during a test\\r\\n\\r\\nMost of the time we need to store and maintain some kind of state during a test. For instance in the Given step we initialize some input data, in the When step we use that input data to send a request to a service and then save the result, and in the When step we execute assertions on the result.\\r\\nThere are different ways to store data in SpecFlow steps. In the rest of the post I will introduce three different ways to achieve this.\\r\\n\\r\\n## Using a private field\\r\\n\\r\\nThe simplest solution is what I used in the Calculator example. Simply create a field in the binding class, and store data in that field, which can be used in all of the steps. A pseudocode of a test sending a GET HTTP request and verifying the returned status code using this technique looks like this:\\r\\n\\r\\n```csharp\\r\\n[Binding]\\r\\npublic class ServiceTestSteps\\r\\n{\\r\\n    private IRestResponse response;\\r\\n\\r\\n    [When(@\\\"I send the request\\\")]\\r\\n    public void WhenISendTheRequest()\\r\\n    {\\r\\n        this.response = new RestClient().Execute(new RestRequest(\\\"http://my-service.com\\\", Method.GET));\\r\\n    }\\r\\n\\r\\n    [Then(@\\\"the status code should be OK\\\")]\\r\\n    public void ThenTheStatusCodeShouldBeOK()\\r\\n    {\\r\\n        Assert.AreEqual(HttpStatusCode.OK, this.response.StatusCode);\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nThe above solution is very simple and it requires the least amount of code, but it has one significant drawback: if we implement a feature, which has steps implemented in different binding classes, the state won't be shared among those classes, since the binding class instances won't see the each other during the feature execution.\\r\\n\\r\\n## Using the ScenarioContext\\r\\n`ScenarioContext` is a static class that has a shared state during the execution of a scenario. It can be freely used as a dictionary, either storing an object of a specific type without a key, or explicitly specifying a string key. The `ScenarioContext` can be accessed by all the binding classes involved, so they can share the state.  \\r\\nThe implementation of the same test using this approach is the following.\\r\\n\\r\\n```csharp\\r\\n[Binding]\\r\\npublic class ServiceTestSteps\\r\\n{\\r\\n    [When(@\\\"I send the request\\\")]\\r\\n    public void WhenISendTheRequest()\\r\\n    {\\r\\n        ScenarioContext.Current.Set<IRestResponse>(new RestClient().Execute(new RestRequest(\\\"http://my-service.com\\\", Method.GET)));\\r\\n    }\\r\\n\\r\\n    [Then(@\\\"the status code should be OK\\\")]\\r\\n    public void ThenTheStatusCodeShouldBeOK(int result)\\r\\n    {\\r\\n        Assert.AreEqual(HttpStatusCode.OK, ScenarioContext.Current.Get<IRestResponse>().StatusCode);\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nThe drawback of using the `ScenarioContext` is that the contract between the different steps will not be explicit. If we change the data stored in one step we have to update all of the other steps which depend on it, and we won't get any compilation errors if we don't do so.\\r\\nThe situation is even worse if we store multiple instances of the same type and have to use and maintain hard-coded string keys.\\r\\nThe last solution solves this problem, but ??? as usual ??? it requires writing the most amount of code.\\r\\n\\r\\n## Injecting a context instance\\r\\n\\r\\nThe SpecFlow engine supports injecting objects into the instantiated binding classes through their constructor. We just simply have to add constructor parameters to the binding classes, the rest will be taken care of by SpecFlow.  \\r\\nIt also makes sure that even if multiple binding classes have a constructor parameter of the same type, only a single instance of the said type will be created during the execution of a feature, so this feature is ideal for sharing state.\\r\\n\\r\\n```csharp\\r\\npublic class ServiceTestContext\\r\\n{\\r\\n\\tpublic IRestResponse Response { get; set; }\\r\\n}\\r\\n\\r\\n[Binding]\\r\\npublic class ServiceTestSteps\\r\\n{\\r\\n    private readonly ServiceTestContext context;\\r\\n\\r\\n    ServiceTestSteps(ServiceTestContext context)\\r\\n    {\\r\\n        this.context = context;\\r\\n    }\\r\\n\\r\\n    [When(@\\\"I send the request\\\")]\\r\\n    public void WhenISendTheRequest()\\r\\n    {\\r\\n        context.Response = new RestClient().Execute(new RestRequest(\\\"http://my-service.com\\\", Method.GET));\\r\\n    }\\r\\n\\r\\n    [Then(@\\\"the status code should be OK\\\")]\\r\\n    public void ThenTheStatusCodeShouldBeOK()\\r\\n    {\\r\\n        Assert.AreEqual(HttpStatusCode.OK, context.Response.StatusCode);\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nThis context instance can be shared simply by adding it to another class as a constructor parameter.\\r\\n\\r\\n```csharp\\r\\n[Binding]\\r\\npublic class FruitTestSteps\\r\\n{\\r\\n    private readonly ServiceTestContext context;\\r\\n\\r\\n    ServiceTestSteps(ServiceTestContext context)\\r\\n    {\\r\\n        this.context = context;\\r\\n    }\\r\\n\\r\\n    [Then(@\\\"the response should contain \\\"\\\"(.*)\\\"\\\"\\\")]\\r\\n    public void ThenTheResponseShouldContain(string fruit)\\r\\n    {\\r\\n        Assert.IsTrue(this.context.Response.Content.Contains(fruit))\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nThen we are able to write a feature which uses steps from both of the binding classes:\\r\\n\\r\\n```gherkin\\r\\nScenario: Service response should contain pear\\r\\n\\tWhen I send the request\\r\\n\\tThen the status code should be OK\\r\\n\\tAnd the response should contain \\\"pear\\\"\\r\\n```\\r\\n##Conclusion\\r\\nThere are multiple ways of storing state in a Specflow feature, all of which have different benefits and drawbacks.\\r\\nIf we only need the state in a single binding class, we can simply store it as a field of that class.\\r\\nOn the other hand if we need to share the state between multiple binding classes, we have two choices: if we're looking for a quick and dirty solution, the current instance of the ScenarioContext can be used as a dictionary, however, if we are willing to invest writing a bit more code to improve maintainability, we can extract the state to be stored to an external context class, which can be injected into all of the binding classes which need to access the shared data.\"}]],\"sections\":[[10,0]]}",
        "html": "<h1 id=\"introduction\">Introduction</h1>\n<p><a href=\"http://www.specflow.org/\">SpecFlow</a> is an implementation of the <a href=\"https://github.com/cucumber/cucumber/wiki/Gherkin\">Gherkin</a> language for the .NET Framework. SpecFlow is to .NET what <a href=\"https://cucumber.io/\">Cucumber</a> is for the JavaScript ecosystem.<br>\nIt is a way to write tests in a DSL that is easily readable (and maybe writable) by not just developers, but also the business. A simple example from the Cucumber web site (which is also generated when a new SpecFlow feature is added in Visual Studio) is the following:</p>\n<pre><code class=\"language-gherkin\">Feature: Addition\n\nScenario Outline: Add two numbers\n    Given I have entered 50 into the calculator\n    And I have entered 70 into the calculator\n    When I press add\n    Then the result should be 120 on the screen\n</code></pre>\n<p>What SpecFlow does is that it generates some C# code based on the Gherkin test descriptions, therefore implementing unit tests which will try to execute the steps in the feature description.<br>\nAll the steps must have a corresponding method implementing the step in a binding class.<br>\nThe binding class implementing the above steps could look like this:</p>\n<pre><code class=\"language-csharp\">[Binding]\npublic class CalculatorTestSteps\n{\n    private readonly Calculator calculator = new Calculator();\n\n    [Given(@&quot;I have entered (\\d+) into the calculator&quot;)]\n\n    public void GivenIHaveEnteredIntoTheCalculator(int number)\n    {\n        this.calculator.Push(number);\n    }\n\n    [When(@&quot;I press add&quot;)]\n    public void GivenIPressAdd()\n    {\n        this.calculator.Add();\n    }\n\n    [Then(@&quot;the result should be (\\d+) on the screen&quot;)]\n    public void ThenTheResultShouldBeOnTheScreen(int result)\n    {\n        Assert.AreEqual(result, this.calculator.result);\n    }\n}\n</code></pre>\n<p>SpecFlow generates a unit test based on the feature definition, which basically instantiates the above class, and executes the instance methods corresponding to the feature steps.<br>\nThe unit tests themselves can be run with the test runner of our choice (for instance the built-in VS test runner or ReSharper), and assertions can be implemented with the usual Assert mechanism of the test framework (which can be either MSTest or NUnit).</p>\n<h1 id=\"maintainingstateduringatest\">Maintaining state during a test</h1>\n<p>Most of the time we need to store and maintain some kind of state during a test. For instance in the Given step we initialize some input data, in the When step we use that input data to send a request to a service and then save the result, and in the When step we execute assertions on the result.<br>\nThere are different ways to store data in SpecFlow steps. In the rest of the post I will introduce three different ways to achieve this.</p>\n<h2 id=\"usingaprivatefield\">Using a private field</h2>\n<p>The simplest solution is what I used in the Calculator example. Simply create a field in the binding class, and store data in that field, which can be used in all of the steps. A pseudocode of a test sending a GET HTTP request and verifying the returned status code using this technique looks like this:</p>\n<pre><code class=\"language-csharp\">[Binding]\npublic class ServiceTestSteps\n{\n    private IRestResponse response;\n\n    [When(@&quot;I send the request&quot;)]\n    public void WhenISendTheRequest()\n    {\n        this.response = new RestClient().Execute(new RestRequest(&quot;http://my-service.com&quot;, Method.GET));\n    }\n\n    [Then(@&quot;the status code should be OK&quot;)]\n    public void ThenTheStatusCodeShouldBeOK()\n    {\n        Assert.AreEqual(HttpStatusCode.OK, this.response.StatusCode);\n    }\n}\n</code></pre>\n<p>The above solution is very simple and it requires the least amount of code, but it has one significant drawback: if we implement a feature, which has steps implemented in different binding classes, the state won't be shared among those classes, since the binding class instances won't see the each other during the feature execution.</p>\n<h2 id=\"usingthescenariocontext\">Using the ScenarioContext</h2>\n<p><code>ScenarioContext</code> is a static class that has a shared state during the execution of a scenario. It can be freely used as a dictionary, either storing an object of a specific type without a key, or explicitly specifying a string key. The <code>ScenarioContext</code> can be accessed by all the binding classes involved, so they can share the state.<br>\nThe implementation of the same test using this approach is the following.</p>\n<pre><code class=\"language-csharp\">[Binding]\npublic class ServiceTestSteps\n{\n    [When(@&quot;I send the request&quot;)]\n    public void WhenISendTheRequest()\n    {\n        ScenarioContext.Current.Set&lt;IRestResponse&gt;(new RestClient().Execute(new RestRequest(&quot;http://my-service.com&quot;, Method.GET)));\n    }\n\n    [Then(@&quot;the status code should be OK&quot;)]\n    public void ThenTheStatusCodeShouldBeOK(int result)\n    {\n        Assert.AreEqual(HttpStatusCode.OK, ScenarioContext.Current.Get&lt;IRestResponse&gt;().StatusCode);\n    }\n}\n</code></pre>\n<p>The drawback of using the <code>ScenarioContext</code> is that the contract between the different steps will not be explicit. If we change the data stored in one step we have to update all of the other steps which depend on it, and we won't get any compilation errors if we don't do so.<br>\nThe situation is even worse if we store multiple instances of the same type and have to use and maintain hard-coded string keys.<br>\nThe last solution solves this problem, but ??? as usual ??? it requires writing the most amount of code.</p>\n<h2 id=\"injectingacontextinstance\">Injecting a context instance</h2>\n<p>The SpecFlow engine supports injecting objects into the instantiated binding classes through their constructor. We just simply have to add constructor parameters to the binding classes, the rest will be taken care of by SpecFlow.<br>\nIt also makes sure that even if multiple binding classes have a constructor parameter of the same type, only a single instance of the said type will be created during the execution of a feature, so this feature is ideal for sharing state.</p>\n<pre><code class=\"language-csharp\">public class ServiceTestContext\n{\n\tpublic IRestResponse Response { get; set; }\n}\n\n[Binding]\npublic class ServiceTestSteps\n{\n    private readonly ServiceTestContext context;\n\n    ServiceTestSteps(ServiceTestContext context)\n    {\n        this.context = context;\n    }\n\n    [When(@&quot;I send the request&quot;)]\n    public void WhenISendTheRequest()\n    {\n        context.Response = new RestClient().Execute(new RestRequest(&quot;http://my-service.com&quot;, Method.GET));\n    }\n\n    [Then(@&quot;the status code should be OK&quot;)]\n    public void ThenTheStatusCodeShouldBeOK()\n    {\n        Assert.AreEqual(HttpStatusCode.OK, context.Response.StatusCode);\n    }\n}\n</code></pre>\n<p>This context instance can be shared simply by adding it to another class as a constructor parameter.</p>\n<pre><code class=\"language-csharp\">[Binding]\npublic class FruitTestSteps\n{\n    private readonly ServiceTestContext context;\n\n    ServiceTestSteps(ServiceTestContext context)\n    {\n        this.context = context;\n    }\n\n    [Then(@&quot;the response should contain &quot;&quot;(.*)&quot;&quot;&quot;)]\n    public void ThenTheResponseShouldContain(string fruit)\n    {\n        Assert.IsTrue(this.context.Response.Content.Contains(fruit))\n    }\n}\n</code></pre>\n<p>Then we are able to write a feature which uses steps from both of the binding classes:</p>\n<pre><code class=\"language-gherkin\">Scenario: Service response should contain pear\n\tWhen I send the request\n\tThen the status code should be OK\n\tAnd the response should contain &quot;pear&quot;\n</code></pre>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>There are multiple ways of storing state in a Specflow feature, all of which have different benefits and drawbacks.<br>\nIf we only need the state in a single binding class, we can simply store it as a field of that class.<br>\nOn the other hand if we need to share the state between multiple binding classes, we have two choices: if we're looking for a quick and dirty solution, the current instance of the ScenarioContext can be used as a dictionary, however, if we are willing to invest writing a bit more code to improve maintainability, we can extract the state to be stored to an external context class, which can be injected into all of the binding classes which need to access the shared data.</p>\n",
        "comment_id": "4",
        "plaintext": "Introduction\nSpecFlow [http://www.specflow.org/]  is an implementation of the Gherkin\n[https://github.com/cucumber/cucumber/wiki/Gherkin]  language for the .NET\nFramework. SpecFlow is to .NET what Cucumber [https://cucumber.io/]  is for the\nJavaScript ecosystem.\nIt is a way to write tests in a DSL that is easily readable (and maybe writable)\nby not just developers, but also the business. A simple example from the\nCucumber web site (which is also generated when a new SpecFlow feature is added\nin Visual Studio) is the following:\n\nFeature: Addition\n\nScenario Outline: Add two numbers\n    Given I have entered 50 into the calculator\n    And I have entered 70 into the calculator\n    When I press add\n    Then the result should be 120 on the screen\n\n\nWhat SpecFlow does is that it generates some C# code based on the Gherkin test\ndescriptions, therefore implementing unit tests which will try to execute the\nsteps in the feature description.\nAll the steps must have a corresponding method implementing the step in a\nbinding class.\nThe binding class implementing the above steps could look like this:\n\n[Binding]\npublic class CalculatorTestSteps\n{\n    private readonly Calculator calculator = new Calculator();\n\n    [Given(@\"I have entered (\\d+) into the calculator\")]\n\n    public void GivenIHaveEnteredIntoTheCalculator(int number)\n    {\n        this.calculator.Push(number);\n    }\n\n    [When(@\"I press add\")]\n    public void GivenIPressAdd()\n    {\n        this.calculator.Add();\n    }\n\n    [Then(@\"the result should be (\\d+) on the screen\")]\n    public void ThenTheResultShouldBeOnTheScreen(int result)\n    {\n        Assert.AreEqual(result, this.calculator.result);\n    }\n}\n\n\nSpecFlow generates a unit test based on the feature definition, which basically\ninstantiates the above class, and executes the instance methods corresponding to\nthe feature steps.\nThe unit tests themselves can be run with the test runner of our choice (for\ninstance the built-in VS test runner or ReSharper), and assertions can be\nimplemented with the usual Assert mechanism of the test framework (which can be\neither MSTest or NUnit).\n\nMaintaining state during a test\nMost of the time we need to store and maintain some kind of state during a test.\nFor instance in the Given step we initialize some input data, in the When step\nwe use that input data to send a request to a service and then save the result,\nand in the When step we execute assertions on the result.\nThere are different ways to store data in SpecFlow steps. In the rest of the\npost I will introduce three different ways to achieve this.\n\nUsing a private field\nThe simplest solution is what I used in the Calculator example. Simply create a\nfield in the binding class, and store data in that field, which can be used in\nall of the steps. A pseudocode of a test sending a GET HTTP request and\nverifying the returned status code using this technique looks like this:\n\n[Binding]\npublic class ServiceTestSteps\n{\n    private IRestResponse response;\n\n    [When(@\"I send the request\")]\n    public void WhenISendTheRequest()\n    {\n        this.response = new RestClient().Execute(new RestRequest(\"http://my-service.com\", Method.GET));\n    }\n\n    [Then(@\"the status code should be OK\")]\n    public void ThenTheStatusCodeShouldBeOK()\n    {\n        Assert.AreEqual(HttpStatusCode.OK, this.response.StatusCode);\n    }\n}\n\n\nThe above solution is very simple and it requires the least amount of code, but\nit has one significant drawback: if we implement a feature, which has steps\nimplemented in different binding classes, the state won't be shared among those\nclasses, since the binding class instances won't see the each other during the\nfeature execution.\n\nUsing the ScenarioContext\nScenarioContext  is a static class that has a shared state during the execution\nof a scenario. It can be freely used as a dictionary, either storing an object\nof a specific type without a key, or explicitly specifying a string key. The \nScenarioContext  can be accessed by all the binding classes involved, so they\ncan share the state.\nThe implementation of the same test using this approach is the following.\n\n[Binding]\npublic class ServiceTestSteps\n{\n    [When(@\"I send the request\")]\n    public void WhenISendTheRequest()\n    {\n        ScenarioContext.Current.Set<IRestResponse>(new RestClient().Execute(new RestRequest(\"http://my-service.com\", Method.GET)));\n    }\n\n    [Then(@\"the status code should be OK\")]\n    public void ThenTheStatusCodeShouldBeOK(int result)\n    {\n        Assert.AreEqual(HttpStatusCode.OK, ScenarioContext.Current.Get<IRestResponse>().StatusCode);\n    }\n}\n\n\nThe drawback of using the ScenarioContext  is that the contract between the\ndifferent steps will not be explicit. If we change the data stored in one step\nwe have to update all of the other steps which depend on it, and we won't get\nany compilation errors if we don't do so.\nThe situation is even worse if we store multiple instances of the same type and\nhave to use and maintain hard-coded string keys.\nThe last solution solves this problem, but ??? as usual ??? it requires writing\nthe most amount of code.\n\nInjecting a context instance\nThe SpecFlow engine supports injecting objects into the instantiated binding\nclasses through their constructor. We just simply have to add constructor\nparameters to the binding classes, the rest will be taken care of by SpecFlow.\nIt also makes sure that even if multiple binding classes have a constructor\nparameter of the same type, only a single instance of the said type will be\ncreated during the execution of a feature, so this feature is ideal for sharing\nstate.\n\npublic class ServiceTestContext\n{\n\tpublic IRestResponse Response { get; set; }\n}\n\n[Binding]\npublic class ServiceTestSteps\n{\n    private readonly ServiceTestContext context;\n\n    ServiceTestSteps(ServiceTestContext context)\n    {\n        this.context = context;\n    }\n\n    [When(@\"I send the request\")]\n    public void WhenISendTheRequest()\n    {\n        context.Response = new RestClient().Execute(new RestRequest(\"http://my-service.com\", Method.GET));\n    }\n\n    [Then(@\"the status code should be OK\")]\n    public void ThenTheStatusCodeShouldBeOK()\n    {\n        Assert.AreEqual(HttpStatusCode.OK, context.Response.StatusCode);\n    }\n}\n\n\nThis context instance can be shared simply by adding it to another class as a\nconstructor parameter.\n\n[Binding]\npublic class FruitTestSteps\n{\n    private readonly ServiceTestContext context;\n\n    ServiceTestSteps(ServiceTestContext context)\n    {\n        this.context = context;\n    }\n\n    [Then(@\"the response should contain \"\"(.*)\"\"\")]\n    public void ThenTheResponseShouldContain(string fruit)\n    {\n        Assert.IsTrue(this.context.Response.Content.Contains(fruit))\n    }\n}\n\n\nThen we are able to write a feature which uses steps from both of the binding\nclasses:\n\nScenario: Service response should contain pear\n\tWhen I send the request\n\tThen the status code should be OK\n\tAnd the response should contain \"pear\"\n\n\nConclusion\nThere are multiple ways of storing state in a Specflow feature, all of which\nhave different benefits and drawbacks.\nIf we only need the state in a single binding class, we can simply store it as a\nfield of that class.\nOn the other hand if we need to share the state between multiple binding\nclasses, we have two choices: if we're looking for a quick and dirty solution,\nthe current instance of the ScenarioContext can be used as a dictionary,\nhowever, if we are willing to invest writing a bit more code to improve\nmaintainability, we can extract the state to be stored to an external context\nclass, which can be injected into all of the binding classes which need to\naccess the shared data.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": "",
        "meta_description": "There are different ways to store state during SpecFlow tests, and all of them have benefits and drawbacks.",
        "author_id": "1",
        "created_at": "2015-06-06 17:01:05",
        "created_by": "1",
        "updated_at": "2015-08-15 17:29:12",
        "updated_by": "1",
        "published_at": "2015-06-06 19:19:58",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2960",
        "uuid": "f960d479-5105-451e-adf4-ed5a34c83037",
        "title": "Back to basics: Dictionary part 1, hash tables",
        "slug": "back-to-basics-dictionary-part-1",
        "mobiledoc": "{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"**Posts in this series**:\\n\\n1. [Part 1: Hash tables](/back-to-basics-dictionary-part-1)\\n2. [Part 2: .NET implementation](/back-to-basics-dictionary-part-2-net-implementation)\\n3. [Part 3: Built-in GetHashCode](/back-to-basics-dictionary-part-3-built-in-gethashcode)\\n4. [Part 4: Custom GetHashCode](/back-to-basics-dictionary-part-4-custom-gethashcode)\\n\\n# Introduction\\n\\nRecently I came across a situation in which I should have known the details about how a .NET Dictionary (and hashmaps in general) worked under the hood.\\nI realized that my knowledge about this topic was a bit rusty, so I decided I'd refresh my memories and look into this topic.\\nIn the first part of these posts we'll take a quick look into how a hash map works in general, and how those concepts are implemented in the .NET framework.\\n\\n# Hash tables\\n\\nA hash table (or hash map) is a data structure implementing one of the most fundamental containers: the associative array (1). The associative array is a structure, which maps keys to values. The basic operation of such a container is the *lookup*, which - as the name suggests - looks up a value based on its key.\\nThe hash map is implemented in a way to make this lookup operation as quick as possible, ideally needing only a constant amount of time.\\n\\n## Mechanisms\\n\\nI'll quickly introduce the most important mechanisms of the kind of hash map which is implemented in .NET.\\nThe hash table contains a list of *buckets*, and in those buckets it stores one or more *values*. A *hash function* is used to compute an *index* based on the *key*. When we insert an item into our container, it will be added to the bucket designated by the calculated index.\\nLikewise, when we are doing a lookup by the key, we can also calculate this index, and we will know the bucket in which we have to look for our item.\\nIdeally, the hash function will assign each key to a unique bucket, so that all buckets contain only a single element. This would mean that our lookup operation is really constant in its run-time, since it has to calculate the hash, and then it has to get the first (and only) item from the appropriate bucket.\\n\\nThe following image illustrates such a container, which stores some books, where the key is the title of the book (assuming that all the books we'd like to store in this case have unique titles).\\n\\n![Structure of a basic hash map](/content/images/2015/07/Basic-hash-map.png)\\n\\nBut it is possible that the hash function generates an identical hash for two different keys, so most hash table designs assume that collisions can and will happen.\\nWhen a collision happen, we will store more than a single elements in a bucket, which will affect the time needed to do a lookup. In order to make the lookup fast, have to keep the ratio of the number of items and the number of buckets in our container low. This number is an essential property of a hash map, and is usually called the *load factor*.\\n\\n## Handling collisions\\n\\nOne of the most important concepts in the implementation of a hash map is the way how we handle when two different keys end up having the same hash value, thus their index will point to the same bucket.\\nThere are many different approaches, but arguably two of them are the most prominent.\\n\\n### Separate chaining\\n\\nWhen using separate chaining, each bucket is handled independently, they all store a list of some kind with all the items having that same key. In this structure, the time needed to find an item is the time needed to calculate the index based on the key, plus the time needed to find the item in the list.\\nBecause of this, ideally most of the lists should contain zero or one item, and some of them two or maybe three, but not many more. This way, the time needed for a lookup can be kept low. However, in order to achieve this, the load factor cannot be too high, so we have to adjust the number of buckets to the number of items being stored.\\nThe following image illustrates a hash map using separate chaining to handle a collision.\\n\\n![Separate chaining with one collision](/content/images/2015/07/Hash-map-sep-chaining.png)\\n\\nIn order to do separate chaining, every bucket should contain a list of items. This can be implemented in various ways, probably the simplest is to store a linked list in every bucket.\\n\\n### Open addressing\\n\\nIn open addressing, the bucket list is simply an array of items, and all the items are stored in this array. This means that if we want to insert an item, but the bucket of the item's key is already filled, we have to store the item in another bucket.\\nIn order to find this \\\"another\\\" bucket to use, a so called *probe sequence* is used, which defines how we find the alternative position for our item. The simplest of such probe sequences is the linear probing, in which we simply start looking at the buckets following the originally designated position, until we find a free slot.\\n\\n![Open addressing with one collision](/content/images/2015/07/Hash-map-open-addressing.png)\\n\\nWhen we are doing a lookup, if the item in the designated bucket has a different key than what we were looking for, we have to continue looking in the buckets according to the probe sequence until we either find our item, or find an empty bucket (which means that the item is not in the hash map).\\n\\n## Additional details\\n\\nThe above summary was very brief and left out many important details. If you are interested in more information about this topic, a good starting point is the [Hash table](https://en.wikipedia.org/wiki/Hash_table) Wikipedia article (on which most parts of this introduction are based).\\n\\n# Next up\\n\\nIn the [next post](/back-to-basics-dictionary-part-2-net-implementation) I'll look at how these concepts are implemented in the Dictionary class of the .NET framework.\"}]],\"markups\":[],\"sections\":[[10,0]]}",
        "html": "<p><strong>Posts in this series</strong>:</p>\n<ol>\n<li><a href=\"/back-to-basics-dictionary-part-1\">Part 1: Hash tables</a></li>\n<li><a href=\"/back-to-basics-dictionary-part-2-net-implementation\">Part 2: .NET implementation</a></li>\n<li><a href=\"/back-to-basics-dictionary-part-3-built-in-gethashcode\">Part 3: Built-in GetHashCode</a></li>\n<li><a href=\"/back-to-basics-dictionary-part-4-custom-gethashcode\">Part 4: Custom GetHashCode</a></li>\n</ol>\n<h1 id=\"introduction\">Introduction</h1>\n<p>Recently I came across a situation in which I should have known the details about how a .NET Dictionary (and hashmaps in general) worked under the hood.<br>\nI realized that my knowledge about this topic was a bit rusty, so I decided I'd refresh my memories and look into this topic.<br>\nIn the first part of these posts we'll take a quick look into how a hash map works in general, and how those concepts are implemented in the .NET framework.</p>\n<h1 id=\"hashtables\">Hash tables</h1>\n<p>A hash table (or hash map) is a data structure implementing one of the most fundamental containers: the associative array (1). The associative array is a structure, which maps keys to values. The basic operation of such a container is the <em>lookup</em>, which - as the name suggests - looks up a value based on its key.<br>\nThe hash map is implemented in a way to make this lookup operation as quick as possible, ideally needing only a constant amount of time.</p>\n<h2 id=\"mechanisms\">Mechanisms</h2>\n<p>I'll quickly introduce the most important mechanisms of the kind of hash map which is implemented in .NET.<br>\nThe hash table contains a list of <em>buckets</em>, and in those buckets it stores one or more <em>values</em>. A <em>hash function</em> is used to compute an <em>index</em> based on the <em>key</em>. When we insert an item into our container, it will be added to the bucket designated by the calculated index.<br>\nLikewise, when we are doing a lookup by the key, we can also calculate this index, and we will know the bucket in which we have to look for our item.<br>\nIdeally, the hash function will assign each key to a unique bucket, so that all buckets contain only a single element. This would mean that our lookup operation is really constant in its run-time, since it has to calculate the hash, and then it has to get the first (and only) item from the appropriate bucket.</p>\n<p>The following image illustrates such a container, which stores some books, where the key is the title of the book (assuming that all the books we'd like to store in this case have unique titles).</p>\n<p><img src=\"/content/images/2015/07/Basic-hash-map.png\" alt=\"Structure of a basic hash map\"></p>\n<p>But it is possible that the hash function generates an identical hash for two different keys, so most hash table designs assume that collisions can and will happen.<br>\nWhen a collision happen, we will store more than a single elements in a bucket, which will affect the time needed to do a lookup. In order to make the lookup fast, have to keep the ratio of the number of items and the number of buckets in our container low. This number is an essential property of a hash map, and is usually called the <em>load factor</em>.</p>\n<h2 id=\"handlingcollisions\">Handling collisions</h2>\n<p>One of the most important concepts in the implementation of a hash map is the way how we handle when two different keys end up having the same hash value, thus their index will point to the same bucket.<br>\nThere are many different approaches, but arguably two of them are the most prominent.</p>\n<h3 id=\"separatechaining\">Separate chaining</h3>\n<p>When using separate chaining, each bucket is handled independently, they all store a list of some kind with all the items having that same key. In this structure, the time needed to find an item is the time needed to calculate the index based on the key, plus the time needed to find the item in the list.<br>\nBecause of this, ideally most of the lists should contain zero or one item, and some of them two or maybe three, but not many more. This way, the time needed for a lookup can be kept low. However, in order to achieve this, the load factor cannot be too high, so we have to adjust the number of buckets to the number of items being stored.<br>\nThe following image illustrates a hash map using separate chaining to handle a collision.</p>\n<p><img src=\"/content/images/2015/07/Hash-map-sep-chaining.png\" alt=\"Separate chaining with one collision\"></p>\n<p>In order to do separate chaining, every bucket should contain a list of items. This can be implemented in various ways, probably the simplest is to store a linked list in every bucket.</p>\n<h3 id=\"openaddressing\">Open addressing</h3>\n<p>In open addressing, the bucket list is simply an array of items, and all the items are stored in this array. This means that if we want to insert an item, but the bucket of the item's key is already filled, we have to store the item in another bucket.<br>\nIn order to find this &quot;another&quot; bucket to use, a so called <em>probe sequence</em> is used, which defines how we find the alternative position for our item. The simplest of such probe sequences is the linear probing, in which we simply start looking at the buckets following the originally designated position, until we find a free slot.</p>\n<p><img src=\"/content/images/2015/07/Hash-map-open-addressing.png\" alt=\"Open addressing with one collision\"></p>\n<p>When we are doing a lookup, if the item in the designated bucket has a different key than what we were looking for, we have to continue looking in the buckets according to the probe sequence until we either find our item, or find an empty bucket (which means that the item is not in the hash map).</p>\n<h2 id=\"additionaldetails\">Additional details</h2>\n<p>The above summary was very brief and left out many important details. If you are interested in more information about this topic, a good starting point is the <a href=\"https://en.wikipedia.org/wiki/Hash_table\">Hash table</a> Wikipedia article (on which most parts of this introduction are based).</p>\n<h1 id=\"nextup\">Next up</h1>\n<p>In the <a href=\"/back-to-basics-dictionary-part-2-net-implementation\">next post</a> I'll look at how these concepts are implemented in the Dictionary class of the .NET framework.</p>\n",
        "comment_id": "5",
        "plaintext": "Posts in this series:\n\n 1. Part 1: Hash tables [/back-to-basics-dictionary-part-1]\n 2. Part 2: .NET implementation\n    [/back-to-basics-dictionary-part-2-net-implementation]\n 3. Part 3: Built-in GetHashCode\n    [/back-to-basics-dictionary-part-3-built-in-gethashcode]\n 4. Part 4: Custom GetHashCode\n    [/back-to-basics-dictionary-part-4-custom-gethashcode]\n\nIntroduction\nRecently I came across a situation in which I should have known the details\nabout how a .NET Dictionary (and hashmaps in general) worked under the hood.\nI realized that my knowledge about this topic was a bit rusty, so I decided I'd\nrefresh my memories and look into this topic.\nIn the first part of these posts we'll take a quick look into how a hash map\nworks in general, and how those concepts are implemented in the .NET framework.\n\nHash tables\nA hash table (or hash map) is a data structure implementing one of the most\nfundamental containers: the associative array (1). The associative array is a\nstructure, which maps keys to values. The basic operation of such a container is\nthe lookup, which - as the name suggests - looks up a value based on its key.\nThe hash map is implemented in a way to make this lookup operation as quick as\npossible, ideally needing only a constant amount of time.\n\nMechanisms\nI'll quickly introduce the most important mechanisms of the kind of hash map\nwhich is implemented in .NET.\nThe hash table contains a list of buckets, and in those buckets it stores one or\nmore values. A hash function  is used to compute an index  based on the key.\nWhen we insert an item into our container, it will be added to the bucket\ndesignated by the calculated index.\nLikewise, when we are doing a lookup by the key, we can also calculate this\nindex, and we will know the bucket in which we have to look for our item.\nIdeally, the hash function will assign each key to a unique bucket, so that all\nbuckets contain only a single element. This would mean that our lookup operation\nis really constant in its run-time, since it has to calculate the hash, and then\nit has to get the first (and only) item from the appropriate bucket.\n\nThe following image illustrates such a container, which stores some books, where\nthe key is the title of the book (assuming that all the books we'd like to store\nin this case have unique titles).\n\n\n\nBut it is possible that the hash function generates an identical hash for two\ndifferent keys, so most hash table designs assume that collisions can and will\nhappen.\nWhen a collision happen, we will store more than a single elements in a bucket,\nwhich will affect the time needed to do a lookup. In order to make the lookup\nfast, have to keep the ratio of the number of items and the number of buckets in\nour container low. This number is an essential property of a hash map, and is\nusually called the load factor.\n\nHandling collisions\nOne of the most important concepts in the implementation of a hash map is the\nway how we handle when two different keys end up having the same hash value,\nthus their index will point to the same bucket.\nThere are many different approaches, but arguably two of them are the most\nprominent.\n\nSeparate chaining\nWhen using separate chaining, each bucket is handled independently, they all\nstore a list of some kind with all the items having that same key. In this\nstructure, the time needed to find an item is the time needed to calculate the\nindex based on the key, plus the time needed to find the item in the list.\nBecause of this, ideally most of the lists should contain zero or one item, and\nsome of them two or maybe three, but not many more. This way, the time needed\nfor a lookup can be kept low. However, in order to achieve this, the load factor\ncannot be too high, so we have to adjust the number of buckets to the number of\nitems being stored.\nThe following image illustrates a hash map using separate chaining to handle a\ncollision.\n\n\n\nIn order to do separate chaining, every bucket should contain a list of items.\nThis can be implemented in various ways, probably the simplest is to store a\nlinked list in every bucket.\n\nOpen addressing\nIn open addressing, the bucket list is simply an array of items, and all the\nitems are stored in this array. This means that if we want to insert an item,\nbut the bucket of the item's key is already filled, we have to store the item in\nanother bucket.\nIn order to find this \"another\" bucket to use, a so called probe sequence  is\nused, which defines how we find the alternative position for our item. The\nsimplest of such probe sequences is the linear probing, in which we simply start\nlooking at the buckets following the originally designated position, until we\nfind a free slot.\n\n\n\nWhen we are doing a lookup, if the item in the designated bucket has a different\nkey than what we were looking for, we have to continue looking in the buckets\naccording to the probe sequence until we either find our item, or find an empty\nbucket (which means that the item is not in the hash map).\n\nAdditional details\nThe above summary was very brief and left out many important details. If you are\ninterested in more information about this topic, a good starting point is the \nHash table [https://en.wikipedia.org/wiki/Hash_table]  Wikipedia article (on\nwhich most parts of this introduction are based).\n\nNext up\nIn the next post [/back-to-basics-dictionary-part-2-net-implementation]  I'll\nlook at how these concepts are implemented in the Dictionary class of the .NET\nframework.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "An introduction to how a hash table works, which is the foundation of the key-value stores implemented in many environments.",
        "author_id": "1",
        "created_at": "2015-07-25 11:46:47",
        "created_by": "1",
        "updated_at": "2019-02-20 20:03:34",
        "updated_by": "1",
        "published_at": "2015-07-25 13:37:04",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2961",
        "uuid": "fadb3795-ffe1-4cd1-ad9d-5615717bed29",
        "title": "Back to basics: Dictionary part 2, .NET implementation",
        "slug": "back-to-basics-dictionary-part-2-net-implementation",
        "mobiledoc": "{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"**Posts in this series**:\\n\\n1. [Part 1: Hash tables](/back-to-basics-dictionary-part-1)\\n2. [Part 2: .NET implementation](/back-to-basics-dictionary-part-2-net-implementation)\\n3. [Part 3: Built-in GetHashCode](/back-to-basics-dictionary-part-3-built-in-gethashcode)\\n4. [Part 4: Custom GetHashCode](/back-to-basics-dictionary-part-4-custom-gethashcode)\\n\\n# Introduction\\n\\n[Last time](http://blog.markvincze.com/back-to-basics-dictionary-part-1/) we saw an overview about the basic concepts behind a hash map.\\n\\nIn this post we will take a look at the .NET Dictionary class, and see what type of hash map it is and how the different mechanisms have been implemented in C#.\\n\\nIn order to investigate, I used the [Reference source](http://referencesource.microsoft.com/) published by Microsoft, which contains the code base of the .NET Framework, in which we can look under the hood of the [System.Collections.Generic.Dictionary](http://referencesource.microsoft.com/#mscorlib/system/collections/generic/dictionary.cs,998e5f475d87f454) class.\\n\\n#Data model\\nEvery object stored in the dictionary is represented by an instance of the Entry struct:\\n\\n```csharp\\nprivate struct Entry\\n{\\n    public int hashCode;\\n    public int next;\\n    public TKey key;\\n    public TValue value;\\n}\\n```\\n\\nThe roles of the `hashCode`, `key` and `value` fields are quite self-explanatory. The field `next` plays a role  in the implementation of collision resolution (see later).\\nEvery entry lives in an array of Entry objects, which is a field of the Dictionary class:\\n\\n```csharp\\nprivate Entry[] entries;\\n```\\n\\nThis can be a bit confusing, but this array is **not** the list of buckets we looked at previously. The key and the hash of the key have no relation with the index at which an entry is stored in the `entries` array, it is rather incidental.\\n\\nThe buckets of the hash map are stored in a separate array:\\n\\n```csharp\\nprivate int[] buckets;\\n```\\n\\nAs you can see, this is a simple array of integers, where every value in this array points to an index in the `entries` array, at which index the *first* entry of that bucket is stored. I write first, because a bucket can contain more than one elements in case of a hash collision, since the .NET Dictionary uses a variant of the technique called *chaining* to resolve collisions, which we looked at in the [previous post](http://blog.markvincze.com/back-to-basics-dictionary-part-1/#separatechaining).\\n\\n## Some observations\\n\\n- When using the hash of a key, we want to map the hash to an index in the `buckets` array, which we do by  calculating the remainder of the key divided by the number of buckets. Because we need an index, we want to avoid having negative values, so in many places the code calculates the logical bitwise AND of the hash and the value `0x7FFFFFFF`, thereby eliminating all negative values.\\n- When a collision happens and two entries fall into the same bucket, they will be chained together using the `next` field of the Entry. It will point to the next entry, and it has the value -1 if the entry is the last in the chain.\\n- Until we don't remove any elements, the `entries` array will be consecutively filled with elements from the 0 index, any new items will be added at `count` position.  \\nWhen we remove an element, we create a \\\"hole\\\" in this array. This hole will be pointed to by the field `freeList`, and the number of free holes will be represented by `freeCount`. These free entries will be chained together with the `Entry.next` field, similarly to how collided entries are chained together.\\n\\n## Inserting items\\n\\nThe following code fragment shows how the Dictionary handles insertions (simplified, comments by me):\\n\\n```csharp\\nprivate void Insert(TKey key, TValue value, bool add)\\n{\\n    // Calculate the hash code of the key, eliminate negative values.\\n    int hashCode = comparer.GetHashCode(key) & 0x7FFFFFFF;\\n\\n    // Calculate the remainder of the hashCode divided by the number of buckets.\\n    // This is the usual way of narrowing the value set of the hash code to the set of possible bucket indices.\\n    int targetBucket = hashCode % buckets.Length;\\n\\n    // Look at all the entries in the target bucket. The next field of the entry points to the next entry in the chain, in case of collision.\\n    // If there are no more items in the chain, its value is -1.\\n    // If we find the key in the dictionary, we update the associated value and return.\\n    for (int i = buckets[targetBucket]; i >= 0; i = entries[i].next) {\\n        if (entries[i].hashCode == hashCode && comparer.Equals(entries[i].key, key)) {\\n            entries[i].value = value;\\n            version++;\\n            return;\\n        }\\n    }\\n\\n    int index;\\n    if (freeCount > 0) {\\n        // There is a \\\"hole\\\" in the entries array, because something has been removed.\\n        // The first empty place is pointed to by freeList, we insert our entry there.\\n        index = freeList;\\n        freeList = entries[index].next;\\n        freeCount--;\\n    }\\n    else {\\n        // There are no \\\"holes\\\" in the entries array.\\n        if (count == entries.Length)\\n        {\\n            // The dictionary is full, we need to increase its size by calling Resize.\\n            // (After Resize, it's guaranteed that there are no holes in the array.)\\n            Resize();\\n            targetBucket = hashCode % buckets.Length;\\n        }\\n\\n        // We can simply take the next consecutive place in the entries array.\\n        index = count;\\n        count++;\\n    }\\n\\n    // Setting the fields of the entry \\n    entries[index].hashCode = hashCode;\\n    entries[index].next = buckets[targetBucket]; // If the bucket already contained an item, it will be the next in the collision resolution chain.\\n    entries[index].key = key;\\n    entries[index].value = value;\\n    buckets[targetBucket] = index; // The bucket will point to this entry from now on.\\n}\\n```\\n\\n# Illustration\\n\\nI created a small class library and GUI app for two purposes:\\n\\n - Dive into and extract the internal state of a Dictionary instance using Reflection\\n - Visualize the hash map in a neat diagram\\n\\n(You can find the source code for the lib and the tool in my [GitHub repo](https://github.com/markvincze/dictionary-edu), sorry for the ugly WPF code, [\\\"MVVM, you must be kidding\\\"](https://vimeo.com/37913054#t=23m10s) :))\\n\\nThe tool displays the buckets array, which contains indices which point to records in the entries array, and shows the details of the actual entries.\\n\\nI created a helper class which represents a book with an author and a title. We will use the author as a key to store the books in the dictionary. First I'll add two books to the dictionary. Luckily the hashes of the keys from these two books will not collide:\\n\\n```csharp\\nvar book1 = new Book(\\\"J.R.R. Tolkien\\\", \\\"The Lord of the Rings\\\");\\nvar book2 = new Book(\\\"Patrick Rothfuss\\\", \\\"Name of the Wind\\\");\\n\\nvar dict = new Dictionary<string, Book>(5)\\n{\\n    { book1.Author, book1 },\\n    { book2.Author, book2 },\\n};\\n```\\n\\nWith these two items, the internal structure of the Dictionary looks like this.\\n![This is how a Dictionary looks with no collisions](/content/images/2015/08/nocollision-1.png)\\nYou can see that the entries are in the buckets designated by their hashcodes, while the Entries array is filled up consecutively.\\n\\nNow we insert a third element of which the remainder of its key hash will collide with an existing entry in the dictionary.\\n\\n```csharp\\nvar book3 = new Book(\\\"Frank Herbert\\\", \\\"Dune\\\");\\n```\\n\\nWe can see that the elements falling in the same bucket are chained together, with the Next field of the entry pointing to the next entry in the chain.\\n\\n![Two collided items chained together in the same bucket.](/content/images/2015/08/collision-1.png)\\n\\nWe can say that the .NET Dictionary implementation conceptually uses chaining as its collision resolution method, but it doesn't use a separate data structure (like a linked list) to store the items in the chain, it rather stores every entry in the same array.\\n\\nNow we add some more elements to the dictionary, then remove two of them.\\n\\n```csharp\\nvar book1 = new Book(\\\"J.R.R. Tolkien\\\", \\\"The Lord of the Rings\\\");\\nvar book2 = new Book(\\\"Patrick Rothfuss\\\", \\\"Name of the Wind\\\");\\nvar book3 = new Book(\\\"Frank Herbert\\\", \\\"Dune\\\");\\nvar book4 = new Book(\\\"Iain M. Banks\\\", \\\"Consider Phlebas\\\");\\nvar book5 = new Book(\\\"Isaac Asimov\\\", \\\"Foundation\\\");\\nvar book6 = new Book(\\\"Arthur Clarke\\\", \\\"2001: Space Odyssey\\\");\\n\\nvar dict = new Dictionary<string, Book>(5)\\n{\\n    { book1.Author, book1},\\n    { book2.Author, book2},\\n    { book3.Author, book3},\\n    { book4.Author, book3},\\n    { book5.Author, book3},\\n    { book6.Author, book3},\\n\\n};\\n\\ndict.Remove(book2.Author);\\ndict.Remove(book5.Author);\\n```\\n\\nAfter this, we can see, that the \\\"holes\\\" in the dictionary will be chained together the same way as collided entries are chained.\\n\\n![Holes chained together in the dictionary.](/content/images/2015/08/removed-entries-1.png)\\n\\nAnd in the code fragment above we saw that when we insert items, we always try to fill these holes first, and then use up the free slots at the end of the array.\\n\\n# Summary\\n\\nWith this we got an overview about how a dictionary works. There are other important aspects, like the [lookup](http://referencesource.microsoft.com/#mscorlib/system/collections/generic/dictionary.cs,bcd13bb775d408f1) itself, or [resizing](http://referencesource.microsoft.com/#mscorlib/system/collections/generic/dictionary.cs,3b9a0882313262cd) the dictionary in case there is no more free space in the array, but I'll leave it up to you to discover. With the above introduction, the code should be rather straightforward to read.\\n\\nIn the [next part](/back-to-basics-dictionary-part-3-built-in-gethashcode) I'll look at how the hash code of the keys are calculated, and what are the pitfalls we have to watch out for when choosing a type to use as a key.\"}]],\"markups\":[],\"sections\":[[10,0]]}",
        "html": "<p><strong>Posts in this series</strong>:</p>\n<ol>\n<li><a href=\"/back-to-basics-dictionary-part-1\">Part 1: Hash tables</a></li>\n<li><a href=\"/back-to-basics-dictionary-part-2-net-implementation\">Part 2: .NET implementation</a></li>\n<li><a href=\"/back-to-basics-dictionary-part-3-built-in-gethashcode\">Part 3: Built-in GetHashCode</a></li>\n<li><a href=\"/back-to-basics-dictionary-part-4-custom-gethashcode\">Part 4: Custom GetHashCode</a></li>\n</ol>\n<h1 id=\"introduction\">Introduction</h1>\n<p><a href=\"http://blog.markvincze.com/back-to-basics-dictionary-part-1/\">Last time</a> we saw an overview about the basic concepts behind a hash map.</p>\n<p>In this post we will take a look at the .NET Dictionary class, and see what type of hash map it is and how the different mechanisms have been implemented in C#.</p>\n<p>In order to investigate, I used the <a href=\"http://referencesource.microsoft.com/\">Reference source</a> published by Microsoft, which contains the code base of the .NET Framework, in which we can look under the hood of the <a href=\"http://referencesource.microsoft.com/#mscorlib/system/collections/generic/dictionary.cs,998e5f475d87f454\">System.Collections.Generic.Dictionary</a> class.</p>\n<h1 id=\"datamodel\">Data model</h1>\n<p>Every object stored in the dictionary is represented by an instance of the Entry struct:</p>\n<pre><code class=\"language-csharp\">private struct Entry\n{\n    public int hashCode;\n    public int next;\n    public TKey key;\n    public TValue value;\n}\n</code></pre>\n<p>The roles of the <code>hashCode</code>, <code>key</code> and <code>value</code> fields are quite self-explanatory. The field <code>next</code> plays a role  in the implementation of collision resolution (see later).<br>\nEvery entry lives in an array of Entry objects, which is a field of the Dictionary class:</p>\n<pre><code class=\"language-csharp\">private Entry[] entries;\n</code></pre>\n<p>This can be a bit confusing, but this array is <strong>not</strong> the list of buckets we looked at previously. The key and the hash of the key have no relation with the index at which an entry is stored in the <code>entries</code> array, it is rather incidental.</p>\n<p>The buckets of the hash map are stored in a separate array:</p>\n<pre><code class=\"language-csharp\">private int[] buckets;\n</code></pre>\n<p>As you can see, this is a simple array of integers, where every value in this array points to an index in the <code>entries</code> array, at which index the <em>first</em> entry of that bucket is stored. I write first, because a bucket can contain more than one elements in case of a hash collision, since the .NET Dictionary uses a variant of the technique called <em>chaining</em> to resolve collisions, which we looked at in the <a href=\"http://blog.markvincze.com/back-to-basics-dictionary-part-1/#separatechaining\">previous post</a>.</p>\n<h2 id=\"someobservations\">Some observations</h2>\n<ul>\n<li>When using the hash of a key, we want to map the hash to an index in the <code>buckets</code> array, which we do by  calculating the remainder of the key divided by the number of buckets. Because we need an index, we want to avoid having negative values, so in many places the code calculates the logical bitwise AND of the hash and the value <code>0x7FFFFFFF</code>, thereby eliminating all negative values.</li>\n<li>When a collision happens and two entries fall into the same bucket, they will be chained together using the <code>next</code> field of the Entry. It will point to the next entry, and it has the value -1 if the entry is the last in the chain.</li>\n<li>Until we don't remove any elements, the <code>entries</code> array will be consecutively filled with elements from the 0 index, any new items will be added at <code>count</code> position.<br>\nWhen we remove an element, we create a &quot;hole&quot; in this array. This hole will be pointed to by the field <code>freeList</code>, and the number of free holes will be represented by <code>freeCount</code>. These free entries will be chained together with the <code>Entry.next</code> field, similarly to how collided entries are chained together.</li>\n</ul>\n<h2 id=\"insertingitems\">Inserting items</h2>\n<p>The following code fragment shows how the Dictionary handles insertions (simplified, comments by me):</p>\n<pre><code class=\"language-csharp\">private void Insert(TKey key, TValue value, bool add)\n{\n    // Calculate the hash code of the key, eliminate negative values.\n    int hashCode = comparer.GetHashCode(key) &amp; 0x7FFFFFFF;\n\n    // Calculate the remainder of the hashCode divided by the number of buckets.\n    // This is the usual way of narrowing the value set of the hash code to the set of possible bucket indices.\n    int targetBucket = hashCode % buckets.Length;\n\n    // Look at all the entries in the target bucket. The next field of the entry points to the next entry in the chain, in case of collision.\n    // If there are no more items in the chain, its value is -1.\n    // If we find the key in the dictionary, we update the associated value and return.\n    for (int i = buckets[targetBucket]; i &gt;= 0; i = entries[i].next) {\n        if (entries[i].hashCode == hashCode &amp;&amp; comparer.Equals(entries[i].key, key)) {\n            entries[i].value = value;\n            version++;\n            return;\n        }\n    }\n\n    int index;\n    if (freeCount &gt; 0) {\n        // There is a &quot;hole&quot; in the entries array, because something has been removed.\n        // The first empty place is pointed to by freeList, we insert our entry there.\n        index = freeList;\n        freeList = entries[index].next;\n        freeCount--;\n    }\n    else {\n        // There are no &quot;holes&quot; in the entries array.\n        if (count == entries.Length)\n        {\n            // The dictionary is full, we need to increase its size by calling Resize.\n            // (After Resize, it's guaranteed that there are no holes in the array.)\n            Resize();\n            targetBucket = hashCode % buckets.Length;\n        }\n\n        // We can simply take the next consecutive place in the entries array.\n        index = count;\n        count++;\n    }\n\n    // Setting the fields of the entry \n    entries[index].hashCode = hashCode;\n    entries[index].next = buckets[targetBucket]; // If the bucket already contained an item, it will be the next in the collision resolution chain.\n    entries[index].key = key;\n    entries[index].value = value;\n    buckets[targetBucket] = index; // The bucket will point to this entry from now on.\n}\n</code></pre>\n<h1 id=\"illustration\">Illustration</h1>\n<p>I created a small class library and GUI app for two purposes:</p>\n<ul>\n<li>Dive into and extract the internal state of a Dictionary instance using Reflection</li>\n<li>Visualize the hash map in a neat diagram</li>\n</ul>\n<p>(You can find the source code for the lib and the tool in my <a href=\"https://github.com/markvincze/dictionary-edu\">GitHub repo</a>, sorry for the ugly WPF code, <a href=\"https://vimeo.com/37913054#t=23m10s\">&quot;MVVM, you must be kidding&quot;</a> :))</p>\n<p>The tool displays the buckets array, which contains indices which point to records in the entries array, and shows the details of the actual entries.</p>\n<p>I created a helper class which represents a book with an author and a title. We will use the author as a key to store the books in the dictionary. First I'll add two books to the dictionary. Luckily the hashes of the keys from these two books will not collide:</p>\n<pre><code class=\"language-csharp\">var book1 = new Book(&quot;J.R.R. Tolkien&quot;, &quot;The Lord of the Rings&quot;);\nvar book2 = new Book(&quot;Patrick Rothfuss&quot;, &quot;Name of the Wind&quot;);\n\nvar dict = new Dictionary&lt;string, Book&gt;(5)\n{\n    { book1.Author, book1 },\n    { book2.Author, book2 },\n};\n</code></pre>\n<p>With these two items, the internal structure of the Dictionary looks like this.<br>\n<img src=\"/content/images/2015/08/nocollision-1.png\" alt=\"This is how a Dictionary looks with no collisions\"><br>\nYou can see that the entries are in the buckets designated by their hashcodes, while the Entries array is filled up consecutively.</p>\n<p>Now we insert a third element of which the remainder of its key hash will collide with an existing entry in the dictionary.</p>\n<pre><code class=\"language-csharp\">var book3 = new Book(&quot;Frank Herbert&quot;, &quot;Dune&quot;);\n</code></pre>\n<p>We can see that the elements falling in the same bucket are chained together, with the Next field of the entry pointing to the next entry in the chain.</p>\n<p><img src=\"/content/images/2015/08/collision-1.png\" alt=\"Two collided items chained together in the same bucket.\"></p>\n<p>We can say that the .NET Dictionary implementation conceptually uses chaining as its collision resolution method, but it doesn't use a separate data structure (like a linked list) to store the items in the chain, it rather stores every entry in the same array.</p>\n<p>Now we add some more elements to the dictionary, then remove two of them.</p>\n<pre><code class=\"language-csharp\">var book1 = new Book(&quot;J.R.R. Tolkien&quot;, &quot;The Lord of the Rings&quot;);\nvar book2 = new Book(&quot;Patrick Rothfuss&quot;, &quot;Name of the Wind&quot;);\nvar book3 = new Book(&quot;Frank Herbert&quot;, &quot;Dune&quot;);\nvar book4 = new Book(&quot;Iain M. Banks&quot;, &quot;Consider Phlebas&quot;);\nvar book5 = new Book(&quot;Isaac Asimov&quot;, &quot;Foundation&quot;);\nvar book6 = new Book(&quot;Arthur Clarke&quot;, &quot;2001: Space Odyssey&quot;);\n\nvar dict = new Dictionary&lt;string, Book&gt;(5)\n{\n    { book1.Author, book1},\n    { book2.Author, book2},\n    { book3.Author, book3},\n    { book4.Author, book3},\n    { book5.Author, book3},\n    { book6.Author, book3},\n\n};\n\ndict.Remove(book2.Author);\ndict.Remove(book5.Author);\n</code></pre>\n<p>After this, we can see, that the &quot;holes&quot; in the dictionary will be chained together the same way as collided entries are chained.</p>\n<p><img src=\"/content/images/2015/08/removed-entries-1.png\" alt=\"Holes chained together in the dictionary.\"></p>\n<p>And in the code fragment above we saw that when we insert items, we always try to fill these holes first, and then use up the free slots at the end of the array.</p>\n<h1 id=\"summary\">Summary</h1>\n<p>With this we got an overview about how a dictionary works. There are other important aspects, like the <a href=\"http://referencesource.microsoft.com/#mscorlib/system/collections/generic/dictionary.cs,bcd13bb775d408f1\">lookup</a> itself, or <a href=\"http://referencesource.microsoft.com/#mscorlib/system/collections/generic/dictionary.cs,3b9a0882313262cd\">resizing</a> the dictionary in case there is no more free space in the array, but I'll leave it up to you to discover. With the above introduction, the code should be rather straightforward to read.</p>\n<p>In the <a href=\"/back-to-basics-dictionary-part-3-built-in-gethashcode\">next part</a> I'll look at how the hash code of the keys are calculated, and what are the pitfalls we have to watch out for when choosing a type to use as a key.</p>\n",
        "comment_id": "6",
        "plaintext": "Posts in this series:\n\n 1. Part 1: Hash tables [/back-to-basics-dictionary-part-1]\n 2. Part 2: .NET implementation\n    [/back-to-basics-dictionary-part-2-net-implementation]\n 3. Part 3: Built-in GetHashCode\n    [/back-to-basics-dictionary-part-3-built-in-gethashcode]\n 4. Part 4: Custom GetHashCode\n    [/back-to-basics-dictionary-part-4-custom-gethashcode]\n\nIntroduction\nLast time [http://blog.markvincze.com/back-to-basics-dictionary-part-1/]  we saw\nan overview about the basic concepts behind a hash map.\n\nIn this post we will take a look at the .NET Dictionary class, and see what type\nof hash map it is and how the different mechanisms have been implemented in C#.\n\nIn order to investigate, I used the Reference source\n[http://referencesource.microsoft.com/]  published by Microsoft, which contains\nthe code base of the .NET Framework, in which we can look under the hood of the \nSystem.Collections.Generic.Dictionary  class.\n\nData model\nEvery object stored in the dictionary is represented by an instance of the Entry\nstruct:\n\nprivate struct Entry\n{\n    public int hashCode;\n    public int next;\n    public TKey key;\n    public TValue value;\n}\n\n\nThe roles of the hashCode, key  and value  fields are quite self-explanatory.\nThe field next  plays a role in the implementation of collision resolution (see\nlater).\nEvery entry lives in an array of Entry objects, which is a field of the\nDictionary class:\n\nprivate Entry[] entries;\n\n\nThis can be a bit confusing, but this array is not  the list of buckets we\nlooked at previously. The key and the hash of the key have no relation with the\nindex at which an entry is stored in the entries  array, it is rather\nincidental.\n\nThe buckets of the hash map are stored in a separate array:\n\nprivate int[] buckets;\n\n\nAs you can see, this is a simple array of integers, where every value in this\narray points to an index in the entries  array, at which index the first  entry\nof that bucket is stored. I write first, because a bucket can contain more than\none elements in case of a hash collision, since the .NET Dictionary uses a\nvariant of the technique called chaining  to resolve collisions, which we looked\nat in the previous post.\n\nSome observations\n * When using the hash of a key, we want to map the hash to an index in the \n   buckets  array, which we do by calculating the remainder of the key divided\n   by the number of buckets. Because we need an index, we want to avoid having\n   negative values, so in many places the code calculates the logical bitwise\n   AND of the hash and the value 0x7FFFFFFF, thereby eliminating all negative\n   values.\n * When a collision happens and two entries fall into the same bucket, they will\n   be chained together using the next  field of the Entry. It will point to the\n   next entry, and it has the value -1 if the entry is the last in the chain.\n * Until we don't remove any elements, the entries  array will be consecutively\n   filled with elements from the 0 index, any new items will be added at count \n   position.\n   When we remove an element, we create a \"hole\" in this array. This hole will\n   be pointed to by the field freeList, and the number of free holes will be\n   represented by freeCount. These free entries will be chained together with\n   the Entry.next  field, similarly to how collided entries are chained\n   together.\n\nInserting items\nThe following code fragment shows how the Dictionary handles insertions\n(simplified, comments by me):\n\nprivate void Insert(TKey key, TValue value, bool add)\n{\n    // Calculate the hash code of the key, eliminate negative values.\n    int hashCode = comparer.GetHashCode(key) & 0x7FFFFFFF;\n\n    // Calculate the remainder of the hashCode divided by the number of buckets.\n    // This is the usual way of narrowing the value set of the hash code to the set of possible bucket indices.\n    int targetBucket = hashCode % buckets.Length;\n\n    // Look at all the entries in the target bucket. The next field of the entry points to the next entry in the chain, in case of collision.\n    // If there are no more items in the chain, its value is -1.\n    // If we find the key in the dictionary, we update the associated value and return.\n    for (int i = buckets[targetBucket]; i >= 0; i = entries[i].next) {\n        if (entries[i].hashCode == hashCode && comparer.Equals(entries[i].key, key)) {\n            entries[i].value = value;\n            version++;\n            return;\n        }\n    }\n\n    int index;\n    if (freeCount > 0) {\n        // There is a \"hole\" in the entries array, because something has been removed.\n        // The first empty place is pointed to by freeList, we insert our entry there.\n        index = freeList;\n        freeList = entries[index].next;\n        freeCount--;\n    }\n    else {\n        // There are no \"holes\" in the entries array.\n        if (count == entries.Length)\n        {\n            // The dictionary is full, we need to increase its size by calling Resize.\n            // (After Resize, it's guaranteed that there are no holes in the array.)\n            Resize();\n            targetBucket = hashCode % buckets.Length;\n        }\n\n        // We can simply take the next consecutive place in the entries array.\n        index = count;\n        count++;\n    }\n\n    // Setting the fields of the entry \n    entries[index].hashCode = hashCode;\n    entries[index].next = buckets[targetBucket]; // If the bucket already contained an item, it will be the next in the collision resolution chain.\n    entries[index].key = key;\n    entries[index].value = value;\n    buckets[targetBucket] = index; // The bucket will point to this entry from now on.\n}\n\n\nIllustration\nI created a small class library and GUI app for two purposes:\n\n * Dive into and extract the internal state of a Dictionary instance using\n   Reflection\n * Visualize the hash map in a neat diagram\n\n(You can find the source code for the lib and the tool in my GitHub repo\n[https://github.com/markvincze/dictionary-edu], sorry for the ugly WPF code, \n\"MVVM, you must be kidding\"  :))\n\nThe tool displays the buckets array, which contains indices which point to\nrecords in the entries array, and shows the details of the actual entries.\n\nI created a helper class which represents a book with an author and a title. We\nwill use the author as a key to store the books in the dictionary. First I'll\nadd two books to the dictionary. Luckily the hashes of the keys from these two\nbooks will not collide:\n\nvar book1 = new Book(\"J.R.R. Tolkien\", \"The Lord of the Rings\");\nvar book2 = new Book(\"Patrick Rothfuss\", \"Name of the Wind\");\n\nvar dict = new Dictionary<string, Book>(5)\n{\n    { book1.Author, book1 },\n    { book2.Author, book2 },\n};\n\n\nWith these two items, the internal structure of the Dictionary looks like this.\n\nYou can see that the entries are in the buckets designated by their hashcodes,\nwhile the Entries array is filled up consecutively.\n\nNow we insert a third element of which the remainder of its key hash will\ncollide with an existing entry in the dictionary.\n\nvar book3 = new Book(\"Frank Herbert\", \"Dune\");\n\n\nWe can see that the elements falling in the same bucket are chained together,\nwith the Next field of the entry pointing to the next entry in the chain.\n\n\n\nWe can say that the .NET Dictionary implementation conceptually uses chaining as\nits collision resolution method, but it doesn't use a separate data structure\n(like a linked list) to store the items in the chain, it rather stores every\nentry in the same array.\n\nNow we add some more elements to the dictionary, then remove two of them.\n\nvar book1 = new Book(\"J.R.R. Tolkien\", \"The Lord of the Rings\");\nvar book2 = new Book(\"Patrick Rothfuss\", \"Name of the Wind\");\nvar book3 = new Book(\"Frank Herbert\", \"Dune\");\nvar book4 = new Book(\"Iain M. Banks\", \"Consider Phlebas\");\nvar book5 = new Book(\"Isaac Asimov\", \"Foundation\");\nvar book6 = new Book(\"Arthur Clarke\", \"2001: Space Odyssey\");\n\nvar dict = new Dictionary<string, Book>(5)\n{\n    { book1.Author, book1},\n    { book2.Author, book2},\n    { book3.Author, book3},\n    { book4.Author, book3},\n    { book5.Author, book3},\n    { book6.Author, book3},\n\n};\n\ndict.Remove(book2.Author);\ndict.Remove(book5.Author);\n\n\nAfter this, we can see, that the \"holes\" in the dictionary will be chained\ntogether the same way as collided entries are chained.\n\n\n\nAnd in the code fragment above we saw that when we insert items, we always try\nto fill these holes first, and then use up the free slots at the end of the\narray.\n\nSummary\nWith this we got an overview about how a dictionary works. There are other\nimportant aspects, like the lookup  itself, or resizing  the dictionary in case\nthere is no more free space in the array, but I'll leave it up to you to\ndiscover. With the above introduction, the code should be rather straightforward\nto read.\n\nIn the next part [/back-to-basics-dictionary-part-3-built-in-gethashcode]  I'll\nlook at how the hash code of the keys are calculated, and what are the pitfalls\nwe have to watch out for when choosing a type to use as a key.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "A look under the hood of Dictionary<TKey, TValue>, an overview of how the hash map data structure has been implemented in the .NET Framework.",
        "author_id": "1",
        "created_at": "2015-08-09 09:23:43",
        "created_by": "1",
        "updated_at": "2019-02-20 20:04:07",
        "updated_by": "1",
        "published_at": "2015-08-15 13:27:35",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2962",
        "uuid": "5b4d990c-4606-466e-991e-a9d20b86f4cf",
        "title": "Back to basics: Dictionary part 3, built-in GetHashCode",
        "slug": "back-to-basics-dictionary-part-3-built-in-gethashcode",
        "mobiledoc": "{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"**Posts in this series**:\\n\\n1. [Part 1: Hash tables](/back-to-basics-dictionary-part-1)\\n2. [Part 2: .NET implementation](/back-to-basics-dictionary-part-2-net-implementation)\\n3. [Part 3: Built-in GetHashCode](/back-to-basics-dictionary-part-3-built-in-gethashcode)\\n4. [Part 4: Custom GetHashCode](/back-to-basics-dictionary-part-4-custom-gethashcode)\\n\\n# Introduction\\n\\nIn the previous two posts we looked at the basic concepts behind the hash map data structure, and checked out how it is implemented in the Dictionary class of the .NET Framework.\\nToday we'll take a look at a very important mechanism behind the Dictionary class: the `GetHashCode` method, and the way its built-in implementation works.\\n\\n# GetHashCode\\n\\n[GetHashCode](https://msdn.microsoft.com/en-us/library/system.object.gethashcode(v=vs.110).aspx) is a built-in method in the .NET Framework defined on the Object class, thereby making every built-in or custom type inherit it. I usually feel there is quite a lot of misunderstanding around the purpose and the proper usage of this method.\\nIt serves as a default, simple way to use a hash function to generate a hash value for an object. The sole purpose of this method is to use it in the implementation of a hash map, as [Eric Lippert](http://blogs.msdn.com/b/ericlippert/archive/2011/02/28/guidelines-and-rules-for-gethashcode.aspx) states:\\n\\n>\\\"It is by design useful for only one thing: putting an object in a hash table. Hence the name.\\\"\\n\\nIn the Dictionary class, the GetHashCode method is used to get a hash value for a key in order to determine the bucket in which the entry has to be stored.\\n\\n## Built-in implementation\\n\\nThe method is defined on the Object class of the Base Class Library, and it has a default implementation, so we can ask for the hash value of any object.\\nThe built-in implementation of the method is different for reference types and value types. Here is a summary about how they work, based on the [documentation](https://msdn.microsoft.com/en-us/library/system.object.gethashcode(v=vs.110).aspx).\\n\\n### Reference types\\n\\nThe hash code of a reference type object is calculated based on its reference, and not its value, with the [RuntimeHelpers.GetHashCode](https://msdn.microsoft.com/en-us/library/system.runtime.compilerservices.runtimehelpers.gethashcode(v=vs.110).aspx) method. What this means is that the value of the hash code does not depend on the values of the object's fields at all. This can be confusing, imagine that you implement the following class to be used as a key.\\n\\n```csharp\\npublic class TestRefKey\\n{\\n    public readonly int Key;\\n\\n    public TestRefKey(int key)\\n    {\\n        Key = key;\\n    }\\n}\\n```\\n\\nYou store an item in a Dictionary with such a key, then you create the *same* key and try to get the item from the Dictionary, yet it will not be found.\\n\\n```csharp\\nvar dict = new Dictionary<TestRefKey, string>();\\nvar key1 = new TestRefKey(5);\\ndict.Add(key1, \\\"hello\\\");\\nvar key2 = new TestRefKey(5);\\nvar item = dict[key2]; // This throws a KeyNotFoundException.\\n```\\n\\nIn the above example, because the two keys are different objects, even if their internal \\\"state\\\" is the same, we won't find the object we stored, because the hash code of the reference type object is calculated based on the object's reference, rather than its value.\\nIf you would like to use a custom reference type as a Dictionary key, you should override its `Equals` and `GetHashCode` methods, and make the implementation based on the fields you want to make part of the hash key.\\nFor the above class, the following is a correct implementation.\\n\\n```csharp\\npublic class GoodTestRefKey\\n{\\n    public readonly int Key;\\n\\n    public GoodTestRefKey(int key)\\n    {\\n        Key = key;\\n    }\\n\\n    public override int GetHashCode()\\n    {\\n        return this.Key.GetHashCode();\\n    }\\n\\n    public override bool Equals(object obj)\\n    {\\n        if (obj == null || GetType() != obj.GetType())\\n            return false;\\n\\n        var other = (GoodTestRefKey) obj;\\n        return this.Key == other.Key;\\n    }\\n}\\n```\\n\\nThis way our `GoodTestRefKey` will have *value-type semantics* in terms of its hash code and equality, and using it as a Dictionary-key will work properly.\\n\\n#### (The equality operator)\\n\\nAs a sidenote: the recommendations about whether you should override the `==` operator as well are a bit unclear. You can either\\n\\n - Don't override the `==` operator, so it will still make a reference comparison, but that way you have to be careful about when to use `Equals` and `==`.\\n - Override `==` as well, so both comparisons will execute our custom implementation.\\n\\nThe [official guide](https://msdn.microsoft.com/en-us/library/vstudio/336aedhh(v=vs.100).aspx) states\\n\\n> \\\"Most reference types should not overload the equality operator, even if they override **Equals**. However, if you are implementing a reference type that is intended to have value semantics, such as a complex number type, you should override the equality operator.\\\"\\n\\nI find this advice a bit strange, since if I wanted a type to have value semantics, I would implement it as a struct. A type like the above `GoodTestRefKey` would also be more idiomatic as a struct in my opinion, I only implemented it as a class now for the sake of the example. (Then again, it only contains an integer, so in this specific case a simple int could've been used as the Dictionary key as well.)\\n\\n#### Built-in reference types\\n\\nWe've seen that the default implementation of the GetHashCode and Equality methods are based on the object's reference for a reference type. However, this is not necessarily true for built-in .NET types, they might have custom implementation. A typical example is `System.String`, of which the `Equals` and `GetHashCode` is based on the actual characters of the string and they don't depend on the reference, so using strings as Dictionary keys is safe to do.\\n\\n### Value types: \\n\\nIf the `GetHashCode` is not overridden for a value type, then the [ValueType.GetHashCode](https://msdn.microsoft.com/en-us/library/system.valuetype.gethashcode(v=vs.110).aspx) method will be called, which looks up all the fields with reflection, and calculates a hash code based on their values. This implicates that value type objects with the same field values will have the same hash code by default, so they are safe to be used as Dictionary keys.\\nIt's important to keep in mind that the default implementation takes into account **all** fields in the struct, so a difference in any of the fields will mean a difference in the hash code as well. If we want to have only a subset of the fields affecting the hash code, we should implement a custom `GetHashCode` method, which I will look at in the next post.\\n\\n## Gotcha: mutable reference type as a key\\n\\nIn the sample code I implemented the `GoodTestRefKey` to be immutable. This is very important when we intend to use a reference type as a key, otherwise the following scenario can happen:\\n\\n1. We create the key object and set its state.\\n2. Add an entry to the Dictionary with the key. The Dictionary will get the hash code of the object and store the entry in the appropriate bucket.\\n3. We change the state of the key object. This causes the key's hash code to change as well, but the entry will stay in the same bucket inside the Dictionary, so from now on it will probably be in an incorrect bucket.\\n4. Now the entry is in \\\"limbo\\\", if we try to look it up by the modified key, we won't find it, because the lookup will look in the wrong bucket. On the other hand, if we try to look it up with a key with the original state, the entry won't be found either, since the equality comparison with the modified key will return false.\\n\\nI will illustrate the situation with the little WPF client created for the last post. I removed the `readonly` keyword from the key's field to make it mutable.\\nIt's a very nice touch that Visual Studio displays a warning if we do this:\\n![Visual Studio warning us about using a mutable field in GetHashCode](/content/images/2015/08/vs-warning.png)\\n(After writing this I realized it isn't VS, but rather ReSharper, so the kudos goes to JetBrains instead of Microsoft in this case :).)\\n\\nI added an entry to a Dictionary with such a key:\\n\\n```csharp\\nvar key = new GoodTestRefKey(3);\\nvar dictWithMutableKey = new Dictionary<GoodTestRefKey, string>(5) { { key, \\\"Hello!\\\" } };\\n```\\n\\nIf we display the state of the Dictionary, we see that everything is good, the hash code of the key is 3, and the entry is in the proper bucket:\\n\\n![State of the Dictionary before mutation](/content/images/2015/08/before-mutate.png)\\n\\nIf we mutate the state of the key object\\n\\n```csharp\\nkey.Key = 4;\\n```\\n\\nAnd visualize the Dictionary again\\n\\n![The state of the Dictionary after mutating the key](/content/images/2015/08/after-mutate.png)\\n\\nYou see that the Key's value was changed to **4**, it's hash code *is* 4, however, it's still in the bucket at the index **3**. (Note that the Dictionary separately stores the hash code, which still has the original value.)\\n\\n```csharp\\nvar key = new GoodTestRefKey(3);\\nvar dictWithMutableKey = new Dictionary<GoodTestRefKey, string>(5) { { key, \\\"Hello!\\\" } };\\n\\nkey.Key = 4;\\n\\nvar obj1 = dictWithMutableKey[key]; // KeyNotFoundException\\nvar obj3 = dictWithMutableKey[new GoodTestRefKey(3)]; // KeyNotFoundException\\nvar obj2 = dictWithMutableKey[new GoodTestRefKey(4)]; // KeyNotFoundException\\n```\\n\\nIf we mutate the key object, we won't be able to retrieve our entry with any of the above approaches.\\n\\n1. In the first case we will look for the item in a different bucket.\\n2. In the second approach we will look at the right bucket, but the equality-comparison of the keys will fail, since they are different.\\n3. Same as the first case.\\n\\nBut even if one of these approaches managed to retrieve our entry, this situation should be generally avoided.\\n\\nThe rule of thumb to follow in order to avoid this is to only use immutable objects as keys. (Or at least the fields which take part in the hash code generation and the equality comparison should be immutable.)\\n\\nIn the [next post](/back-to-basics-dictionary-part-4-custom-gethashcode) I'll take a look at what to consider when implementing a custom `GetHashCode` method either for a reference or a value type.\"}]],\"markups\":[],\"sections\":[[10,0]]}",
        "html": "<p><strong>Posts in this series</strong>:</p>\n<ol>\n<li><a href=\"/back-to-basics-dictionary-part-1\">Part 1: Hash tables</a></li>\n<li><a href=\"/back-to-basics-dictionary-part-2-net-implementation\">Part 2: .NET implementation</a></li>\n<li><a href=\"/back-to-basics-dictionary-part-3-built-in-gethashcode\">Part 3: Built-in GetHashCode</a></li>\n<li><a href=\"/back-to-basics-dictionary-part-4-custom-gethashcode\">Part 4: Custom GetHashCode</a></li>\n</ol>\n<h1 id=\"introduction\">Introduction</h1>\n<p>In the previous two posts we looked at the basic concepts behind the hash map data structure, and checked out how it is implemented in the Dictionary class of the .NET Framework.<br>\nToday we'll take a look at a very important mechanism behind the Dictionary class: the <code>GetHashCode</code> method, and the way its built-in implementation works.</p>\n<h1 id=\"gethashcode\">GetHashCode</h1>\n<p><a href=\"https://msdn.microsoft.com/en-us/library/system.object.gethashcode(v=vs.110).aspx\">GetHashCode</a> is a built-in method in the .NET Framework defined on the Object class, thereby making every built-in or custom type inherit it. I usually feel there is quite a lot of misunderstanding around the purpose and the proper usage of this method.<br>\nIt serves as a default, simple way to use a hash function to generate a hash value for an object. The sole purpose of this method is to use it in the implementation of a hash map, as <a href=\"http://blogs.msdn.com/b/ericlippert/archive/2011/02/28/guidelines-and-rules-for-gethashcode.aspx\">Eric Lippert</a> states:</p>\n<blockquote>\n<p>&quot;It is by design useful for only one thing: putting an object in a hash table. Hence the name.&quot;</p>\n</blockquote>\n<p>In the Dictionary class, the GetHashCode method is used to get a hash value for a key in order to determine the bucket in which the entry has to be stored.</p>\n<h2 id=\"builtinimplementation\">Built-in implementation</h2>\n<p>The method is defined on the Object class of the Base Class Library, and it has a default implementation, so we can ask for the hash value of any object.<br>\nThe built-in implementation of the method is different for reference types and value types. Here is a summary about how they work, based on the <a href=\"https://msdn.microsoft.com/en-us/library/system.object.gethashcode(v=vs.110).aspx\">documentation</a>.</p>\n<h3 id=\"referencetypes\">Reference types</h3>\n<p>The hash code of a reference type object is calculated based on its reference, and not its value, with the <a href=\"https://msdn.microsoft.com/en-us/library/system.runtime.compilerservices.runtimehelpers.gethashcode(v=vs.110).aspx\">RuntimeHelpers.GetHashCode</a> method. What this means is that the value of the hash code does not depend on the values of the object's fields at all. This can be confusing, imagine that you implement the following class to be used as a key.</p>\n<pre><code class=\"language-csharp\">public class TestRefKey\n{\n    public readonly int Key;\n\n    public TestRefKey(int key)\n    {\n        Key = key;\n    }\n}\n</code></pre>\n<p>You store an item in a Dictionary with such a key, then you create the <em>same</em> key and try to get the item from the Dictionary, yet it will not be found.</p>\n<pre><code class=\"language-csharp\">var dict = new Dictionary&lt;TestRefKey, string&gt;();\nvar key1 = new TestRefKey(5);\ndict.Add(key1, &quot;hello&quot;);\nvar key2 = new TestRefKey(5);\nvar item = dict[key2]; // This throws a KeyNotFoundException.\n</code></pre>\n<p>In the above example, because the two keys are different objects, even if their internal &quot;state&quot; is the same, we won't find the object we stored, because the hash code of the reference type object is calculated based on the object's reference, rather than its value.<br>\nIf you would like to use a custom reference type as a Dictionary key, you should override its <code>Equals</code> and <code>GetHashCode</code> methods, and make the implementation based on the fields you want to make part of the hash key.<br>\nFor the above class, the following is a correct implementation.</p>\n<pre><code class=\"language-csharp\">public class GoodTestRefKey\n{\n    public readonly int Key;\n\n    public GoodTestRefKey(int key)\n    {\n        Key = key;\n    }\n\n    public override int GetHashCode()\n    {\n        return this.Key.GetHashCode();\n    }\n\n    public override bool Equals(object obj)\n    {\n        if (obj == null || GetType() != obj.GetType())\n            return false;\n\n        var other = (GoodTestRefKey) obj;\n        return this.Key == other.Key;\n    }\n}\n</code></pre>\n<p>This way our <code>GoodTestRefKey</code> will have <em>value-type semantics</em> in terms of its hash code and equality, and using it as a Dictionary-key will work properly.</p>\n<h4 id=\"theequalityoperator\">(The equality operator)</h4>\n<p>As a sidenote: the recommendations about whether you should override the <code>==</code> operator as well are a bit unclear. You can either</p>\n<ul>\n<li>Don't override the <code>==</code> operator, so it will still make a reference comparison, but that way you have to be careful about when to use <code>Equals</code> and <code>==</code>.</li>\n<li>Override <code>==</code> as well, so both comparisons will execute our custom implementation.</li>\n</ul>\n<p>The <a href=\"https://msdn.microsoft.com/en-us/library/vstudio/336aedhh(v=vs.100).aspx\">official guide</a> states</p>\n<blockquote>\n<p>&quot;Most reference types should not overload the equality operator, even if they override <strong>Equals</strong>. However, if you are implementing a reference type that is intended to have value semantics, such as a complex number type, you should override the equality operator.&quot;</p>\n</blockquote>\n<p>I find this advice a bit strange, since if I wanted a type to have value semantics, I would implement it as a struct. A type like the above <code>GoodTestRefKey</code> would also be more idiomatic as a struct in my opinion, I only implemented it as a class now for the sake of the example. (Then again, it only contains an integer, so in this specific case a simple int could've been used as the Dictionary key as well.)</p>\n<h4 id=\"builtinreferencetypes\">Built-in reference types</h4>\n<p>We've seen that the default implementation of the GetHashCode and Equality methods are based on the object's reference for a reference type. However, this is not necessarily true for built-in .NET types, they might have custom implementation. A typical example is <code>System.String</code>, of which the <code>Equals</code> and <code>GetHashCode</code> is based on the actual characters of the string and they don't depend on the reference, so using strings as Dictionary keys is safe to do.</p>\n<h3 id=\"valuetypes\">Value types:</h3>\n<p>If the <code>GetHashCode</code> is not overridden for a value type, then the <a href=\"https://msdn.microsoft.com/en-us/library/system.valuetype.gethashcode(v=vs.110).aspx\">ValueType.GetHashCode</a> method will be called, which looks up all the fields with reflection, and calculates a hash code based on their values. This implicates that value type objects with the same field values will have the same hash code by default, so they are safe to be used as Dictionary keys.<br>\nIt's important to keep in mind that the default implementation takes into account <strong>all</strong> fields in the struct, so a difference in any of the fields will mean a difference in the hash code as well. If we want to have only a subset of the fields affecting the hash code, we should implement a custom <code>GetHashCode</code> method, which I will look at in the next post.</p>\n<h2 id=\"gotchamutablereferencetypeasakey\">Gotcha: mutable reference type as a key</h2>\n<p>In the sample code I implemented the <code>GoodTestRefKey</code> to be immutable. This is very important when we intend to use a reference type as a key, otherwise the following scenario can happen:</p>\n<ol>\n<li>We create the key object and set its state.</li>\n<li>Add an entry to the Dictionary with the key. The Dictionary will get the hash code of the object and store the entry in the appropriate bucket.</li>\n<li>We change the state of the key object. This causes the key's hash code to change as well, but the entry will stay in the same bucket inside the Dictionary, so from now on it will probably be in an incorrect bucket.</li>\n<li>Now the entry is in &quot;limbo&quot;, if we try to look it up by the modified key, we won't find it, because the lookup will look in the wrong bucket. On the other hand, if we try to look it up with a key with the original state, the entry won't be found either, since the equality comparison with the modified key will return false.</li>\n</ol>\n<p>I will illustrate the situation with the little WPF client created for the last post. I removed the <code>readonly</code> keyword from the key's field to make it mutable.<br>\nIt's a very nice touch that Visual Studio displays a warning if we do this:<br>\n<img src=\"/content/images/2015/08/vs-warning.png\" alt=\"Visual Studio warning us about using a mutable field in GetHashCode\"><br>\n(After writing this I realized it isn't VS, but rather ReSharper, so the kudos goes to JetBrains instead of Microsoft in this case :).)</p>\n<p>I added an entry to a Dictionary with such a key:</p>\n<pre><code class=\"language-csharp\">var key = new GoodTestRefKey(3);\nvar dictWithMutableKey = new Dictionary&lt;GoodTestRefKey, string&gt;(5) { { key, &quot;Hello!&quot; } };\n</code></pre>\n<p>If we display the state of the Dictionary, we see that everything is good, the hash code of the key is 3, and the entry is in the proper bucket:</p>\n<p><img src=\"/content/images/2015/08/before-mutate.png\" alt=\"State of the Dictionary before mutation\"></p>\n<p>If we mutate the state of the key object</p>\n<pre><code class=\"language-csharp\">key.Key = 4;\n</code></pre>\n<p>And visualize the Dictionary again</p>\n<p><img src=\"/content/images/2015/08/after-mutate.png\" alt=\"The state of the Dictionary after mutating the key\"></p>\n<p>You see that the Key's value was changed to <strong>4</strong>, it's hash code <em>is</em> 4, however, it's still in the bucket at the index <strong>3</strong>. (Note that the Dictionary separately stores the hash code, which still has the original value.)</p>\n<pre><code class=\"language-csharp\">var key = new GoodTestRefKey(3);\nvar dictWithMutableKey = new Dictionary&lt;GoodTestRefKey, string&gt;(5) { { key, &quot;Hello!&quot; } };\n\nkey.Key = 4;\n\nvar obj1 = dictWithMutableKey[key]; // KeyNotFoundException\nvar obj3 = dictWithMutableKey[new GoodTestRefKey(3)]; // KeyNotFoundException\nvar obj2 = dictWithMutableKey[new GoodTestRefKey(4)]; // KeyNotFoundException\n</code></pre>\n<p>If we mutate the key object, we won't be able to retrieve our entry with any of the above approaches.</p>\n<ol>\n<li>In the first case we will look for the item in a different bucket.</li>\n<li>In the second approach we will look at the right bucket, but the equality-comparison of the keys will fail, since they are different.</li>\n<li>Same as the first case.</li>\n</ol>\n<p>But even if one of these approaches managed to retrieve our entry, this situation should be generally avoided.</p>\n<p>The rule of thumb to follow in order to avoid this is to only use immutable objects as keys. (Or at least the fields which take part in the hash code generation and the equality comparison should be immutable.)</p>\n<p>In the <a href=\"/back-to-basics-dictionary-part-4-custom-gethashcode\">next post</a> I'll take a look at what to consider when implementing a custom <code>GetHashCode</code> method either for a reference or a value type.</p>\n",
        "comment_id": "8",
        "plaintext": "Posts in this series:\n\n 1. Part 1: Hash tables [/back-to-basics-dictionary-part-1]\n 2. Part 2: .NET implementation\n    [/back-to-basics-dictionary-part-2-net-implementation]\n 3. Part 3: Built-in GetHashCode\n    [/back-to-basics-dictionary-part-3-built-in-gethashcode]\n 4. Part 4: Custom GetHashCode\n    [/back-to-basics-dictionary-part-4-custom-gethashcode]\n\nIntroduction\nIn the previous two posts we looked at the basic concepts behind the hash map\ndata structure, and checked out how it is implemented in the Dictionary class of\nthe .NET Framework.\nToday we'll take a look at a very important mechanism behind the Dictionary\nclass: the GetHashCode  method, and the way its built-in implementation works.\n\nGetHashCode\nGetHashCode\n[https://msdn.microsoft.com/en-us/library/system.object.gethashcode(v=vs.110).aspx] \n is a built-in method in the .NET Framework defined on the Object class, thereby\nmaking every built-in or custom type inherit it. I usually feel there is quite a\nlot of misunderstanding around the purpose and the proper usage of this method.\nIt serves as a default, simple way to use a hash function to generate a hash\nvalue for an object. The sole purpose of this method is to use it in the\nimplementation of a hash map, as Eric Lippert\n[http://blogs.msdn.com/b/ericlippert/archive/2011/02/28/guidelines-and-rules-for-gethashcode.aspx] \n states:\n\n\"It is by design useful for only one thing: putting an object in a hash table.\nHence the name.\"\n\nIn the Dictionary class, the GetHashCode method is used to get a hash value for\na key in order to determine the bucket in which the entry has to be stored.\n\nBuilt-in implementation\nThe method is defined on the Object class of the Base Class Library, and it has\na default implementation, so we can ask for the hash value of any object.\nThe built-in implementation of the method is different for reference types and\nvalue types. Here is a summary about how they work, based on the documentation\n[https://msdn.microsoft.com/en-us/library/system.object.gethashcode(v=vs.110).aspx]\n.\n\nReference types\nThe hash code of a reference type object is calculated based on its reference,\nand not its value, with the RuntimeHelpers.GetHashCode\n[https://msdn.microsoft.com/en-us/library/system.runtime.compilerservices.runtimehelpers.gethashcode(v=vs.110).aspx] \n method. What this means is that the value of the hash code does not depend on\nthe values of the object's fields at all. This can be confusing, imagine that\nyou implement the following class to be used as a key.\n\npublic class TestRefKey\n{\n    public readonly int Key;\n\n    public TestRefKey(int key)\n    {\n        Key = key;\n    }\n}\n\n\nYou store an item in a Dictionary with such a key, then you create the same  key\nand try to get the item from the Dictionary, yet it will not be found.\n\nvar dict = new Dictionary<TestRefKey, string>();\nvar key1 = new TestRefKey(5);\ndict.Add(key1, \"hello\");\nvar key2 = new TestRefKey(5);\nvar item = dict[key2]; // This throws a KeyNotFoundException.\n\n\nIn the above example, because the two keys are different objects, even if their\ninternal \"state\" is the same, we won't find the object we stored, because the\nhash code of the reference type object is calculated based on the object's\nreference, rather than its value.\nIf you would like to use a custom reference type as a Dictionary key, you should\noverride its Equals  and GetHashCode  methods, and make the implementation based\non the fields you want to make part of the hash key.\nFor the above class, the following is a correct implementation.\n\npublic class GoodTestRefKey\n{\n    public readonly int Key;\n\n    public GoodTestRefKey(int key)\n    {\n        Key = key;\n    }\n\n    public override int GetHashCode()\n    {\n        return this.Key.GetHashCode();\n    }\n\n    public override bool Equals(object obj)\n    {\n        if (obj == null || GetType() != obj.GetType())\n            return false;\n\n        var other = (GoodTestRefKey) obj;\n        return this.Key == other.Key;\n    }\n}\n\n\nThis way our GoodTestRefKey  will have value-type semantics  in terms of its\nhash code and equality, and using it as a Dictionary-key will work properly.\n\n(The equality operator)\nAs a sidenote: the recommendations about whether you should override the == \noperator as well are a bit unclear. You can either\n\n * Don't override the ==  operator, so it will still make a reference\n   comparison, but that way you have to be careful about when to use Equals  and\n    ==.\n * Override ==  as well, so both comparisons will execute our custom\n   implementation.\n\nThe official guide\n[https://msdn.microsoft.com/en-us/library/vstudio/336aedhh(v=vs.100).aspx] \nstates\n\n\"Most reference types should not overload the equality operator, even if they\noverride Equals. However, if you are implementing a reference type that is\nintended to have value semantics, such as a complex number type, you should\noverride the equality operator.\"\n\nI find this advice a bit strange, since if I wanted a type to have value\nsemantics, I would implement it as a struct. A type like the above \nGoodTestRefKey  would also be more idiomatic as a struct in my opinion, I only\nimplemented it as a class now for the sake of the example. (Then again, it only\ncontains an integer, so in this specific case a simple int could've been used as\nthe Dictionary key as well.)\n\nBuilt-in reference types\nWe've seen that the default implementation of the GetHashCode and Equality\nmethods are based on the object's reference for a reference type. However, this\nis not necessarily true for built-in .NET types, they might have custom\nimplementation. A typical example is System.String, of which the Equals  and \nGetHashCode  is based on the actual characters of the string and they don't\ndepend on the reference, so using strings as Dictionary keys is safe to do.\n\nValue types:\nIf the GetHashCode  is not overridden for a value type, then the \nValueType.GetHashCode\n[https://msdn.microsoft.com/en-us/library/system.valuetype.gethashcode(v=vs.110).aspx] \n method will be called, which looks up all the fields with reflection, and\ncalculates a hash code based on their values. This implicates that value type\nobjects with the same field values will have the same hash code by default, so\nthey are safe to be used as Dictionary keys.\nIt's important to keep in mind that the default implementation takes into\naccount all  fields in the struct, so a difference in any of the fields will\nmean a difference in the hash code as well. If we want to have only a subset of\nthe fields affecting the hash code, we should implement a custom GetHashCode \nmethod, which I will look at in the next post.\n\nGotcha: mutable reference type as a key\nIn the sample code I implemented the GoodTestRefKey  to be immutable. This is\nvery important when we intend to use a reference type as a key, otherwise the\nfollowing scenario can happen:\n\n 1. We create the key object and set its state.\n 2. Add an entry to the Dictionary with the key. The Dictionary will get the\n    hash code of the object and store the entry in the appropriate bucket.\n 3. We change the state of the key object. This causes the key's hash code to\n    change as well, but the entry will stay in the same bucket inside the\n    Dictionary, so from now on it will probably be in an incorrect bucket.\n 4. Now the entry is in \"limbo\", if we try to look it up by the modified key, we\n    won't find it, because the lookup will look in the wrong bucket. On the\n    other hand, if we try to look it up with a key with the original state, the\n    entry won't be found either, since the equality comparison with the modified\n    key will return false.\n\nI will illustrate the situation with the little WPF client created for the last\npost. I removed the readonly  keyword from the key's field to make it mutable.\nIt's a very nice touch that Visual Studio displays a warning if we do this:\n\n(After writing this I realized it isn't VS, but rather ReSharper, so the kudos\ngoes to JetBrains instead of Microsoft in this case :).)\n\nI added an entry to a Dictionary with such a key:\n\nvar key = new GoodTestRefKey(3);\nvar dictWithMutableKey = new Dictionary<GoodTestRefKey, string>(5) { { key, \"Hello!\" } };\n\n\nIf we display the state of the Dictionary, we see that everything is good, the\nhash code of the key is 3, and the entry is in the proper bucket:\n\n\n\nIf we mutate the state of the key object\n\nkey.Key = 4;\n\n\nAnd visualize the Dictionary again\n\n\n\nYou see that the Key's value was changed to 4, it's hash code is  4, however,\nit's still in the bucket at the index 3. (Note that the Dictionary separately\nstores the hash code, which still has the original value.)\n\nvar key = new GoodTestRefKey(3);\nvar dictWithMutableKey = new Dictionary<GoodTestRefKey, string>(5) { { key, \"Hello!\" } };\n\nkey.Key = 4;\n\nvar obj1 = dictWithMutableKey[key]; // KeyNotFoundException\nvar obj3 = dictWithMutableKey[new GoodTestRefKey(3)]; // KeyNotFoundException\nvar obj2 = dictWithMutableKey[new GoodTestRefKey(4)]; // KeyNotFoundException\n\n\nIf we mutate the key object, we won't be able to retrieve our entry with any of\nthe above approaches.\n\n 1. In the first case we will look for the item in a different bucket.\n 2. In the second approach we will look at the right bucket, but the\n    equality-comparison of the keys will fail, since they are different.\n 3. Same as the first case.\n\nBut even if one of these approaches managed to retrieve our entry, this\nsituation should be generally avoided.\n\nThe rule of thumb to follow in order to avoid this is to only use immutable\nobjects as keys. (Or at least the fields which take part in the hash code\ngeneration and the equality comparison should be immutable.)\n\nIn the next post [/back-to-basics-dictionary-part-4-custom-gethashcode]  I'll\ntake a look at what to consider when implementing a custom GetHashCode  method\neither for a reference or a value type.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "GetHashCode plays an important part in the implementation of a hash map. We should be familiar with it when using a custom type as a Dictionary key.",
        "author_id": "1",
        "created_at": "2015-08-29 10:08:10",
        "created_by": "1",
        "updated_at": "2019-02-20 20:04:43",
        "updated_by": "1",
        "published_at": "2015-08-29 14:02:09",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2963",
        "uuid": "50212125-ecf8-4174-96eb-2b80f08cc29d",
        "title": "Back to basics: Dictionary part 4, custom GetHashCode",
        "slug": "back-to-basics-dictionary-part-4-custom-gethashcode",
        "mobiledoc": "{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"**Posts in this series**:\\n\\n1. [Part 1: Hash tables](/back-to-basics-dictionary-part-1)\\n2. [Part 2: .NET implementation](/back-to-basics-dictionary-part-2-net-implementation)\\n3. [Part 3: Built-in GetHashCode](/back-to-basics-dictionary-part-3-built-in-gethashcode)\\n4. [Part 4: Custom GetHashCode](/back-to-basics-dictionary-part-4-custom-gethashcode)\\n\\n# General guidelines\\n\\nThis is the last part in the series about the Dictionary class and the GetHashCode method. In this post we'll take a look at what to look out for when implementing a custom GetHashCode method. In the [previous post](/back-to-basics-dictionary-part-3-built-in-gethashcode/) we've seen how the built-in GetHashCode works.\\n\\nWe create a custom implementation when we want to deviate from the default behavior, namely:\\n\\n - In case of a reference type, we don't want to base the hash code on the reference of the object, but rather on its value (by default, the hash code is calculated based on the reference).\\n - In case of a value type, we want to base the hash code on only a subset of its fields (by default, the hash code depends on all of the fields).\\n\\nThis post won't be about what algorithm to use when calculating the actual hash value. That's a much deeper topic, and it's not particularly my area of expertise. If you're interested in a guide for that, check out chapter 3.9 of Effective Java, that is a good starting point.\\n\\nHowever, here are some general rules of thumb to follow (source: [Object.GetHashCode](https://msdn.microsoft.com/en-us/library/system.object.gethashcode(v=vs.110).aspx) documentation on MSDN).\\n\\n - Equality of hash keys does not imply the equality of the objects itself. This is logical, because there are much more possible different objects than different hash codes.\\n - However, if two objects are equal (so that obj.Equals(other) returns true), their hash codes should be equal too.\\n - If a class overrides GetHashCode, it should do so with Equals too, and it should work in the way defined in the above two points.\\n - During a single execution of an application, GetHashCode should consistently return the same value for an object every time, if there was no modification in the state of the object. What this means in practice is that we shouldn't make the implementation depend on random values, or the current date or time, etc.\\n - GetHashCode should not throw an exception.\\n - The implementation should be generally inexpensive to execute. Otherwise, it might slow down all the classes depending on it, for instance the HashTable and the Dictionary.  \\nIn practice this means that we shouldn't do anything *crazy* in it, we shouldn't start a background thread, do a DB-query or an HTTP-call. Also, the implementation should be fairly simple in order to execute fast.  \\n**Note**: Sometimes it makes sense to make a hash function complicated, so it's *guaranteed* that it takes a long time to compute, so that an attacker cannot mass-compute the hashes of many values. Such a function is called a *cryptographic* hash function, but that shouldn't be implemented with Object.GetHashCode. If you're interested, look up the [HashAlgorithm](https://msdn.microsoft.com/en-us/library/system.security.cryptography.hashalgorithm(v=vs.110).aspx) class.\\n\\nThese are all requirements on which classes using hash codes can depend on. So if you want to use GetHashCode in your own code, then for example you don't have to handle exceptions, or worry about calling it many times slowing your application down, because every well-behaving .NET class should have a GetHashCode implementation which is fast, and does not throw.\\n\\n# Mutability\\n\\nEven if we adhere to the above points, we can have a different set of problems when mutability comes into the picture.\\n\\nThe root cause of the problem is the same as what we looked at in the previous post when discussing the built-in GetHash-code implementation of value types: we get the hash code by calling GetHashCode, the implementation of which depends on some mutable field of the object.  \\nThen we process that object depending on the code, for instance we put it into a bucket of a hash map.\\nIf we mutate the object so that its GetHashCode method returns a different value, then the state of our program becomes inconsistent. That's what we saw in the previous post, when we weren't able to find our entry in the Dictionary any more.\\n\\nAn easy solution to this problem is to follow the general rule, that no mutable fields should be used in the implementation of GetHashCode. As we've seen in the previous post, ReSharper warns you about this:\\n![Visual Studio warning us about using a mutable field in GetHashCode](/content/images/2015/08/vs-warning.png)\\n\\n# The importance of Equals\\n\\nAs noted above, if we override GetHashCode, we must do so with Equals, in a way that they are \\\"consistent\\\", so that if two objects are equal, then they also have the same hash code.  \\nThe classes using GetHashCode depend on this requirement, so if we don't adhere to it, then we quickly get into an inconsistent state.\\n\\nThe following example shows a class which has an incorrect implementation.\\n\\n```csharp\\nvar testDict = new Dictionary<WrongClass, string>();\\n\\nvar key = new WrongClass(\\\"key1\\\");\\ntestDict.Add(key, \\\"content\\\");\\n\\nvar contains1 = testDict.ContainsKey(key); // Returns true\\nvar contains2 = testDict.ContainsKey(new WrongClass(\\\"key1\\\")); // Returns false\\n\\nvar entry1 = testDict[key]; // Returns the entry\\nvar entry2 = testDict[new WrongClass(\\\"key1\\\")]; // Throws KeyNotFoundException\\n```\\n\\n# Conclusion\\n\\nWith this I finish this series of posts about the Dictionary. When using built-in classes of the framework, it's always nice to have at least some understanding of what is going on under the hood. Maybe in the future I'll take a look at another class of the .NET Framework.  \\nAnd if you're uncertain, you can always take a look at the source code at the [Reference Source](http://referencesource.microsoft.com/).\"}]],\"markups\":[],\"sections\":[[10,0]]}",
        "html": "<p><strong>Posts in this series</strong>:</p>\n<ol>\n<li><a href=\"/back-to-basics-dictionary-part-1\">Part 1: Hash tables</a></li>\n<li><a href=\"/back-to-basics-dictionary-part-2-net-implementation\">Part 2: .NET implementation</a></li>\n<li><a href=\"/back-to-basics-dictionary-part-3-built-in-gethashcode\">Part 3: Built-in GetHashCode</a></li>\n<li><a href=\"/back-to-basics-dictionary-part-4-custom-gethashcode\">Part 4: Custom GetHashCode</a></li>\n</ol>\n<h1 id=\"generalguidelines\">General guidelines</h1>\n<p>This is the last part in the series about the Dictionary class and the GetHashCode method. In this post we'll take a look at what to look out for when implementing a custom GetHashCode method. In the <a href=\"/back-to-basics-dictionary-part-3-built-in-gethashcode/\">previous post</a> we've seen how the built-in GetHashCode works.</p>\n<p>We create a custom implementation when we want to deviate from the default behavior, namely:</p>\n<ul>\n<li>In case of a reference type, we don't want to base the hash code on the reference of the object, but rather on its value (by default, the hash code is calculated based on the reference).</li>\n<li>In case of a value type, we want to base the hash code on only a subset of its fields (by default, the hash code depends on all of the fields).</li>\n</ul>\n<p>This post won't be about what algorithm to use when calculating the actual hash value. That's a much deeper topic, and it's not particularly my area of expertise. If you're interested in a guide for that, check out chapter 3.9 of Effective Java, that is a good starting point.</p>\n<p>However, here are some general rules of thumb to follow (source: <a href=\"https://msdn.microsoft.com/en-us/library/system.object.gethashcode(v=vs.110).aspx\">Object.GetHashCode</a> documentation on MSDN).</p>\n<ul>\n<li>Equality of hash keys does not imply the equality of the objects itself. This is logical, because there are much more possible different objects than different hash codes.</li>\n<li>However, if two objects are equal (so that obj.Equals(other) returns true), their hash codes should be equal too.</li>\n<li>If a class overrides GetHashCode, it should do so with Equals too, and it should work in the way defined in the above two points.</li>\n<li>During a single execution of an application, GetHashCode should consistently return the same value for an object every time, if there was no modification in the state of the object. What this means in practice is that we shouldn't make the implementation depend on random values, or the current date or time, etc.</li>\n<li>GetHashCode should not throw an exception.</li>\n<li>The implementation should be generally inexpensive to execute. Otherwise, it might slow down all the classes depending on it, for instance the HashTable and the Dictionary.<br>\nIn practice this means that we shouldn't do anything <em>crazy</em> in it, we shouldn't start a background thread, do a DB-query or an HTTP-call. Also, the implementation should be fairly simple in order to execute fast.<br>\n<strong>Note</strong>: Sometimes it makes sense to make a hash function complicated, so it's <em>guaranteed</em> that it takes a long time to compute, so that an attacker cannot mass-compute the hashes of many values. Such a function is called a <em>cryptographic</em> hash function, but that shouldn't be implemented with Object.GetHashCode. If you're interested, look up the <a href=\"https://msdn.microsoft.com/en-us/library/system.security.cryptography.hashalgorithm(v=vs.110).aspx\">HashAlgorithm</a> class.</li>\n</ul>\n<p>These are all requirements on which classes using hash codes can depend on. So if you want to use GetHashCode in your own code, then for example you don't have to handle exceptions, or worry about calling it many times slowing your application down, because every well-behaving .NET class should have a GetHashCode implementation which is fast, and does not throw.</p>\n<h1 id=\"mutability\">Mutability</h1>\n<p>Even if we adhere to the above points, we can have a different set of problems when mutability comes into the picture.</p>\n<p>The root cause of the problem is the same as what we looked at in the previous post when discussing the built-in GetHash-code implementation of value types: we get the hash code by calling GetHashCode, the implementation of which depends on some mutable field of the object.<br>\nThen we process that object depending on the code, for instance we put it into a bucket of a hash map.<br>\nIf we mutate the object so that its GetHashCode method returns a different value, then the state of our program becomes inconsistent. That's what we saw in the previous post, when we weren't able to find our entry in the Dictionary any more.</p>\n<p>An easy solution to this problem is to follow the general rule, that no mutable fields should be used in the implementation of GetHashCode. As we've seen in the previous post, ReSharper warns you about this:<br>\n<img src=\"/content/images/2015/08/vs-warning.png\" alt=\"Visual Studio warning us about using a mutable field in GetHashCode\"></p>\n<h1 id=\"theimportanceofequals\">The importance of Equals</h1>\n<p>As noted above, if we override GetHashCode, we must do so with Equals, in a way that they are &quot;consistent&quot;, so that if two objects are equal, then they also have the same hash code.<br>\nThe classes using GetHashCode depend on this requirement, so if we don't adhere to it, then we quickly get into an inconsistent state.</p>\n<p>The following example shows a class which has an incorrect implementation.</p>\n<pre><code class=\"language-csharp\">var testDict = new Dictionary&lt;WrongClass, string&gt;();\n\nvar key = new WrongClass(&quot;key1&quot;);\ntestDict.Add(key, &quot;content&quot;);\n\nvar contains1 = testDict.ContainsKey(key); // Returns true\nvar contains2 = testDict.ContainsKey(new WrongClass(&quot;key1&quot;)); // Returns false\n\nvar entry1 = testDict[key]; // Returns the entry\nvar entry2 = testDict[new WrongClass(&quot;key1&quot;)]; // Throws KeyNotFoundException\n</code></pre>\n<h1 id=\"conclusion\">Conclusion</h1>\n<p>With this I finish this series of posts about the Dictionary. When using built-in classes of the framework, it's always nice to have at least some understanding of what is going on under the hood. Maybe in the future I'll take a look at another class of the .NET Framework.<br>\nAnd if you're uncertain, you can always take a look at the source code at the <a href=\"http://referencesource.microsoft.com/\">Reference Source</a>.</p>\n",
        "comment_id": "9",
        "plaintext": "Posts in this series:\n\n 1. Part 1: Hash tables [/back-to-basics-dictionary-part-1]\n 2. Part 2: .NET implementation\n    [/back-to-basics-dictionary-part-2-net-implementation]\n 3. Part 3: Built-in GetHashCode\n    [/back-to-basics-dictionary-part-3-built-in-gethashcode]\n 4. Part 4: Custom GetHashCode\n    [/back-to-basics-dictionary-part-4-custom-gethashcode]\n\nGeneral guidelines\nThis is the last part in the series about the Dictionary class and the\nGetHashCode method. In this post we'll take a look at what to look out for when\nimplementing a custom GetHashCode method. In the previous post\n[/back-to-basics-dictionary-part-3-built-in-gethashcode/]  we've seen how the\nbuilt-in GetHashCode works.\n\nWe create a custom implementation when we want to deviate from the default\nbehavior, namely:\n\n * In case of a reference type, we don't want to base the hash code on the\n   reference of the object, but rather on its value (by default, the hash code\n   is calculated based on the reference).\n * In case of a value type, we want to base the hash code on only a subset of\n   its fields (by default, the hash code depends on all of the fields).\n\nThis post won't be about what algorithm to use when calculating the actual hash\nvalue. That's a much deeper topic, and it's not particularly my area of\nexpertise. If you're interested in a guide for that, check out chapter 3.9 of\nEffective Java, that is a good starting point.\n\nHowever, here are some general rules of thumb to follow (source: \nObject.GetHashCode\n[https://msdn.microsoft.com/en-us/library/system.object.gethashcode(v=vs.110).aspx] \n documentation on MSDN).\n\n * Equality of hash keys does not imply the equality of the objects itself. This\n   is logical, because there are much more possible different objects than\n   different hash codes.\n * However, if two objects are equal (so that obj.Equals(other) returns true),\n   their hash codes should be equal too.\n * If a class overrides GetHashCode, it should do so with Equals too, and it\n   should work in the way defined in the above two points.\n * During a single execution of an application, GetHashCode should consistently\n   return the same value for an object every time, if there was no modification\n   in the state of the object. What this means in practice is that we shouldn't\n   make the implementation depend on random values, or the current date or time,\n   etc.\n * GetHashCode should not throw an exception.\n * The implementation should be generally inexpensive to execute. Otherwise, it\n   might slow down all the classes depending on it, for instance the HashTable\n   and the Dictionary.\n   In practice this means that we shouldn't do anything crazy  in it, we\n   shouldn't start a background thread, do a DB-query or an HTTP-call. Also, the\n   implementation should be fairly simple in order to execute fast.\n   Note: Sometimes it makes sense to make a hash function complicated, so it's \n   guaranteed  that it takes a long time to compute, so that an attacker cannot\n   mass-compute the hashes of many values. Such a function is called a \n   cryptographic  hash function, but that shouldn't be implemented with\n   Object.GetHashCode. If you're interested, look up the HashAlgorithm\n   [https://msdn.microsoft.com/en-us/library/system.security.cryptography.hashalgorithm(v=vs.110).aspx] \n    class.\n\nThese are all requirements on which classes using hash codes can depend on. So\nif you want to use GetHashCode in your own code, then for example you don't have\nto handle exceptions, or worry about calling it many times slowing your\napplication down, because every well-behaving .NET class should have a\nGetHashCode implementation which is fast, and does not throw.\n\nMutability\nEven if we adhere to the above points, we can have a different set of problems\nwhen mutability comes into the picture.\n\nThe root cause of the problem is the same as what we looked at in the previous\npost when discussing the built-in GetHash-code implementation of value types: we\nget the hash code by calling GetHashCode, the implementation of which depends on\nsome mutable field of the object.\nThen we process that object depending on the code, for instance we put it into a\nbucket of a hash map.\nIf we mutate the object so that its GetHashCode method returns a different\nvalue, then the state of our program becomes inconsistent. That's what we saw in\nthe previous post, when we weren't able to find our entry in the Dictionary any\nmore.\n\nAn easy solution to this problem is to follow the general rule, that no mutable\nfields should be used in the implementation of GetHashCode. As we've seen in the\nprevious post, ReSharper warns you about this:\n\n\nThe importance of Equals\nAs noted above, if we override GetHashCode, we must do so with Equals, in a way\nthat they are \"consistent\", so that if two objects are equal, then they also\nhave the same hash code.\nThe classes using GetHashCode depend on this requirement, so if we don't adhere\nto it, then we quickly get into an inconsistent state.\n\nThe following example shows a class which has an incorrect implementation.\n\nvar testDict = new Dictionary<WrongClass, string>();\n\nvar key = new WrongClass(\"key1\");\ntestDict.Add(key, \"content\");\n\nvar contains1 = testDict.ContainsKey(key); // Returns true\nvar contains2 = testDict.ContainsKey(new WrongClass(\"key1\")); // Returns false\n\nvar entry1 = testDict[key]; // Returns the entry\nvar entry2 = testDict[new WrongClass(\"key1\")]; // Throws KeyNotFoundException\n\n\nConclusion\nWith this I finish this series of posts about the Dictionary. When using\nbuilt-in classes of the framework, it's always nice to have at least some\nunderstanding of what is going on under the hood. Maybe in the future I'll take\na look at another class of the .NET Framework.\nAnd if you're uncertain, you can always take a look at the source code at the \nReference Source [http://referencesource.microsoft.com/].",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "author_id": "1",
        "created_at": "2015-10-25 16:06:31",
        "created_by": "1",
        "updated_at": "2019-02-20 20:05:10",
        "updated_by": "1",
        "published_at": "2015-10-25 18:53:07",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2964",
        "uuid": "5960f61b-ae07-42d8-b83c-6de3cd6bc5a3",
        "title": "Couchbase Server: tips for troubleshooting issues",
        "slug": "couchbase-server-tips-for-troubleshooting-issues",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Recently at work we started using Couchbase Server to replace a rather outdated caching solution in our architecture. This was the first time I had to use Couchbase and its .NET SDK, and I have encountered a couple of issues along the way.\\r\\n\\r\\nThis post is a recollection of the problems we faced. (If you are interested in a \\\"getting started\\\" tutorial, I recommend reading the [official documentation](http://docs.couchbase.com/developer/dotnet-2.0/dotnet-intro.html) of the .NET SDK, it has a pretty good description about what you can with the SDK, and the API is rather simple, so you shouldn't have a hard time getting started with it.)\\r\\n\\r\\n#Logging\\r\\n\\r\\nProbably the single most important tool for troubleshooting problems for me was using the built-in logging of the .NET SDK. It can give much more detailed insights into the mechanisms going on in the SDK than what you can get from the `IDocumentResult` or `OperationResult` object you get from the API.\\r\\n\\r\\nThe SDK uses `Common.Logging` and `log4net`. Enabling file-based logging is very simple, you basically just have to add the following section to your app or web configuration file.\\r\\n\\r\\n    <configuration>\\r\\n      <configSections>\\r\\n        <sectionGroup name=\\\"common\\\">\\r\\n          <section name=\\\"logging\\\" type=\\\"Common.Logging.ConfigurationSectionHandler, Common.Logging\\\" />\\r\\n        </sectionGroup>\\r\\n        <section name=\\\"log4net\\\" type=\\\"log4net.Config.Log4NetConfigurationSectionHandler, log4net\\\" />\\r\\n      </configSections>\\r\\n\\r\\n      <common>\\r\\n        <logging>\\r\\n          <factoryAdapter type=\\\"Common.Logging.Log4Net.Log4NetLoggerFactoryAdapter, Common.Logging.Log4Net\\\">\\r\\n            <arg key=\\\"configType\\\" value=\\\"INLINE\\\" />\\r\\n          </factoryAdapter>\\r\\n        </logging>\\r\\n      </common>\\r\\n      \\r\\n      <log4net>\\r\\n        <appender name=\\\"FileAppender\\\" type=\\\"log4net.Appender.FileAppender\\\">\\r\\n        <param name=\\\"File\\\" value=\\\"C:\\\\temp\\\\log.txt\\\" />\\r\\n        <layout type=\\\"log4net.Layout.PatternLayout\\\">\\r\\n          <conversionPattern value=\\\"%date [%thread] %level %logger - %message%newline\\\" />\\r\\n        </layout>\\r\\n        </appender>\\r\\n        <root>\\r\\n          <level value=\\\"INFO\\\" />\\r\\n          <appender-ref ref=\\\"FileAppender\\\" />\\r\\n        </root> \\r\\n      </log4net>\\r\\n    </configuration>\\r\\n\\r\\nSource and more details on the official Couchbase blog: [Couchbase .NET SDK 2.0 Development Series: Logging](http://blog.couchbase.com/couchbase-net-sdk-20-development-series-logging)\\r\\n\\r\\n#Using Fiddler\\r\\n\\r\\nWhen troubleshooting timeout problems or sluggishness, it can be beneficial to take a look at what kind of requests the SDK is trying to send, and what ports it is trying to access.  \\r\\nYou don't necessarily have to understand what the requests are about, it is often enough to \\r\\nsee which request is failing to get some clue about the problem.\\r\\n\\r\\n![Screenshot illustrating Couchbase requests captured with Fiddler.](/content/images/2016/01/couchbase-fiddler.png)\\r\\n\\r\\nKeep in mind that you might have to explicitly configure your application to use the proxy provided by Fiddler, you can find the details [here](http://docs.telerik.com/fiddler/Configure-Fiddler/Tasks/ConfigureDotNETApp).  \\r\\nAlso, if you have your Couchbase server installation on the local machine, you have to use `ipv4.fiddler` instead of `localhost` in the url configuration.\\r\\n\\r\\n#Networking issues\\r\\n\\r\\nIt is pretty easy to run into networking issues when using Couchbase, especially if the server and the client run in a different network or environment, for example in different clouds.\\r\\n\\r\\nOne of the reasons for these issues can often be a firewall between the two computers, because Couchbase uses many different ports, and not just port 80. You can find the list of these ports in the [official documentation](http://docs.couchbase.com/admin/admin/Install/install-networkPorts.html).\\r\\n\\r\\nThis can cause either the connection to not work at all, or can cause long response times of the SDK.  \\r\\nLuckily these issues are usually easy to spot, because they produce an error in the logs, or you can see the failed requests with Fiddler, and opening the port in the firewall solves the problem.\\r\\n\\r\\nThere is still one problem I couldn't solve. If I hosted a Couchbase cluster in Amazon EC2, and tried to access it from a virtual machine in Azure, I got randomly slow response times. You can find the details in this [Stack Overflow question](http://stackoverflow.com/questions/34204339/use-couchbase-hosted-in-amazon-from-a-client-hosted-in-azure-what-can-cause-bad?noredirect=1#comment56154827_34204339). Luckily this was only a test setup, our production architecture is different, but it would be still interesting to find the root cause of this.\\r\\n\\r\\n#Default `<bucket>` configuration\\r\\n\\r\\nThere is a surprising gotcha related to the bucket configurations of the SDK.  \\r\\nThere are two ways to open buckets: you can either statically preconfigure the bucket to use in the app.config:\\r\\n\\r\\n    <couchbaseClients>\\r\\n      <couchbase useSsl=\\\"false\\\" operationLifeSpan=\\\"1000\\\">\\r\\n        <servers>\\r\\n          <add uri=\\\"http://192.168.56.101:8091/pools\\\"></add>\\r\\n        </servers>\\r\\n        <buckets>\\r\\n          <add name=\\\"my-bucket-name\\\" useSsl=\\\"false\\\" password=\\\"\\\" operationLifespan=\\\"2000\\\">\\r\\n            <connectionPool name=\\\"custom\\\" maxSize=\\\"10\\\" minSize=\\\"5\\\" sendTimeout=\\\"12000\\\"></connectionPool>\\r\\n          </add>\\r\\n        </buckets>\\r\\n      </couchbase>\\r\\n    </couchbaseClients>\\r\\n\\r\\nIn this case if you try to open the bucket `my-bucket-name`, its configuration will be picked up from the config file.\\r\\n\\r\\nHowever, there can be situations in which the name of the buckets you want to use are only known dynamically during runtime, so you cannot preconfigure any buckets. In that case you `couchbaseClients` section will look like this:\\r\\n\\r\\n    <couchbaseClients>\\r\\n      <couchbase useSsl=\\\"false\\\" operationLifeSpan=\\\"1000\\\">\\r\\n        <servers>\\r\\n          <add uri=\\\"http://192.168.56.101:8091/pools\\\"></add>\\r\\n        </servers>\\r\\n      </couchbase>\\r\\n    </couchbaseClients>\\r\\n\\r\\nThe weird thing is that now if you try to open a bucket by using `cluster.OpenBucket(bucketName)`, the SDK does not pick up the server configuration from the `<servers>` section, but at first it tries to connect to `localhost`, and it only uses the proper server after that fails.  \\r\\nThis can have two consequences:\\r\\n\\r\\n - If we have no Couchbase server running at localhost:8091, opening the bucket can take seconds, because the client is trying to connect to localhost at first, and only after that fails does it connect to the configured server. This problem and a workaround is described in [this SO question](http://stackoverflow.com/questions/34183342/why-can-opening-a-couchbase-bucket-be-very-slow).\\r\\n - If we actually do have a server at localhost:8091, then that is getting used instead of the one configured in `<servers>`. I consider this clearly a bug.\\r\\n\\r\\nThe workaround is simple: you just have to add a dummy bucket to the configuration:\\r\\n\\r\\n    <couchbaseClients>\\r\\n      <couchbase useSsl=\\\"false\\\" operationLifeSpan=\\\"1000\\\">\\r\\n        <servers>\\r\\n          <add uri=\\\"http://192.168.56.101:8091/pools\\\"></add>\\r\\n        </servers>\\r\\n        <buckets>\\r\\n          <add name=\\\"dummy-bucket\\\" useSsl=\\\"false\\\" operationLifespan=\\\"1000\\\">\\r\\n          </add>\\r\\n        </buckets>\\r\\n      </couchbase>\\r\\n    </couchbaseClients>\\r\\n\\r\\nA bucket with the specified name doesn't even have to exist on the server. Solely the existence of that section will make the SDK pick up the server configuration from the `<servers>` section.\\r\\n\\r\\nYou can find some more details in [this SO question](http://stackoverflow.com/questions/34183342/why-can-opening-a-couchbase-bucket-be-very-slow), and my proposed fix in the [following pull request](https://github.com/couchbase/couchbase-net-client/pull/52).\\r\\n\\r\\n#TCP connection count\\r\\n\\r\\nIf you are accessing Couchbase at a high volume, possibly on multiple threads, the client can easily run out of available TCP connections. The maximum number of TCP connections can be configured in the app.config. Its default value is 2, which is really low, and it can cause the operation to fail. In the logs and in the result object you can see an exception similar to this.\\r\\n\\r\\n    The operation has timed out.\\r\\n    Couchbase.IO.ConnectionUnavailableException: Failed to acquire a pooled client connection on 192.168.56.101:11210 after 5 tries.\\r\\n       at Couchbase.IO.ConnectionPool`1.Acquire()\\r\\n       at Couchbase.IO.ConnectionPool`1.Acquire()\\r\\n       at Couchbase.IO.ConnectionPool`1.Acquire()\\r\\n       at Couchbase.IO.ConnectionPool`1.Couchbase.IO.IConnectionPool.Acquire()\\r\\n       at Couchbase.IO.Strategies.DefaultIOStrategy.Execute[T](IOperation`1 operation)\\r\\n       at Couchbase.Core.Server.Send[T](IOperation`1 operation)\\r\\n\\r\\nThe solution is to increase the number of maximum TCP connections, which can be separately controlled on a default level, or for a specific bucket:\\r\\n\\r\\n    <couchbaseClients>\\r\\n        <couchbase>\\r\\n            <servers>\\r\\n                <add uri=\\\"http://192.168.56.101:8091/pools\\\"/>\\r\\n            </servers>\\r\\n            <connectionPool name=\\\"default\\\" maxSize=\\\"35\\\" minSize=\\\"5\\\" sendTimeout=\\\"15000\\\">\\r\\n            </connectionPool>\\r\\n            <buckets>\\r\\n                <add name=\\\"default\\\">\\r\\n                    <connectionPool maxSize=\\\"30\\\" minSize=\\\"5\\\" name=\\\"default\\\"/>\\r\\n                </add>\\r\\n            </buckets>\\r\\n        </couchbase>\\r\\n    </couchbaseClients>\\r\\n\\r\\nFor us a pool size of 30 seems to have solved the problem, you should fine tune the configuration until you find a proper value for your scenario.\\r\\n\\r\\nI found this solution with an official answer in the [Couchbase Forums](https://forums.couchbase.com/t/failed-to-acquire-a-pooled-client-connection-after-5-tries/6062/6?u=markvincze)\\r\\n\\r\\n#The importance of `ClusterHelper`\\r\\n\\r\\nWhen we use the `OpenBucket` of the `ICluster` interface, a new TCP connection will be made to the Couchbase server. That bucket object should be used in a `using` block, and when the object is disposed, the connection will be closed. The performance of this approach is not ideal if we are accessing Couchbase at a high volume.\\r\\n\\r\\nLuckily the SDK contains support for pooling TCP connection in the form of the `ClusterHelper` class. A couple of things to keep in mind:\\r\\n\\r\\n- During the startup of your application (for example in the `Global.asax.cs` in case of a web application) you need to initialize the helper by calling `ClusterHelper.Initialize(...)`.\\r\\n- When your application shuts down, you should call `ClusterHelper.Close()`.\\r\\n- If you open a bucket with `ClusterHelper.GetBucket(...)`, you **should not** put it in a using block. The ClusterHelper will internally manage and dispose the bucket objects.\\r\\n\\r\\nYou can find more details about this in the [official documentation](http://developer.couchbase.com/documentation/server/4.0/sdks/dotnet-2.2/cluster-helper.html).\\r\\n\\r\\nCouchbase is a very exciting technology, on which big companies are betting (Ebay, LinkedIn, Ryanair, etc.). It's great to be able to replace an outdated legacy caching system with something more modern, and far better scalable.  \\r\\nHowever, the above problems can be quite annoying, and they can make the introduction of Couchbase in your architecture more challenging.  \\r\\nI hope this post will help you avoid making the same mistakes, or at least spend less time looking for the solution.\"}]],\"sections\":[[10,0]]}",
        "html": "<p>Recently at work we started using Couchbase Server to replace a rather outdated caching solution in our architecture. This was the first time I had to use Couchbase and its .NET SDK, and I have encountered a couple of issues along the way.</p>\n<p>This post is a recollection of the problems we faced. (If you are interested in a &quot;getting started&quot; tutorial, I recommend reading the <a href=\"http://docs.couchbase.com/developer/dotnet-2.0/dotnet-intro.html\">official documentation</a> of the .NET SDK, it has a pretty good description about what you can with the SDK, and the API is rather simple, so you shouldn't have a hard time getting started with it.)</p>\n<h1 id=\"logging\">Logging</h1>\n<p>Probably the single most important tool for troubleshooting problems for me was using the built-in logging of the .NET SDK. It can give much more detailed insights into the mechanisms going on in the SDK than what you can get from the <code>IDocumentResult</code> or <code>OperationResult</code> object you get from the API.</p>\n<p>The SDK uses <code>Common.Logging</code> and <code>log4net</code>. Enabling file-based logging is very simple, you basically just have to add the following section to your app or web configuration file.</p>\n<pre><code>&lt;configuration&gt;\n  &lt;configSections&gt;\n    &lt;sectionGroup name=&quot;common&quot;&gt;\n      &lt;section name=&quot;logging&quot; type=&quot;Common.Logging.ConfigurationSectionHandler, Common.Logging&quot; /&gt;\n    &lt;/sectionGroup&gt;\n    &lt;section name=&quot;log4net&quot; type=&quot;log4net.Config.Log4NetConfigurationSectionHandler, log4net&quot; /&gt;\n  &lt;/configSections&gt;\n\n  &lt;common&gt;\n    &lt;logging&gt;\n      &lt;factoryAdapter type=&quot;Common.Logging.Log4Net.Log4NetLoggerFactoryAdapter, Common.Logging.Log4Net&quot;&gt;\n        &lt;arg key=&quot;configType&quot; value=&quot;INLINE&quot; /&gt;\n      &lt;/factoryAdapter&gt;\n    &lt;/logging&gt;\n  &lt;/common&gt;\n  \n  &lt;log4net&gt;\n    &lt;appender name=&quot;FileAppender&quot; type=&quot;log4net.Appender.FileAppender&quot;&gt;\n    &lt;param name=&quot;File&quot; value=&quot;C:\\temp\\log.txt&quot; /&gt;\n    &lt;layout type=&quot;log4net.Layout.PatternLayout&quot;&gt;\n      &lt;conversionPattern value=&quot;%date [%thread] %level %logger - %message%newline&quot; /&gt;\n    &lt;/layout&gt;\n    &lt;/appender&gt;\n    &lt;root&gt;\n      &lt;level value=&quot;INFO&quot; /&gt;\n      &lt;appender-ref ref=&quot;FileAppender&quot; /&gt;\n    &lt;/root&gt; \n  &lt;/log4net&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>Source and more details on the official Couchbase blog: <a href=\"http://blog.couchbase.com/couchbase-net-sdk-20-development-series-logging\">Couchbase .NET SDK 2.0 Development Series: Logging</a></p>\n<h1 id=\"usingfiddler\">Using Fiddler</h1>\n<p>When troubleshooting timeout problems or sluggishness, it can be beneficial to take a look at what kind of requests the SDK is trying to send, and what ports it is trying to access.<br>\nYou don't necessarily have to understand what the requests are about, it is often enough to<br>\nsee which request is failing to get some clue about the problem.</p>\n<p><img src=\"/content/images/2016/01/couchbase-fiddler.png\" alt=\"Screenshot illustrating Couchbase requests captured with Fiddler.\"></p>\n<p>Keep in mind that you might have to explicitly configure your application to use the proxy provided by Fiddler, you can find the details <a href=\"http://docs.telerik.com/fiddler/Configure-Fiddler/Tasks/ConfigureDotNETApp\">here</a>.<br>\nAlso, if you have your Couchbase server installation on the local machine, you have to use <code>ipv4.fiddler</code> instead of <code>localhost</code> in the url configuration.</p>\n<h1 id=\"networkingissues\">Networking issues</h1>\n<p>It is pretty easy to run into networking issues when using Couchbase, especially if the server and the client run in a different network or environment, for example in different clouds.</p>\n<p>One of the reasons for these issues can often be a firewall between the two computers, because Couchbase uses many different ports, and not just port 80. You can find the list of these ports in the <a href=\"http://docs.couchbase.com/admin/admin/Install/install-networkPorts.html\">official documentation</a>.</p>\n<p>This can cause either the connection to not work at all, or can cause long response times of the SDK.<br>\nLuckily these issues are usually easy to spot, because they produce an error in the logs, or you can see the failed requests with Fiddler, and opening the port in the firewall solves the problem.</p>\n<p>There is still one problem I couldn't solve. If I hosted a Couchbase cluster in Amazon EC2, and tried to access it from a virtual machine in Azure, I got randomly slow response times. You can find the details in this <a href=\"http://stackoverflow.com/questions/34204339/use-couchbase-hosted-in-amazon-from-a-client-hosted-in-azure-what-can-cause-bad?noredirect=1#comment56154827_34204339\">Stack Overflow question</a>. Luckily this was only a test setup, our production architecture is different, but it would be still interesting to find the root cause of this.</p>\n<h1 id=\"defaultbucketconfiguration\">Default <code>&lt;bucket&gt;</code> configuration</h1>\n<p>There is a surprising gotcha related to the bucket configurations of the SDK.<br>\nThere are two ways to open buckets: you can either statically preconfigure the bucket to use in the app.config:</p>\n<pre><code>&lt;couchbaseClients&gt;\n  &lt;couchbase useSsl=&quot;false&quot; operationLifeSpan=&quot;1000&quot;&gt;\n    &lt;servers&gt;\n      &lt;add uri=&quot;http://192.168.56.101:8091/pools&quot;&gt;&lt;/add&gt;\n    &lt;/servers&gt;\n    &lt;buckets&gt;\n      &lt;add name=&quot;my-bucket-name&quot; useSsl=&quot;false&quot; password=&quot;&quot; operationLifespan=&quot;2000&quot;&gt;\n        &lt;connectionPool name=&quot;custom&quot; maxSize=&quot;10&quot; minSize=&quot;5&quot; sendTimeout=&quot;12000&quot;&gt;&lt;/connectionPool&gt;\n      &lt;/add&gt;\n    &lt;/buckets&gt;\n  &lt;/couchbase&gt;\n&lt;/couchbaseClients&gt;\n</code></pre>\n<p>In this case if you try to open the bucket <code>my-bucket-name</code>, its configuration will be picked up from the config file.</p>\n<p>However, there can be situations in which the name of the buckets you want to use are only known dynamically during runtime, so you cannot preconfigure any buckets. In that case you <code>couchbaseClients</code> section will look like this:</p>\n<pre><code>&lt;couchbaseClients&gt;\n  &lt;couchbase useSsl=&quot;false&quot; operationLifeSpan=&quot;1000&quot;&gt;\n    &lt;servers&gt;\n      &lt;add uri=&quot;http://192.168.56.101:8091/pools&quot;&gt;&lt;/add&gt;\n    &lt;/servers&gt;\n  &lt;/couchbase&gt;\n&lt;/couchbaseClients&gt;\n</code></pre>\n<p>The weird thing is that now if you try to open a bucket by using <code>cluster.OpenBucket(bucketName)</code>, the SDK does not pick up the server configuration from the <code>&lt;servers&gt;</code> section, but at first it tries to connect to <code>localhost</code>, and it only uses the proper server after that fails.<br>\nThis can have two consequences:</p>\n<ul>\n<li>If we have no Couchbase server running at localhost:8091, opening the bucket can take seconds, because the client is trying to connect to localhost at first, and only after that fails does it connect to the configured server. This problem and a workaround is described in <a href=\"http://stackoverflow.com/questions/34183342/why-can-opening-a-couchbase-bucket-be-very-slow\">this SO question</a>.</li>\n<li>If we actually do have a server at localhost:8091, then that is getting used instead of the one configured in <code>&lt;servers&gt;</code>. I consider this clearly a bug.</li>\n</ul>\n<p>The workaround is simple: you just have to add a dummy bucket to the configuration:</p>\n<pre><code>&lt;couchbaseClients&gt;\n  &lt;couchbase useSsl=&quot;false&quot; operationLifeSpan=&quot;1000&quot;&gt;\n    &lt;servers&gt;\n      &lt;add uri=&quot;http://192.168.56.101:8091/pools&quot;&gt;&lt;/add&gt;\n    &lt;/servers&gt;\n    &lt;buckets&gt;\n      &lt;add name=&quot;dummy-bucket&quot; useSsl=&quot;false&quot; operationLifespan=&quot;1000&quot;&gt;\n      &lt;/add&gt;\n    &lt;/buckets&gt;\n  &lt;/couchbase&gt;\n&lt;/couchbaseClients&gt;\n</code></pre>\n<p>A bucket with the specified name doesn't even have to exist on the server. Solely the existence of that section will make the SDK pick up the server configuration from the <code>&lt;servers&gt;</code> section.</p>\n<p>You can find some more details in <a href=\"http://stackoverflow.com/questions/34183342/why-can-opening-a-couchbase-bucket-be-very-slow\">this SO question</a>, and my proposed fix in the <a href=\"https://github.com/couchbase/couchbase-net-client/pull/52\">following pull request</a>.</p>\n<h1 id=\"tcpconnectioncount\">TCP connection count</h1>\n<p>If you are accessing Couchbase at a high volume, possibly on multiple threads, the client can easily run out of available TCP connections. The maximum number of TCP connections can be configured in the app.config. Its default value is 2, which is really low, and it can cause the operation to fail. In the logs and in the result object you can see an exception similar to this.</p>\n<pre><code>The operation has timed out.\nCouchbase.IO.ConnectionUnavailableException: Failed to acquire a pooled client connection on 192.168.56.101:11210 after 5 tries.\n   at Couchbase.IO.ConnectionPool`1.Acquire()\n   at Couchbase.IO.ConnectionPool`1.Acquire()\n   at Couchbase.IO.ConnectionPool`1.Acquire()\n   at Couchbase.IO.ConnectionPool`1.Couchbase.IO.IConnectionPool.Acquire()\n   at Couchbase.IO.Strategies.DefaultIOStrategy.Execute[T](IOperation`1 operation)\n   at Couchbase.Core.Server.Send[T](IOperation`1 operation)\n</code></pre>\n<p>The solution is to increase the number of maximum TCP connections, which can be separately controlled on a default level, or for a specific bucket:</p>\n<pre><code>&lt;couchbaseClients&gt;\n    &lt;couchbase&gt;\n        &lt;servers&gt;\n            &lt;add uri=&quot;http://192.168.56.101:8091/pools&quot;/&gt;\n        &lt;/servers&gt;\n        &lt;connectionPool name=&quot;default&quot; maxSize=&quot;35&quot; minSize=&quot;5&quot; sendTimeout=&quot;15000&quot;&gt;\n        &lt;/connectionPool&gt;\n        &lt;buckets&gt;\n            &lt;add name=&quot;default&quot;&gt;\n                &lt;connectionPool maxSize=&quot;30&quot; minSize=&quot;5&quot; name=&quot;default&quot;/&gt;\n            &lt;/add&gt;\n        &lt;/buckets&gt;\n    &lt;/couchbase&gt;\n&lt;/couchbaseClients&gt;\n</code></pre>\n<p>For us a pool size of 30 seems to have solved the problem, you should fine tune the configuration until you find a proper value for your scenario.</p>\n<p>I found this solution with an official answer in the <a href=\"https://forums.couchbase.com/t/failed-to-acquire-a-pooled-client-connection-after-5-tries/6062/6?u=markvincze\">Couchbase Forums</a></p>\n<h1 id=\"theimportanceofclusterhelper\">The importance of <code>ClusterHelper</code></h1>\n<p>When we use the <code>OpenBucket</code> of the <code>ICluster</code> interface, a new TCP connection will be made to the Couchbase server. That bucket object should be used in a <code>using</code> block, and when the object is disposed, the connection will be closed. The performance of this approach is not ideal if we are accessing Couchbase at a high volume.</p>\n<p>Luckily the SDK contains support for pooling TCP connection in the form of the <code>ClusterHelper</code> class. A couple of things to keep in mind:</p>\n<ul>\n<li>During the startup of your application (for example in the <code>Global.asax.cs</code> in case of a web application) you need to initialize the helper by calling <code>ClusterHelper.Initialize(...)</code>.</li>\n<li>When your application shuts down, you should call <code>ClusterHelper.Close()</code>.</li>\n<li>If you open a bucket with <code>ClusterHelper.GetBucket(...)</code>, you <strong>should not</strong> put it in a using block. The ClusterHelper will internally manage and dispose the bucket objects.</li>\n</ul>\n<p>You can find more details about this in the <a href=\"http://developer.couchbase.com/documentation/server/4.0/sdks/dotnet-2.2/cluster-helper.html\">official documentation</a>.</p>\n<p>Couchbase is a very exciting technology, on which big companies are betting (Ebay, LinkedIn, Ryanair, etc.). It's great to be able to replace an outdated legacy caching system with something more modern, and far better scalable.<br>\nHowever, the above problems can be quite annoying, and they can make the introduction of Couchbase in your architecture more challenging.<br>\nI hope this post will help you avoid making the same mistakes, or at least spend less time looking for the solution.</p>\n",
        "comment_id": "10",
        "plaintext": "Recently at work we started using Couchbase Server to replace a rather outdated\ncaching solution in our architecture. This was the first time I had to use\nCouchbase and its .NET SDK, and I have encountered a couple of issues along the\nway.\n\nThis post is a recollection of the problems we faced. (If you are interested in\na \"getting started\" tutorial, I recommend reading the official documentation\n[http://docs.couchbase.com/developer/dotnet-2.0/dotnet-intro.html]  of the .NET\nSDK, it has a pretty good description about what you can with the SDK, and the\nAPI is rather simple, so you shouldn't have a hard time getting started with\nit.)\n\nLogging\nProbably the single most important tool for troubleshooting problems for me was\nusing the built-in logging of the .NET SDK. It can give much more detailed\ninsights into the mechanisms going on in the SDK than what you can get from the \nIDocumentResult  or OperationResult  object you get from the API.\n\nThe SDK uses Common.Logging  and log4net. Enabling file-based logging is very\nsimple, you basically just have to add the following section to your app or web\nconfiguration file.\n\n<configuration>\n  <configSections>\n    <sectionGroup name=\"common\">\n      <section name=\"logging\" type=\"Common.Logging.ConfigurationSectionHandler, Common.Logging\" />\n    </sectionGroup>\n    <section name=\"log4net\" type=\"log4net.Config.Log4NetConfigurationSectionHandler, log4net\" />\n  </configSections>\n\n  <common>\n    <logging>\n      <factoryAdapter type=\"Common.Logging.Log4Net.Log4NetLoggerFactoryAdapter, Common.Logging.Log4Net\">\n        <arg key=\"configType\" value=\"INLINE\" />\n      </factoryAdapter>\n    </logging>\n  </common>\n  \n  <log4net>\n    <appender name=\"FileAppender\" type=\"log4net.Appender.FileAppender\">\n    <param name=\"File\" value=\"C:\\temp\\log.txt\" />\n    <layout type=\"log4net.Layout.PatternLayout\">\n      <conversionPattern value=\"%date [%thread] %level %logger - %message%newline\" />\n    </layout>\n    </appender>\n    <root>\n      <level value=\"INFO\" />\n      <appender-ref ref=\"FileAppender\" />\n    </root> \n  </log4net>\n</configuration>\n\n\nSource and more details on the official Couchbase blog: Couchbase .NET SDK 2.0\nDevelopment Series: Logging\n[http://blog.couchbase.com/couchbase-net-sdk-20-development-series-logging]\n\nUsing Fiddler\nWhen troubleshooting timeout problems or sluggishness, it can be beneficial to\ntake a look at what kind of requests the SDK is trying to send, and what ports\nit is trying to access.\nYou don't necessarily have to understand what the requests are about, it is\noften enough to\nsee which request is failing to get some clue about the problem.\n\n\n\nKeep in mind that you might have to explicitly configure your application to use\nthe proxy provided by Fiddler, you can find the details here\n[http://docs.telerik.com/fiddler/Configure-Fiddler/Tasks/ConfigureDotNETApp].\nAlso, if you have your Couchbase server installation on the local machine, you\nhave to use ipv4.fiddler  instead of localhost  in the url configuration.\n\nNetworking issues\nIt is pretty easy to run into networking issues when using Couchbase, especially\nif the server and the client run in a different network or environment, for\nexample in different clouds.\n\nOne of the reasons for these issues can often be a firewall between the two\ncomputers, because Couchbase uses many different ports, and not just port 80.\nYou can find the list of these ports in the official documentation\n[http://docs.couchbase.com/admin/admin/Install/install-networkPorts.html].\n\nThis can cause either the connection to not work at all, or can cause long\nresponse times of the SDK.\nLuckily these issues are usually easy to spot, because they produce an error in\nthe logs, or you can see the failed requests with Fiddler, and opening the port\nin the firewall solves the problem.\n\nThere is still one problem I couldn't solve. If I hosted a Couchbase cluster in\nAmazon EC2, and tried to access it from a virtual machine in Azure, I got\nrandomly slow response times. You can find the details in this Stack Overflow\nquestion. Luckily this was only a test setup, our production architecture is\ndifferent, but it would be still interesting to find the root cause of this.\n\nDefault <bucket>  configuration\nThere is a surprising gotcha related to the bucket configurations of the SDK.\nThere are two ways to open buckets: you can either statically preconfigure the\nbucket to use in the app.config:\n\n<couchbaseClients>\n  <couchbase useSsl=\"false\" operationLifeSpan=\"1000\">\n    <servers>\n      <add uri=\"http://192.168.56.101:8091/pools\"></add>\n    </servers>\n    <buckets>\n      <add name=\"my-bucket-name\" useSsl=\"false\" password=\"\" operationLifespan=\"2000\">\n        <connectionPool name=\"custom\" maxSize=\"10\" minSize=\"5\" sendTimeout=\"12000\"></connectionPool>\n      </add>\n    </buckets>\n  </couchbase>\n</couchbaseClients>\n\n\nIn this case if you try to open the bucket my-bucket-name, its configuration\nwill be picked up from the config file.\n\nHowever, there can be situations in which the name of the buckets you want to\nuse are only known dynamically during runtime, so you cannot preconfigure any\nbuckets. In that case you couchbaseClients  section will look like this:\n\n<couchbaseClients>\n  <couchbase useSsl=\"false\" operationLifeSpan=\"1000\">\n    <servers>\n      <add uri=\"http://192.168.56.101:8091/pools\"></add>\n    </servers>\n  </couchbase>\n</couchbaseClients>\n\n\nThe weird thing is that now if you try to open a bucket by using \ncluster.OpenBucket(bucketName), the SDK does not pick up the server\nconfiguration from the <servers>  section, but at first it tries to connect to \nlocalhost, and it only uses the proper server after that fails.\nThis can have two consequences:\n\n * If we have no Couchbase server running at localhost:8091, opening the bucket\n   can take seconds, because the client is trying to connect to localhost at\n   first, and only after that fails does it connect to the configured server.\n   This problem and a workaround is described in this SO question\n   [http://stackoverflow.com/questions/34183342/why-can-opening-a-couchbase-bucket-be-very-slow]\n   .\n * If we actually do have a server at localhost:8091, then that is getting used\n   instead of the one configured in <servers>. I consider this clearly a bug.\n\nThe workaround is simple: you just have to add a dummy bucket to the\nconfiguration:\n\n<couchbaseClients>\n  <couchbase useSsl=\"false\" operationLifeSpan=\"1000\">\n    <servers>\n      <add uri=\"http://192.168.56.101:8091/pools\"></add>\n    </servers>\n    <buckets>\n      <add name=\"dummy-bucket\" useSsl=\"false\" operationLifespan=\"1000\">\n      </add>\n    </buckets>\n  </couchbase>\n</couchbaseClients>\n\n\nA bucket with the specified name doesn't even have to exist on the server.\nSolely the existence of that section will make the SDK pick up the server\nconfiguration from the <servers>  section.\n\nYou can find some more details in this SO question\n[http://stackoverflow.com/questions/34183342/why-can-opening-a-couchbase-bucket-be-very-slow]\n, and my proposed fix in the following pull request\n[https://github.com/couchbase/couchbase-net-client/pull/52].\n\nTCP connection count\nIf you are accessing Couchbase at a high volume, possibly on multiple threads,\nthe client can easily run out of available TCP connections. The maximum number\nof TCP connections can be configured in the app.config. Its default value is 2,\nwhich is really low, and it can cause the operation to fail. In the logs and in\nthe result object you can see an exception similar to this.\n\nThe operation has timed out.\nCouchbase.IO.ConnectionUnavailableException: Failed to acquire a pooled client connection on 192.168.56.101:11210 after 5 tries.\n   at Couchbase.IO.ConnectionPool`1.Acquire()\n   at Couchbase.IO.ConnectionPool`1.Acquire()\n   at Couchbase.IO.ConnectionPool`1.Acquire()\n   at Couchbase.IO.ConnectionPool`1.Couchbase.IO.IConnectionPool.Acquire()\n   at Couchbase.IO.Strategies.DefaultIOStrategy.Execute[T](IOperation`1 operation)\n   at Couchbase.Core.Server.Send[T](IOperation`1 operation)\n\n\nThe solution is to increase the number of maximum TCP connections, which can be\nseparately controlled on a default level, or for a specific bucket:\n\n<couchbaseClients>\n    <couchbase>\n        <servers>\n            <add uri=\"http://192.168.56.101:8091/pools\"/>\n        </servers>\n        <connectionPool name=\"default\" maxSize=\"35\" minSize=\"5\" sendTimeout=\"15000\">\n        </connectionPool>\n        <buckets>\n            <add name=\"default\">\n                <connectionPool maxSize=\"30\" minSize=\"5\" name=\"default\"/>\n            </add>\n        </buckets>\n    </couchbase>\n</couchbaseClients>\n\n\nFor us a pool size of 30 seems to have solved the problem, you should fine tune\nthe configuration until you find a proper value for your scenario.\n\nI found this solution with an official answer in the Couchbase Forums\n[https://forums.couchbase.com/t/failed-to-acquire-a-pooled-client-connection-after-5-tries/6062/6?u=markvincze]\n\nThe importance of ClusterHelper\nWhen we use the OpenBucket  of the ICluster  interface, a new TCP connection\nwill be made to the Couchbase server. That bucket object should be used in a \nusing  block, and when the object is disposed, the connection will be closed.\nThe performance of this approach is not ideal if we are accessing Couchbase at a\nhigh volume.\n\nLuckily the SDK contains support for pooling TCP connection in the form of the \nClusterHelper  class. A couple of things to keep in mind:\n\n * During the startup of your application (for example in the Global.asax.cs  in\n   case of a web application) you need to initialize the helper by calling \n   ClusterHelper.Initialize(...).\n * When your application shuts down, you should call ClusterHelper.Close().\n * If you open a bucket with ClusterHelper.GetBucket(...), you should not  put\n   it in a using block. The ClusterHelper will internally manage and dispose the\n   bucket objects.\n\nYou can find more details about this in the official documentation\n[http://developer.couchbase.com/documentation/server/4.0/sdks/dotnet-2.2/cluster-helper.html]\n.\n\nCouchbase is a very exciting technology, on which big companies are betting\n(Ebay, LinkedIn, Ryanair, etc.). It's great to be able to replace an outdated\nlegacy caching system with something more modern, and far better scalable.\nHowever, the above problems can be quite annoying, and they can make the\nintroduction of Couchbase in your architecture more challenging.\nI hope this post will help you avoid making the same mistakes, or at least spend\nless time looking for the solution.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": "Tips for troubleshooting Couchbase issues",
        "meta_description": "This blog post describes some quirks and issues with Couchbase Server which can make getting started with it more difficult and troublesome.",
        "author_id": "1",
        "created_at": "2016-01-10 15:45:07",
        "created_by": "1",
        "updated_at": "2016-01-10 15:53:41",
        "updated_by": "1",
        "published_at": "2016-01-10 15:45:00",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2965",
        "uuid": "be4ad3e4-608e-4376-898f-0042813fec22",
        "title": "ASP.NET Core 1.0: hints to get started",
        "slug": "getting-started-with-asp-net-core-1-0-tips-and-tricks",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I recently started working on implementing a Web Api application using ASP.NET Core 1.0, running it on Linux with the CoreCLR.\\r\\n\\r\\nThere have been many changes introduced in this new version of ASP.NET, and there are also differences in how we are running and deploying applications using CoreCLR, so I'm going to document a couple of things you might encounter if you get started with using this new ecosystem.\\r\\n\\r\\n## Version numbers\\r\\n\\r\\nIt is quite easy to get lost in the sea of different products and version numbers. In the following table I'll try to summarize the ones related to ASP.NET in the existing and the upcoming release.\\r\\n\\r\\n<table>\\r\\n    <tr>\\r\\n        <th>\\r\\n            Component\\r\\n        </th>\\r\\n        <th>\\r\\n            Current release\\r\\n        </th>\\r\\n        <th>\\r\\n            Upcoming version\\r\\n        </th>\\r\\n    </tr>\\r\\n    <tr>\\r\\n        <td>\\r\\n            ASP.NET\\r\\n        </td>\\r\\n        <td>\\r\\n            4.6\\r\\n        </td>\\r\\n        <td>\\r\\n            ASP.NET Core 1.0<br/ >(Initially was called ASP.NET 5)\\r\\n        </td>\\r\\n    </tr>\\r\\n    <tr>\\r\\n        <td>\\r\\n            MVC\\r\\n        </td>\\r\\n        <td>\\r\\n            5\\r\\n        </td>\\r\\n        <td>\\r\\n            6\\r\\n        </td>\\r\\n    </tr>\\r\\n    <tr>\\r\\n        <td>\\r\\n            Web Api\\r\\n        </td>\\r\\n        <td>\\r\\n            2\\r\\n        </td>\\r\\n        <td>\\r\\n            No new version, merged into MVC 6\\r\\n        </td>\\r\\n    </tr>\\r\\n    <tr>\\r\\n        <td>\\r\\n            Entity Framework (EF)\\r\\n        </td>\\r\\n        <td>\\r\\n            6\\r\\n        </td>\\r\\n        <td>\\r\\n            EF Core 1.0<br />(Initially called EF 7)\\r\\n        </td>\\r\\n    </tr>\\r\\n</table>\\r\\n\\r\\n\\r\\nOne thing to keep in mind is that Core 1.0 of ASP.NET and EF shouldn't necessarily be considered as the the latest updates to which everybody should update. The reasoning behind the 1.0 version number is that these are new, standalone technologies which will live alongside the existing ASP.NET 4.6 and EF 6 releases.\\r\\n\\r\\nThe reason for this is that they have been reimplemented in a cross-platform way using .NET Core 1.0, so they are less mature, and not as full-featured as their existing counterparts. Thus, if somebody does not need to target Linux, they might be better off sticking to the older versions, since for the time being those are more reliable, and have more features and wider support.\\r\\n\\r\\n## Api changes\\r\\n\\r\\nASP.NET Core 1.0 is a rewrite of the ASP.NET platform, so many of the Apis changed. If you find out that one of your favorite framework classes or methods has disappeared, don't worry. Usually, the same functionality still exists, it just has been moved or renamed.\\r\\n\\r\\nIf you can't find something, simply googling for the method or class name can often help. And because the development is being done in the open, you can also search in the source code on [Github](https://github.com/aspnet) to find a particular feature.\\r\\n\\r\\nIf you want to figure out the NuGet package in which a certain framework class resides, you can use the great [Reverse package search](http://packagesearch.azurewebsites.net/) tool.\\r\\n\\r\\n### MVC vs Web Api\\r\\n\\r\\nOne of the reasons for the Api changes is that the distinction between Web Api and MVC will cease to exist, and from now on they are unified in a single framework called **MVC**, of which version 6 will be the current release.\\r\\n\\r\\nI'm really happy about this change, there will be no more distinction between the types we need to use to implement MVC and Web Api controllers.\\r\\n\\r\\nIn the previous version we had to use different base classes for the different controllers, `Controller` was the base class for MVC, and `ApiController` was the one for Web Api. Similarly, we needed to use two different classes for other aspects too, for example the base class of an action filter had to be `System.Web.MVC.ActionFilterAttribute` for MVC, and `System.Web.Http.Filters.ActionFilterAttribute` for Web Api. Now all these classes are unified.\\r\\n\\r\\nThis makes it much easier and less confusing to mix and match the two, and have endpoints in a single application (or even in the same Controller) returning HTML content and Json or XML data.\\r\\n\\r\\n## Introducing project.json\\r\\n\\r\\nOne of the new concepts introduced in ASP.NET Core 1.0 is the new project file format, project.json, which is a more human-friendly project file with Json format we can use instead of the csproj file.\\r\\n\\r\\nThe main reason to introduce this is to make development more friendly on Linux and Mac, where we don't have the rich support of Visual Studio, but only text editors, with different level of support for .NET development, but still very far from what VS provides on Windows.\\r\\n\\r\\nStill, this is a very welcomed change on Windows too, it makes editing our project details easier and quicker. You can read about its format in more detail on the [ASP.NET Documentation](http://docs.asp.net/en/latest/conceptual-overview/understanding-aspnet5-apps.html#the-project-json-file) site.\\r\\n\\r\\n### NuGet packages in the project.json\\r\\n\\r\\nAnother change related to the new project file format is that in projects using the new project.json we don't need to have a separate packages.config file to specify the necessary NuGet packages any more. Rather, they are pulled in from the project.json. In your project description, you can specify the dependencies of your project in the `dependencies` collection:\\r\\n\\r\\n    \\\"dependencies\\\": {\\r\\n        \\\"EntityFramework.Commands\\\": \\\"7.0.0-rc1-final\\\",\\r\\n        \\\"EntityFramework.MicrosoftSqlServer\\\": \\\"7.0.0-rc1-final\\\",\\r\\n        \\\"MyOtherProjectInTheSolution\\\": \\\"\\\"\\r\\n    }\\r\\n\\r\\nAn entry in the `dependencies` can be either a reference to another project in the solution, or a reference to a NuGet package, so finally the references to packages don't live in two different places.\\r\\n\\r\\nPreviously I had many problems (and I'm sure lots of other people did) with NuGet package references living in two different places: the package to download was specified in packages.config, and the actual binary be references was specified in the csproj file. And they could really easily get out of sync, especially in large solution, where we ended up referencing different versions of the same package.\\r\\n\\r\\nThe fact that all the indirectly needed NuGet packages (packages referenced by some of the packages we need) were all separate entries in our list of references just made this more complicated and unstable.\\r\\n\\r\\nWith the new project.json these only live in a single place, so the version numbers will always be in sync, and the other advantage is that we only need to specify the top-level package we need, and all the indirect dependencies will be automatically pulled in.\\r\\n\\r\\nAnd when we start typing in a reference in the project file, we get autocomplete support for both the package names and versions. I know that the NuGet Package Manager window was also redesigned, but since I started working on this new project, I didn't have to open it even once. \\r\\n\\r\\n## Unit testing\\r\\n\\r\\nThe support for unit testing is changing too. The library currently supporting .NET Core is XUnit, but NUnit support is also in the works. I don't know what the plan with MSTest is, but given that it's a Visual Studio specific testing library, I don't think it'll be supported for DNX-based projects.\\r\\n\\r\\nIn the [ASP.NET Documentation](http://docs.asp.net/en/latest/testing/unit-testing.html) you can find a guide about how to create a test project, write some unit tests, and run them either in Visual Studio, or from the command line. (As far as I know, ReSharper is not able to run the unit tests in a DNX project, but the support for that is also in the works.)\\r\\n\\r\\n### Mocking\\r\\n\\r\\nThe other frameworks you are used to might not be available yet for .NET Core.  \\r\\nThe thing I desperately need and probably wouldn't be able to comfortable write unit tests without is a mocking framework.\\r\\n\\r\\nAs far as I know, the major mocking frameworks out there haven't been fully ported to .NET Core yet. The one I could find is called [LightMock.vNext](https://www.nuget.org/packages/LightMock.vNext/), but it seems to have much fewer features than what I'm used to in Moq.\\r\\n\\r\\nIn other .NET projects I use [Moq](https://github.com/moq/moq4) as a mocking frameworks, so I was happy to find out there is a already a working version of Moq developed by the ASP.NET team. I have been using version `4.4.0-beta8` without any problems so far.\\r\\n\\r\\n    \\\"dependencies\\\": {\\r\\n      \\\"moq.netcore\\\": \\\"4.4.0-beta8\\\",\\r\\n      ...\\r\\n    }\\r\\n\\r\\nTo be able to have this reference, there is one extra thing we have to do. We have to create a NuGet.config file in our solution folder, and add another NuGet feed, which publishes development packages coming from the ASP.NET team:\\r\\n\\r\\n    <?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?>\\r\\n    <configuration>\\r\\n      <packageSources>\\r\\n        <add key=\\\"AspNetVNext\\\" value=\\\"https://www.myget.org/F/aspnetcidev/api/v3/index.json\\\" />\\r\\n        <add key=\\\"NuGet\\\" value=\\\"https://api.nuget.org/v3/index.json\\\" />\\r\\n      </packageSources>\\r\\n    </configuration>\\r\\n\\r\\n(Source: I found this information on [Stack Overflow](http://stackoverflow.com/questions/27918305/mocking-framework-for-asp-net-core-5-0), thanks Lukasz Pyrzyk!)\\r\\n\\r\\n#### Api changes\\r\\n\\r\\nThe Api changes can greatly affect how we are mocking certain aspects of an MVC controller. Some properties which used to have setters don't have one any more, so instead of simply setting them to a mock object, we have to go through some extra hoops to stub them.\\r\\n\\r\\nFor example we can replace the Response object with a mock using the following code:\\r\\n\\r\\n    var sut = new MyController();\\r\\n\\r\\n    var responseMock = new Mock<HttpResponse>();\\r\\n\\r\\n    var httpContextMock = new Mock<HttpContext>();\\r\\n    httpContextMock.SetupGet(a => a.Response).Returns(responseMock.Object);\\r\\n\\r\\n    sut.ActionContext = new ActionContext()\\r\\n    {\\r\\n        HttpContext = httpContextMock.Object\\r\\n    };\\r\\n\\r\\n## Command line\\r\\n\\r\\nBuilding projects, running applications and executing unit tests are all possible with the new command-line tools. Again, this is essential to support development on platforms where Visual Studio is not available.\\r\\n\\r\\nBut even on Windows, where VS is available, the command line tools can prove to be very convenient.  \\r\\nThe thing I like the most about the command line tools is [dnx-watch](https://github.com/aspnet/dnx-watch). You can install it by issuing the following command:\\r\\n\\r\\n    dnu commands install Microsoft.Dnx.Watcher\\r\\n\\r\\nThen you can execute any dnx command specified for your project by typing\\r\\n\\r\\n    dnx-watch web\\r\\n\\r\\nor \\r\\n\\r\\n    dnx-watch test\\r\\n\\r\\nand dnx-watch will restart and execute the command every time any of the files in your solution changes. This is a great way to keep a fresh version of your site running all the time, or to automatically execute all your unit tests on every code change.\\r\\n\\r\\nThe command line I use on Windows is [cmder](http://cmder.net/). One of my favorite features is being able to split the terminal window vertically or horizontally.    \\r\\nThe setup I started using after a while is to split terminal into 3 parts:\\r\\n\\r\\n - Left: use `git` and the occasional `dnu` commands like `restore` and `build`.\\r\\n - Top-right: constantly run `dnx-watch web` to always host the latest version of the site.\\r\\n - Top-left: constantly run `dnx-watch test` to execute my unit test suite on every change.\\r\\n\\r\\n![Screeshot illustrating cmder used with an ASP.NET Core project](/content/images/2016/02/asp-net-core-cmder.png)\\r\\n\\r\\nIf you are using the built-in command line in Windows, I can really recommend [cmder](http://cmder.net/), it's a great improvement with features like syntax-highlighting, better text-selection with mouse, CTRL+C and CTRL+V support, and many more.\\r\\n\\r\\n# Conclusion\\r\\n\\r\\nNo real conclusion here, this post ended up being quite random :). I've probably written about some stuff which is not essential for everybody, and I'm sure there are some others which might be much more important for someone else. However, these are the ones which were at the forefront of my thoughts.\\r\\n\\r\\nI might revisit this topic in a later post as I learn more and more about ASP.NET Core.\"}]],\"sections\":[[10,0]]}",
        "html": "<p>I recently started working on implementing a Web Api application using ASP.NET Core 1.0, running it on Linux with the CoreCLR.</p>\n<p>There have been many changes introduced in this new version of ASP.NET, and there are also differences in how we are running and deploying applications using CoreCLR, so I'm going to document a couple of things you might encounter if you get started with using this new ecosystem.</p>\n<h2 id=\"versionnumbers\">Version numbers</h2>\n<p>It is quite easy to get lost in the sea of different products and version numbers. In the following table I'll try to summarize the ones related to ASP.NET in the existing and the upcoming release.</p>\n<table>\n    <tr>\n        <th>\n            Component\n        </th>\n        <th>\n            Current release\n        </th>\n        <th>\n            Upcoming version\n        </th>\n    </tr>\n    <tr>\n        <td>\n            ASP.NET\n        </td>\n        <td>\n            4.6\n        </td>\n        <td>\n            ASP.NET Core 1.0<br/ >(Initially was called ASP.NET 5)\n        </td>\n    </tr>\n    <tr>\n        <td>\n            MVC\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            6\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Web Api\n        </td>\n        <td>\n            2\n        </td>\n        <td>\n            No new version, merged into MVC 6\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Entity Framework (EF)\n        </td>\n        <td>\n            6\n        </td>\n        <td>\n            EF Core 1.0<br />(Initially called EF 7)\n        </td>\n    </tr>\n</table>\n<p>One thing to keep in mind is that Core 1.0 of ASP.NET and EF shouldn't necessarily be considered as the the latest updates to which everybody should update. The reasoning behind the 1.0 version number is that these are new, standalone technologies which will live alongside the existing ASP.NET 4.6 and EF 6 releases.</p>\n<p>The reason for this is that they have been reimplemented in a cross-platform way using .NET Core 1.0, so they are less mature, and not as full-featured as their existing counterparts. Thus, if somebody does not need to target Linux, they might be better off sticking to the older versions, since for the time being those are more reliable, and have more features and wider support.</p>\n<h2 id=\"apichanges\">Api changes</h2>\n<p>ASP.NET Core 1.0 is a rewrite of the ASP.NET platform, so many of the Apis changed. If you find out that one of your favorite framework classes or methods has disappeared, don't worry. Usually, the same functionality still exists, it just has been moved or renamed.</p>\n<p>If you can't find something, simply googling for the method or class name can often help. And because the development is being done in the open, you can also search in the source code on <a href=\"https://github.com/aspnet\">Github</a> to find a particular feature.</p>\n<p>If you want to figure out the NuGet package in which a certain framework class resides, you can use the great <a href=\"http://packagesearch.azurewebsites.net/\">Reverse package search</a> tool.</p>\n<h3 id=\"mvcvswebapi\">MVC vs Web Api</h3>\n<p>One of the reasons for the Api changes is that the distinction between Web Api and MVC will cease to exist, and from now on they are unified in a single framework called <strong>MVC</strong>, of which version 6 will be the current release.</p>\n<p>I'm really happy about this change, there will be no more distinction between the types we need to use to implement MVC and Web Api controllers.</p>\n<p>In the previous version we had to use different base classes for the different controllers, <code>Controller</code> was the base class for MVC, and <code>ApiController</code> was the one for Web Api. Similarly, we needed to use two different classes for other aspects too, for example the base class of an action filter had to be <code>System.Web.MVC.ActionFilterAttribute</code> for MVC, and <code>System.Web.Http.Filters.ActionFilterAttribute</code> for Web Api. Now all these classes are unified.</p>\n<p>This makes it much easier and less confusing to mix and match the two, and have endpoints in a single application (or even in the same Controller) returning HTML content and Json or XML data.</p>\n<h2 id=\"introducingprojectjson\">Introducing project.json</h2>\n<p>One of the new concepts introduced in ASP.NET Core 1.0 is the new project file format, project.json, which is a more human-friendly project file with Json format we can use instead of the csproj file.</p>\n<p>The main reason to introduce this is to make development more friendly on Linux and Mac, where we don't have the rich support of Visual Studio, but only text editors, with different level of support for .NET development, but still very far from what VS provides on Windows.</p>\n<p>Still, this is a very welcomed change on Windows too, it makes editing our project details easier and quicker. You can read about its format in more detail on the <a href=\"http://docs.asp.net/en/latest/conceptual-overview/understanding-aspnet5-apps.html#the-project-json-file\">ASP.NET Documentation</a> site.</p>\n<h3 id=\"nugetpackagesintheprojectjson\">NuGet packages in the project.json</h3>\n<p>Another change related to the new project file format is that in projects using the new project.json we don't need to have a separate packages.config file to specify the necessary NuGet packages any more. Rather, they are pulled in from the project.json. In your project description, you can specify the dependencies of your project in the <code>dependencies</code> collection:</p>\n<pre><code>&quot;dependencies&quot;: {\n    &quot;EntityFramework.Commands&quot;: &quot;7.0.0-rc1-final&quot;,\n    &quot;EntityFramework.MicrosoftSqlServer&quot;: &quot;7.0.0-rc1-final&quot;,\n    &quot;MyOtherProjectInTheSolution&quot;: &quot;&quot;\n}\n</code></pre>\n<p>An entry in the <code>dependencies</code> can be either a reference to another project in the solution, or a reference to a NuGet package, so finally the references to packages don't live in two different places.</p>\n<p>Previously I had many problems (and I'm sure lots of other people did) with NuGet package references living in two different places: the package to download was specified in packages.config, and the actual binary be references was specified in the csproj file. And they could really easily get out of sync, especially in large solution, where we ended up referencing different versions of the same package.</p>\n<p>The fact that all the indirectly needed NuGet packages (packages referenced by some of the packages we need) were all separate entries in our list of references just made this more complicated and unstable.</p>\n<p>With the new project.json these only live in a single place, so the version numbers will always be in sync, and the other advantage is that we only need to specify the top-level package we need, and all the indirect dependencies will be automatically pulled in.</p>\n<p>And when we start typing in a reference in the project file, we get autocomplete support for both the package names and versions. I know that the NuGet Package Manager window was also redesigned, but since I started working on this new project, I didn't have to open it even once.</p>\n<h2 id=\"unittesting\">Unit testing</h2>\n<p>The support for unit testing is changing too. The library currently supporting .NET Core is XUnit, but NUnit support is also in the works. I don't know what the plan with MSTest is, but given that it's a Visual Studio specific testing library, I don't think it'll be supported for DNX-based projects.</p>\n<p>In the <a href=\"http://docs.asp.net/en/latest/testing/unit-testing.html\">ASP.NET Documentation</a> you can find a guide about how to create a test project, write some unit tests, and run them either in Visual Studio, or from the command line. (As far as I know, ReSharper is not able to run the unit tests in a DNX project, but the support for that is also in the works.)</p>\n<h3 id=\"mocking\">Mocking</h3>\n<p>The other frameworks you are used to might not be available yet for .NET Core.<br>\nThe thing I desperately need and probably wouldn't be able to comfortable write unit tests without is a mocking framework.</p>\n<p>As far as I know, the major mocking frameworks out there haven't been fully ported to .NET Core yet. The one I could find is called <a href=\"https://www.nuget.org/packages/LightMock.vNext/\">LightMock.vNext</a>, but it seems to have much fewer features than what I'm used to in Moq.</p>\n<p>In other .NET projects I use <a href=\"https://github.com/moq/moq4\">Moq</a> as a mocking frameworks, so I was happy to find out there is a already a working version of Moq developed by the ASP.NET team. I have been using version <code>4.4.0-beta8</code> without any problems so far.</p>\n<pre><code>&quot;dependencies&quot;: {\n  &quot;moq.netcore&quot;: &quot;4.4.0-beta8&quot;,\n  ...\n}\n</code></pre>\n<p>To be able to have this reference, there is one extra thing we have to do. We have to create a NuGet.config file in our solution folder, and add another NuGet feed, which publishes development packages coming from the ASP.NET team:</p>\n<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;\n&lt;configuration&gt;\n  &lt;packageSources&gt;\n    &lt;add key=&quot;AspNetVNext&quot; value=&quot;https://www.myget.org/F/aspnetcidev/api/v3/index.json&quot; /&gt;\n    &lt;add key=&quot;NuGet&quot; value=&quot;https://api.nuget.org/v3/index.json&quot; /&gt;\n  &lt;/packageSources&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>(Source: I found this information on <a href=\"http://stackoverflow.com/questions/27918305/mocking-framework-for-asp-net-core-5-0\">Stack Overflow</a>, thanks Lukasz Pyrzyk!)</p>\n<h4 id=\"apichanges\">Api changes</h4>\n<p>The Api changes can greatly affect how we are mocking certain aspects of an MVC controller. Some properties which used to have setters don't have one any more, so instead of simply setting them to a mock object, we have to go through some extra hoops to stub them.</p>\n<p>For example we can replace the Response object with a mock using the following code:</p>\n<pre><code>var sut = new MyController();\n\nvar responseMock = new Mock&lt;HttpResponse&gt;();\n\nvar httpContextMock = new Mock&lt;HttpContext&gt;();\nhttpContextMock.SetupGet(a =&gt; a.Response).Returns(responseMock.Object);\n\nsut.ActionContext = new ActionContext()\n{\n    HttpContext = httpContextMock.Object\n};\n</code></pre>\n<h2 id=\"commandline\">Command line</h2>\n<p>Building projects, running applications and executing unit tests are all possible with the new command-line tools. Again, this is essential to support development on platforms where Visual Studio is not available.</p>\n<p>But even on Windows, where VS is available, the command line tools can prove to be very convenient.<br>\nThe thing I like the most about the command line tools is <a href=\"https://github.com/aspnet/dnx-watch\">dnx-watch</a>. You can install it by issuing the following command:</p>\n<pre><code>dnu commands install Microsoft.Dnx.Watcher\n</code></pre>\n<p>Then you can execute any dnx command specified for your project by typing</p>\n<pre><code>dnx-watch web\n</code></pre>\n<p>or</p>\n<pre><code>dnx-watch test\n</code></pre>\n<p>and dnx-watch will restart and execute the command every time any of the files in your solution changes. This is a great way to keep a fresh version of your site running all the time, or to automatically execute all your unit tests on every code change.</p>\n<p>The command line I use on Windows is <a href=\"http://cmder.net/\">cmder</a>. One of my favorite features is being able to split the terminal window vertically or horizontally.<br>\nThe setup I started using after a while is to split terminal into 3 parts:</p>\n<ul>\n<li>Left: use <code>git</code> and the occasional <code>dnu</code> commands like <code>restore</code> and <code>build</code>.</li>\n<li>Top-right: constantly run <code>dnx-watch web</code> to always host the latest version of the site.</li>\n<li>Top-left: constantly run <code>dnx-watch test</code> to execute my unit test suite on every change.</li>\n</ul>\n<p><img src=\"/content/images/2016/02/asp-net-core-cmder.png\" alt=\"Screeshot illustrating cmder used with an ASP.NET Core project\"></p>\n<p>If you are using the built-in command line in Windows, I can really recommend <a href=\"http://cmder.net/\">cmder</a>, it's a great improvement with features like syntax-highlighting, better text-selection with mouse, CTRL+C and CTRL+V support, and many more.</p>\n<h1 id=\"conclusion\">Conclusion</h1>\n<p>No real conclusion here, this post ended up being quite random :). I've probably written about some stuff which is not essential for everybody, and I'm sure there are some others which might be much more important for someone else. However, these are the ones which were at the forefront of my thoughts.</p>\n<p>I might revisit this topic in a later post as I learn more and more about ASP.NET Core.</p>\n",
        "comment_id": "12",
        "plaintext": "I recently started working on implementing a Web Api application using ASP.NET\nCore 1.0, running it on Linux with the CoreCLR.\n\nThere have been many changes introduced in this new version of ASP.NET, and\nthere are also differences in how we are running and deploying applications\nusing CoreCLR, so I'm going to document a couple of things you might encounter\nif you get started with using this new ecosystem.\n\nVersion numbers\nIt is quite easy to get lost in the sea of different products and version\nnumbers. In the following table I'll try to summarize the ones related to\nASP.NET in the existing and the upcoming release.\n\nComponent\n Current release\n Upcoming version\n ASP.NET\n 4.6\n ASP.NET Core 1.0\n(Initially was called ASP.NET 5)\n MVC\n 5\n 6\n Web Api\n 2\n No new version, merged into MVC 6\n Entity Framework (EF)\n 6\n EF Core 1.0\n(Initially called EF 7)\n One thing to keep in mind is that Core 1.0 of ASP.NET and EF shouldn't\nnecessarily be considered as the the latest updates to which everybody should\nupdate. The reasoning behind the 1.0 version number is that these are new,\nstandalone technologies which will live alongside the existing ASP.NET 4.6 and\nEF 6 releases.\n\nThe reason for this is that they have been reimplemented in a cross-platform way\nusing .NET Core 1.0, so they are less mature, and not as full-featured as their\nexisting counterparts. Thus, if somebody does not need to target Linux, they\nmight be better off sticking to the older versions, since for the time being\nthose are more reliable, and have more features and wider support.\n\nApi changes\nASP.NET Core 1.0 is a rewrite of the ASP.NET platform, so many of the Apis\nchanged. If you find out that one of your favorite framework classes or methods\nhas disappeared, don't worry. Usually, the same functionality still exists, it\njust has been moved or renamed.\n\nIf you can't find something, simply googling for the method or class name can\noften help. And because the development is being done in the open, you can also\nsearch in the source code on Github [https://github.com/aspnet]  to find a\nparticular feature.\n\nIf you want to figure out the NuGet package in which a certain framework class\nresides, you can use the great Reverse package search\n[http://packagesearch.azurewebsites.net/]  tool.\n\nMVC vs Web Api\nOne of the reasons for the Api changes is that the distinction between Web Api\nand MVC will cease to exist, and from now on they are unified in a single\nframework called MVC, of which version 6 will be the current release.\n\nI'm really happy about this change, there will be no more distinction between\nthe types we need to use to implement MVC and Web Api controllers.\n\nIn the previous version we had to use different base classes for the different\ncontrollers, Controller  was the base class for MVC, and ApiController  was the\none for Web Api. Similarly, we needed to use two different classes for other\naspects too, for example the base class of an action filter had to be \nSystem.Web.MVC.ActionFilterAttribute  for MVC, and \nSystem.Web.Http.Filters.ActionFilterAttribute  for Web Api. Now all these\nclasses are unified.\n\nThis makes it much easier and less confusing to mix and match the two, and have\nendpoints in a single application (or even in the same Controller) returning\nHTML content and Json or XML data.\n\nIntroducing project.json\nOne of the new concepts introduced in ASP.NET Core 1.0 is the new project file\nformat, project.json, which is a more human-friendly project file with Json\nformat we can use instead of the csproj file.\n\nThe main reason to introduce this is to make development more friendly on Linux\nand Mac, where we don't have the rich support of Visual Studio, but only text\neditors, with different level of support for .NET development, but still very\nfar from what VS provides on Windows.\n\nStill, this is a very welcomed change on Windows too, it makes editing our\nproject details easier and quicker. You can read about its format in more detail\non the ASP.NET Documentation  site.\n\nNuGet packages in the project.json\nAnother change related to the new project file format is that in projects using\nthe new project.json we don't need to have a separate packages.config file to\nspecify the necessary NuGet packages any more. Rather, they are pulled in from\nthe project.json. In your project description, you can specify the dependencies\nof your project in the dependencies  collection:\n\n\"dependencies\": {\n    \"EntityFramework.Commands\": \"7.0.0-rc1-final\",\n    \"EntityFramework.MicrosoftSqlServer\": \"7.0.0-rc1-final\",\n    \"MyOtherProjectInTheSolution\": \"\"\n}\n\n\nAn entry in the dependencies  can be either a reference to another project in\nthe solution, or a reference to a NuGet package, so finally the references to\npackages don't live in two different places.\n\nPreviously I had many problems (and I'm sure lots of other people did) with\nNuGet package references living in two different places: the package to download\nwas specified in packages.config, and the actual binary be references was\nspecified in the csproj file. And they could really easily get out of sync,\nespecially in large solution, where we ended up referencing different versions\nof the same package.\n\nThe fact that all the indirectly needed NuGet packages (packages referenced by\nsome of the packages we need) were all separate entries in our list of\nreferences just made this more complicated and unstable.\n\nWith the new project.json these only live in a single place, so the version\nnumbers will always be in sync, and the other advantage is that we only need to\nspecify the top-level package we need, and all the indirect dependencies will be\nautomatically pulled in.\n\nAnd when we start typing in a reference in the project file, we get autocomplete\nsupport for both the package names and versions. I know that the NuGet Package\nManager window was also redesigned, but since I started working on this new\nproject, I didn't have to open it even once.\n\nUnit testing\nThe support for unit testing is changing too. The library currently supporting\n.NET Core is XUnit, but NUnit support is also in the works. I don't know what\nthe plan with MSTest is, but given that it's a Visual Studio specific testing\nlibrary, I don't think it'll be supported for DNX-based projects.\n\nIn the ASP.NET Documentation\n[http://docs.asp.net/en/latest/testing/unit-testing.html]  you can find a guide\nabout how to create a test project, write some unit tests, and run them either\nin Visual Studio, or from the command line. (As far as I know, ReSharper is not\nable to run the unit tests in a DNX project, but the support for that is also in\nthe works.)\n\nMocking\nThe other frameworks you are used to might not be available yet for .NET Core.\nThe thing I desperately need and probably wouldn't be able to comfortable write\nunit tests without is a mocking framework.\n\nAs far as I know, the major mocking frameworks out there haven't been fully\nported to .NET Core yet. The one I could find is called LightMock.vNext\n[https://www.nuget.org/packages/LightMock.vNext/], but it seems to have much\nfewer features than what I'm used to in Moq.\n\nIn other .NET projects I use Moq [https://github.com/moq/moq4]  as a mocking\nframeworks, so I was happy to find out there is a already a working version of\nMoq developed by the ASP.NET team. I have been using version 4.4.0-beta8 \nwithout any problems so far.\n\n\"dependencies\": {\n  \"moq.netcore\": \"4.4.0-beta8\",\n  ...\n}\n\n\nTo be able to have this reference, there is one extra thing we have to do. We\nhave to create a NuGet.config file in our solution folder, and add another NuGet\nfeed, which publishes development packages coming from the ASP.NET team:\n\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<configuration>\n  <packageSources>\n    <add key=\"AspNetVNext\" value=\"https://www.myget.org/F/aspnetcidev/api/v3/index.json\" />\n    <add key=\"NuGet\" value=\"https://api.nuget.org/v3/index.json\" />\n  </packageSources>\n</configuration>\n\n\n(Source: I found this information on Stack Overflow\n[http://stackoverflow.com/questions/27918305/mocking-framework-for-asp-net-core-5-0]\n, thanks Lukasz Pyrzyk!)\n\nApi changes\nThe Api changes can greatly affect how we are mocking certain aspects of an MVC\ncontroller. Some properties which used to have setters don't have one any more,\nso instead of simply setting them to a mock object, we have to go through some\nextra hoops to stub them.\n\nFor example we can replace the Response object with a mock using the following\ncode:\n\nvar sut = new MyController();\n\nvar responseMock = new Mock<HttpResponse>();\n\nvar httpContextMock = new Mock<HttpContext>();\nhttpContextMock.SetupGet(a => a.Response).Returns(responseMock.Object);\n\nsut.ActionContext = new ActionContext()\n{\n    HttpContext = httpContextMock.Object\n};\n\n\nCommand line\nBuilding projects, running applications and executing unit tests are all\npossible with the new command-line tools. Again, this is essential to support\ndevelopment on platforms where Visual Studio is not available.\n\nBut even on Windows, where VS is available, the command line tools can prove to\nbe very convenient.\nThe thing I like the most about the command line tools is dnx-watch\n[https://github.com/aspnet/dnx-watch]. You can install it by issuing the\nfollowing command:\n\ndnu commands install Microsoft.Dnx.Watcher\n\n\nThen you can execute any dnx command specified for your project by typing\n\ndnx-watch web\n\n\nor\n\ndnx-watch test\n\n\nand dnx-watch will restart and execute the command every time any of the files\nin your solution changes. This is a great way to keep a fresh version of your\nsite running all the time, or to automatically execute all your unit tests on\nevery code change.\n\nThe command line I use on Windows is cmder [http://cmder.net/]. One of my\nfavorite features is being able to split the terminal window vertically or\nhorizontally.\nThe setup I started using after a while is to split terminal into 3 parts:\n\n * Left: use git  and the occasional dnu  commands like restore  and build.\n * Top-right: constantly run dnx-watch web  to always host the latest version of\n   the site.\n * Top-left: constantly run dnx-watch test  to execute my unit test suite on\n   every change.\n\n\n\nIf you are using the built-in command line in Windows, I can really recommend \ncmder [http://cmder.net/], it's a great improvement with features like\nsyntax-highlighting, better text-selection with mouse, CTRL+C and CTRL+V\nsupport, and many more.\n\nConclusion\nNo real conclusion here, this post ended up being quite random :). I've probably\nwritten about some stuff which is not essential for everybody, and I'm sure\nthere are some others which might be much more important for someone else.\nHowever, these are the ones which were at the forefront of my thoughts.\n\nI might revisit this topic in a later post as I learn more and more about\nASP.NET Core.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": "ASP.NET Core 1.0: hints to get started",
        "meta_description": "Some random tips and tricks I have learnt during spending a couple of weeks with getting started with ASP.NET Core.",
        "author_id": "1",
        "created_at": "2016-02-14 14:04:00",
        "created_by": "1",
        "updated_at": "2016-03-02 16:28:10",
        "updated_by": "1",
        "published_at": "2016-02-14 16:30:58",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2966",
        "uuid": "874bef84-3451-40da-afb8-d047d577ce22",
        "title": "Simple client-side compression for Couchbase - with benchmarks",
        "slug": "simple-client-side-compression-for-couchbase-with-benchmarks",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"# Introduction\\n\\nIn the [last post](http://blog.markvincze.com/couchbase-server-tips-for-troubleshooting-issues/) I described the quirks and problems I encountered during getting started with using Couchbase Server in a production environment.\\n\\nWhen we are storing complex objects, by default the .NET Couchbase SDK uses Json.NET serialization. It is possible to create indices and views on this Json structure, and execute custom queries to filter our documents based on their Json property values.\\nI guess in this scenario we can think of Couchbase as a NoSQL Document-store.\\n\\nOn the other hand, there is another scenario for using Couchbase, and that is to utilize it as a simple key-value store. In this scenario we don't care about the internal structure of the documents, because we don't want to use any custom indices or queries, we only want to query for single elements based on their keys.\\n  \\nWhen we use a database or a document store, we would like to achieve the following things (amongst others).\\n\\n - Use as little disk space as possible.\\n - Use as little memory as possible.\\n - Store and return objects as fast as possible.\\n\\nWith my colleagues we figured that if we let the SDK use its default Json.NET serialization, we are definitely not operating optimally regarding the first two goals. Json serialization is much less verbose than - let's say - XML, but still, a Json document can usually be heavily compressed compared to its raw Json representation. And because we only use Couchbase as a key-value store, having the structure of a documents as raw Json is completely wasted.\\n\\nAn obvious approach to improve this situation would be to store our documents compressed in Couchbase. Before starting to implement something, I wanted to ask around to see if there is a built-in solution to achieve this, so I posted a question in the official [Couchbase developer forums](https://forums.couchbase.com/t/how-to-properly-compress-documents/6418/).\\n\\nI received the suggestion that the simplest way to do this would be to simply compress my documents into a binary blob before inserting them into the Couchbase server.\\n\\n>> The easiest way to do this would probably to store the content as a compressed byte array. A byte array will by-pass the serialization process and be stored on the server as a binary blob.  \\n\\nHowever, someone else warned me about the fact that Couchbase already uses some kind of compression when it stores documents on disk, so by manually compressing objects I would just put extra load on the client without achieving any improvement.\\n\\n>>On another note, I'm not sure compressing the documents before serializing to Couchbase is necessary. Couchbase already compresses the data store when it's persisted to disk on the server.\\n\\n>>So you could just serialize as JSON, and let the server pick up the processing load of compression/decompression.\\n\\nSo I decided to do a simple benchmark to figure out how much we can gain from using compression, and how significant its drawbacks are.\\n\\n# Implementing compression\\n\\nMy implementation of compression is based on the `JilSerializer` described by Jeff Morris in his [blog post](http://blog.couchbase.com/2015/june/using-jil-for-custom-json-serialization-in-the-couchbase-.net-sdk) about improving the performance of the built-in serializer. I extended his code with using the `GZipStream` class included in the .NET Framework. You can find the full code for this serializer in [this gist](https://gist.github.com/markvincze/c585dce7d8f76964f5ed).  \\n(I also replaced the Jil serializer with Json.NET, because I got a weirdly long startup time with the Jil serializer, for which I couldn't figure out the reason yet.)\\n\\nThis implementation seemed to work nicely, now it's time to measure whether it brings any value to the table. \\n\\n# Benchmark\\n\\nI have learned many times that it's frighteningly easy to get benchmarks wrong. Very often we get misleading results, because we overlook something: the system under test is not warmed up, the measured timing is affected by some mechanism we don't want to test, or simply the results end up being too random.  \\nIn my experience the best tool to avoid these problems is to  make our benchmark as simple as possible.\\n\\nThe simplest approach I could come up with is the following.\\n\\n1. We take a rather large C# object, ~400 Kbytes if serialized into Json.\\n2. Insert this document into Couchbase, and then retrieve the same document.\\n3. Repeat this a thousand times.\\n4. During this process, measure the total elapsed time and also the average time needed for the insert and the retrieval.\\n5. After running the benchmark, examine the memory and the disk usage of the Couchbase bucket.\\n\\nI've run this benchmark with both the default serializer and the simple GZip serializer described above. The timings are measured by the benchmark code, but I couldn't find any way to get the memory and disk usage out of the server programatically with the Couchbase SDK, so I read those from the management interface:\\n\\n![](/content/images/2016/01/management-screenshot.png)\\n\\n## Test results\\nI did the first set of tests with a semi-random data set. I generated some random Json objects using [JSON GENERATOR](http://www.json-generator.com/), and generated a C# object model with [json2csharp](http://json2csharp.com/). You can find the data and the code in this [Github repository](https://github.com/markvincze/couchbase-perf-test).\\n\\nThe results for the 1000 inserts and retrievals are the following\\n\\n![Timings for a semi-random data set.](/content/images/2016/01/timings-random-data-1.png)\\n\\n![Memory and disk usage for a semi-random data set.](/content/images/2016/01/memory-random-data-1.png)\\n\\nI did the second set of tests with a real-life dataset I extracted from our production cache. With this data I got the following results\\n\\n![Timings for a real data set.](/content/images/2016/01/timings-real-data-2.png)\\n\\n![Memory and disk usage for a real data set.](/content/images/2016/01/memory-real-data-1.png)\\n\\nThe results suggest that\\n\\n - Inserting and reading are slower if we use compression on the client side. Reading compressed data can be significantly slower if we have to decompress, the writing seems to be less affected. The penalty we have to pay for compression seems to depend a lot on our data set, the difference might be negligible.\\n - This penalty is contrasted by the gain we can see in the memory and disk usage. Both the disk and memory usage is significantly lower if we use compression. The saving in memory usage was 63% for the random data, and 85% in case of the real data. The disk usage was 31% lower when storing random data, and 62% lower in case of real data.\\n\\nWe can clearly see that the nature of our data set greatly affects the extra time needed for the compression and the amount of savings we can achieve in the memory and disk usage. Data sets containing pseudo-random data, like statistical information with lots of numbers can be compressed less effectively than datasets containing lots of text.\\n\\n## Conclusion\\n\\nIn the scenario I'm working with, using compression is clearly worth it: the extra computation time needs to be payed is negligible, however, the saving in memory consumption and disk usage is significant.\\n\\nHowever: if you want to make a decision for your scenario, I definitely advise you to do some measurements with your own dataset. To give a starting point for doing your own benchmark, I uploaded the code I used to this [Github repository](https://github.com/markvincze/couchbase-perf-test).  \\n(In the code I'm using the Couchbase server running on localhost, and the benchmark uses two buckets, `withcompression` and `withoutcompression`. Flushing needs to be enabled for the buckets, because the benchmark completely clears all the documents when it starts.)\"}]],\"sections\":[[10,0]]}",
        "html": "<h1 id=\"introduction\">Introduction</h1>\n<p>In the <a href=\"http://blog.markvincze.com/couchbase-server-tips-for-troubleshooting-issues/\">last post</a> I described the quirks and problems I encountered during getting started with using Couchbase Server in a production environment.</p>\n<p>When we are storing complex objects, by default the .NET Couchbase SDK uses Json.NET serialization. It is possible to create indices and views on this Json structure, and execute custom queries to filter our documents based on their Json property values.<br>\nI guess in this scenario we can think of Couchbase as a NoSQL Document-store.</p>\n<p>On the other hand, there is another scenario for using Couchbase, and that is to utilize it as a simple key-value store. In this scenario we don't care about the internal structure of the documents, because we don't want to use any custom indices or queries, we only want to query for single elements based on their keys.</p>\n<p>When we use a database or a document store, we would like to achieve the following things (amongst others).</p>\n<ul>\n<li>Use as little disk space as possible.</li>\n<li>Use as little memory as possible.</li>\n<li>Store and return objects as fast as possible.</li>\n</ul>\n<p>With my colleagues we figured that if we let the SDK use its default Json.NET serialization, we are definitely not operating optimally regarding the first two goals. Json serialization is much less verbose than - let's say - XML, but still, a Json document can usually be heavily compressed compared to its raw Json representation. And because we only use Couchbase as a key-value store, having the structure of a documents as raw Json is completely wasted.</p>\n<p>An obvious approach to improve this situation would be to store our documents compressed in Couchbase. Before starting to implement something, I wanted to ask around to see if there is a built-in solution to achieve this, so I posted a question in the official <a href=\"https://forums.couchbase.com/t/how-to-properly-compress-documents/6418/\">Couchbase developer forums</a>.</p>\n<p>I received the suggestion that the simplest way to do this would be to simply compress my documents into a binary blob before inserting them into the Couchbase server.</p>\n<blockquote>\n<blockquote>\n<p>The easiest way to do this would probably to store the content as a compressed byte array. A byte array will by-pass the serialization process and be stored on the server as a binary blob.</p>\n</blockquote>\n</blockquote>\n<p>However, someone else warned me about the fact that Couchbase already uses some kind of compression when it stores documents on disk, so by manually compressing objects I would just put extra load on the client without achieving any improvement.</p>\n<blockquote>\n<blockquote>\n<p>On another note, I'm not sure compressing the documents before serializing to Couchbase is necessary. Couchbase already compresses the data store when it's persisted to disk on the server.</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>So you could just serialize as JSON, and let the server pick up the processing load of compression/decompression.</p>\n</blockquote>\n</blockquote>\n<p>So I decided to do a simple benchmark to figure out how much we can gain from using compression, and how significant its drawbacks are.</p>\n<h1 id=\"implementingcompression\">Implementing compression</h1>\n<p>My implementation of compression is based on the <code>JilSerializer</code> described by Jeff Morris in his <a href=\"http://blog.couchbase.com/2015/june/using-jil-for-custom-json-serialization-in-the-couchbase-.net-sdk\">blog post</a> about improving the performance of the built-in serializer. I extended his code with using the <code>GZipStream</code> class included in the .NET Framework. You can find the full code for this serializer in <a href=\"https://gist.github.com/markvincze/c585dce7d8f76964f5ed\">this gist</a>.<br>\n(I also replaced the Jil serializer with Json.NET, because I got a weirdly long startup time with the Jil serializer, for which I couldn't figure out the reason yet.)</p>\n<p>This implementation seemed to work nicely, now it's time to measure whether it brings any value to the table.</p>\n<h1 id=\"benchmark\">Benchmark</h1>\n<p>I have learned many times that it's frighteningly easy to get benchmarks wrong. Very often we get misleading results, because we overlook something: the system under test is not warmed up, the measured timing is affected by some mechanism we don't want to test, or simply the results end up being too random.<br>\nIn my experience the best tool to avoid these problems is to  make our benchmark as simple as possible.</p>\n<p>The simplest approach I could come up with is the following.</p>\n<ol>\n<li>We take a rather large C# object, ~400 Kbytes if serialized into Json.</li>\n<li>Insert this document into Couchbase, and then retrieve the same document.</li>\n<li>Repeat this a thousand times.</li>\n<li>During this process, measure the total elapsed time and also the average time needed for the insert and the retrieval.</li>\n<li>After running the benchmark, examine the memory and the disk usage of the Couchbase bucket.</li>\n</ol>\n<p>I've run this benchmark with both the default serializer and the simple GZip serializer described above. The timings are measured by the benchmark code, but I couldn't find any way to get the memory and disk usage out of the server programatically with the Couchbase SDK, so I read those from the management interface:</p>\n<p><img src=\"/content/images/2016/01/management-screenshot.png\" alt=\"\"></p>\n<h2 id=\"testresults\">Test results</h2>\n<p>I did the first set of tests with a semi-random data set. I generated some random Json objects using <a href=\"http://www.json-generator.com/\">JSON GENERATOR</a>, and generated a C# object model with <a href=\"http://json2csharp.com/\">json2csharp</a>. You can find the data and the code in this <a href=\"https://github.com/markvincze/couchbase-perf-test\">Github repository</a>.</p>\n<p>The results for the 1000 inserts and retrievals are the following</p>\n<p><img src=\"/content/images/2016/01/timings-random-data-1.png\" alt=\"Timings for a semi-random data set.\"></p>\n<p><img src=\"/content/images/2016/01/memory-random-data-1.png\" alt=\"Memory and disk usage for a semi-random data set.\"></p>\n<p>I did the second set of tests with a real-life dataset I extracted from our production cache. With this data I got the following results</p>\n<p><img src=\"/content/images/2016/01/timings-real-data-2.png\" alt=\"Timings for a real data set.\"></p>\n<p><img src=\"/content/images/2016/01/memory-real-data-1.png\" alt=\"Memory and disk usage for a real data set.\"></p>\n<p>The results suggest that</p>\n<ul>\n<li>Inserting and reading are slower if we use compression on the client side. Reading compressed data can be significantly slower if we have to decompress, the writing seems to be less affected. The penalty we have to pay for compression seems to depend a lot on our data set, the difference might be negligible.</li>\n<li>This penalty is contrasted by the gain we can see in the memory and disk usage. Both the disk and memory usage is significantly lower if we use compression. The saving in memory usage was 63% for the random data, and 85% in case of the real data. The disk usage was 31% lower when storing random data, and 62% lower in case of real data.</li>\n</ul>\n<p>We can clearly see that the nature of our data set greatly affects the extra time needed for the compression and the amount of savings we can achieve in the memory and disk usage. Data sets containing pseudo-random data, like statistical information with lots of numbers can be compressed less effectively than datasets containing lots of text.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>In the scenario I'm working with, using compression is clearly worth it: the extra computation time needs to be payed is negligible, however, the saving in memory consumption and disk usage is significant.</p>\n<p>However: if you want to make a decision for your scenario, I definitely advise you to do some measurements with your own dataset. To give a starting point for doing your own benchmark, I uploaded the code I used to this <a href=\"https://github.com/markvincze/couchbase-perf-test\">Github repository</a>.<br>\n(In the code I'm using the Couchbase server running on localhost, and the benchmark uses two buckets, <code>withcompression</code> and <code>withoutcompression</code>. Flushing needs to be enabled for the buckets, because the benchmark completely clears all the documents when it starts.)</p>\n",
        "comment_id": "11",
        "plaintext": "Introduction\nIn the last post\n[http://blog.markvincze.com/couchbase-server-tips-for-troubleshooting-issues/] \nI described the quirks and problems I encountered during getting started with\nusing Couchbase Server in a production environment.\n\nWhen we are storing complex objects, by default the .NET Couchbase SDK uses\nJson.NET serialization. It is possible to create indices and views on this Json\nstructure, and execute custom queries to filter our documents based on their\nJson property values.\nI guess in this scenario we can think of Couchbase as a NoSQL Document-store.\n\nOn the other hand, there is another scenario for using Couchbase, and that is to\nutilize it as a simple key-value store. In this scenario we don't care about the\ninternal structure of the documents, because we don't want to use any custom\nindices or queries, we only want to query for single elements based on their\nkeys.\n\nWhen we use a database or a document store, we would like to achieve the\nfollowing things (amongst others).\n\n * Use as little disk space as possible.\n * Use as little memory as possible.\n * Store and return objects as fast as possible.\n\nWith my colleagues we figured that if we let the SDK use its default Json.NET\nserialization, we are definitely not operating optimally regarding the first two\ngoals. Json serialization is much less verbose than - let's say - XML, but\nstill, a Json document can usually be heavily compressed compared to its raw\nJson representation. And because we only use Couchbase as a key-value store,\nhaving the structure of a documents as raw Json is completely wasted.\n\nAn obvious approach to improve this situation would be to store our documents\ncompressed in Couchbase. Before starting to implement something, I wanted to ask\naround to see if there is a built-in solution to achieve this, so I posted a\nquestion in the official Couchbase developer forums\n[https://forums.couchbase.com/t/how-to-properly-compress-documents/6418/].\n\nI received the suggestion that the simplest way to do this would be to simply\ncompress my documents into a binary blob before inserting them into the\nCouchbase server.\n\nThe easiest way to do this would probably to store the content as a compressed\nbyte array. A byte array will by-pass the serialization process and be stored on\nthe server as a binary blob.\n\nHowever, someone else warned me about the fact that Couchbase already uses some\nkind of compression when it stores documents on disk, so by manually compressing\nobjects I would just put extra load on the client without achieving any\nimprovement.\n\nOn another note, I'm not sure compressing the documents before serializing to\nCouchbase is necessary. Couchbase already compresses the data store when it's\npersisted to disk on the server.\n\nSo you could just serialize as JSON, and let the server pick up the processing\nload of compression/decompression.\n\nSo I decided to do a simple benchmark to figure out how much we can gain from\nusing compression, and how significant its drawbacks are.\n\nImplementing compression\nMy implementation of compression is based on the JilSerializer  described by\nJeff Morris in his blog post\n[http://blog.couchbase.com/2015/june/using-jil-for-custom-json-serialization-in-the-couchbase-.net-sdk] \n about improving the performance of the built-in serializer. I extended his code\nwith using the GZipStream  class included in the .NET Framework. You can find\nthe full code for this serializer in this gist\n[https://gist.github.com/markvincze/c585dce7d8f76964f5ed].\n(I also replaced the Jil serializer with Json.NET, because I got a weirdly long\nstartup time with the Jil serializer, for which I couldn't figure out the reason\nyet.)\n\nThis implementation seemed to work nicely, now it's time to measure whether it\nbrings any value to the table.\n\nBenchmark\nI have learned many times that it's frighteningly easy to get benchmarks wrong.\nVery often we get misleading results, because we overlook something: the system\nunder test is not warmed up, the measured timing is affected by some mechanism\nwe don't want to test, or simply the results end up being too random.\nIn my experience the best tool to avoid these problems is to make our benchmark\nas simple as possible.\n\nThe simplest approach I could come up with is the following.\n\n 1. We take a rather large C# object, ~400 Kbytes if serialized into Json.\n 2. Insert this document into Couchbase, and then retrieve the same document.\n 3. Repeat this a thousand times.\n 4. During this process, measure the total elapsed time and also the average\n    time needed for the insert and the retrieval.\n 5. After running the benchmark, examine the memory and the disk usage of the\n    Couchbase bucket.\n\nI've run this benchmark with both the default serializer and the simple GZip\nserializer described above. The timings are measured by the benchmark code, but\nI couldn't find any way to get the memory and disk usage out of the server\nprogramatically with the Couchbase SDK, so I read those from the management\ninterface:\n\n\n\nTest results\nI did the first set of tests with a semi-random data set. I generated some\nrandom Json objects using JSON GENERATOR [http://www.json-generator.com/], and\ngenerated a C# object model with json2csharp [http://json2csharp.com/]. You can\nfind the data and the code in this Github repository\n[https://github.com/markvincze/couchbase-perf-test].\n\nThe results for the 1000 inserts and retrievals are the following\n\n\n\n\n\nI did the second set of tests with a real-life dataset I extracted from our\nproduction cache. With this data I got the following results\n\n\n\n\n\nThe results suggest that\n\n * Inserting and reading are slower if we use compression on the client side.\n   Reading compressed data can be significantly slower if we have to decompress,\n   the writing seems to be less affected. The penalty we have to pay for\n   compression seems to depend a lot on our data set, the difference might be\n   negligible.\n * This penalty is contrasted by the gain we can see in the memory and disk\n   usage. Both the disk and memory usage is significantly lower if we use\n   compression. The saving in memory usage was 63% for the random data, and 85%\n   in case of the real data. The disk usage was 31% lower when storing random\n   data, and 62% lower in case of real data.\n\nWe can clearly see that the nature of our data set greatly affects the extra\ntime needed for the compression and the amount of savings we can achieve in the\nmemory and disk usage. Data sets containing pseudo-random data, like statistical\ninformation with lots of numbers can be compressed less effectively than\ndatasets containing lots of text.\n\nConclusion\nIn the scenario I'm working with, using compression is clearly worth it: the\nextra computation time needs to be payed is negligible, however, the saving in\nmemory consumption and disk usage is significant.\n\nHowever: if you want to make a decision for your scenario, I definitely advise\nyou to do some measurements with your own dataset. To give a starting point for\ndoing your own benchmark, I uploaded the code I used to this Github repository\n[https://github.com/markvincze/couchbase-perf-test].\n(In the code I'm using the Couchbase server running on localhost, and the\nbenchmark uses two buckets, withcompression  and withoutcompression. Flushing\nneeds to be enabled for the buckets, because the benchmark completely clears all\nthe documents when it starts.)",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": "Client-side compression for Couchbase - with benchmarks",
        "meta_description": "Implementing client-side compression for Couchbase is pretty simple. In this post we take a look at what this costs us and how significant the benefits are.",
        "author_id": "1",
        "created_at": "2016-01-16 16:53:50",
        "created_by": "1",
        "updated_at": "2016-06-25 21:19:12",
        "updated_by": "1",
        "published_at": "2016-01-16 16:53:00",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2967",
        "uuid": "1622e74d-82da-46c4-8cd2-2e68ae962372",
        "title": "How to validate action parameters with DataAnnotation attributes?",
        "slug": "how-to-validate-action-parameters-with-dataannotation-attributes",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"## Model validation in MVC\\n\\nIn both MVC and Web Api we can use the attributes provided in the `System.ComponentModel.DataAnnotations` namespace to specify validation rules for our models.\\n\\nLet's say we have a controller action with the following signature, accepting a single parameter populated from the request body.\\n\\n```csharp\\npublic IActionResult Post([FromBody]Product product);\\n```\\n\\nAnd we decorated our `Product` type with the following validation attributes (example taken from the [official ASP.NET documentation](http://www.asp.net/web-api/overview/formats-and-model-binding/model-validation-in-aspnet-web-api)).\\n\\n```csharp\\npublic class Product\\n{\\n    public int Id { get; set; }\\n    [Required]\\n    public string Name { get; set; }\\n    public decimal Price { get; set; }\\n    [Range(0, 999)]\\n    public double Weight { get; set; }\\n}\\n```\\n\\nWhen we call our endpoint by posting a product object in the body of our request, the framework is going to evaluate our validation attributes during the model binding process, and save its result (with possibly errors) in the `ModelState` property of our controller.  \\nSo in the implementation of our action we can simply use the `ModelState` property to check whether the input is valid or not, and we can return its value as our response in case of an error.\\n\\n```csharp\\npublic IActionResult Post(Product product)\\n{\\n    if (!ModelState.IsValid)\\n    {\\n        return HttpBadRequest(ModelState);\\n    }\\n\\n    // Process the product...\\n\\n    return Ok();\\n}\\n```\\n\\nIf we post the following Json to our endpoint (note that the required `Name` property is missing)\\n\\n```json\\n{ \\\"Id\\\":4, \\\"Price\\\":2.99, \\\"Weight\\\":5 }\\n```\\n\\nwe get back an error response containing all the validation errors produced during the model binding.\\n\\n```json\\n{\\n  \\\"name\\\": [\\n    \\\"The Name field is required.\\\"\\n  ]\\n}\\n```\\n\\n## Validation attributes on action parameters\\n\\nThere is nothing preventing us from putting validation attributes not on a model property, but on the method parameters of an action. We might want to enforce our callers to post a product object to this endpoint, so it seems logical to add the `Required` attribute on the method parameter itself.\\n\\n```csharp\\npublic IActionResult Post([FromBody][Required]Product product)\\n```\\n\\nThe problem with this approach - which surprised me - is that it simply doesn't work. The framework does not seem to evaluate these attributes at all.\\n\\nIf we call the endpoint with an empty request body, the value of the `product` argument will be `null`, but `ModelState.IsValid` will return true.\\n\\n### Solution\\n\\nLuckily, it is not difficult to hook into the MVC pipeline with a custom filter attribute. With the following custom filter attribute we can iterate over all of the action parameters and evaluate all the validation attributes specified for them.\\n\\n```csharp\\npublic class ValidateActionParametersAttribute : ActionFilterAttribute\\n{\\n    public override void OnActionExecuting(ActionExecutingContext context)\\n    {\\n        var descriptor = context.ActionDescriptor as ControllerActionDescriptor;\\n\\n        if (descriptor != null)\\n        {\\n            var parameters = descriptor.MethodInfo.GetParameters();\\n\\n            foreach (var parameter in parameters)\\n            {\\n                var argument = context.ActionArguments[parameter.Name];\\n\\n                EvaluateValidationAttributes(parameter, argument, context.ModelState);\\n            }\\n        }\\n\\n        base.OnActionExecuting(context);\\n    }\\n\\n    private void EvaluateValidationAttributes(ParameterInfo parameter, object argument, ModelStateDictionary modelState)\\n    {\\n        var validationAttributes = parameter.CustomAttributes;\\n\\n        foreach (var attributeData in validationAttributes)\\n        {\\n            var attributeInstance = CustomAttributeExtensions.GetCustomAttribute(parameter, attributeData.AttributeType);\\n\\n            var validationAttribute = attributeInstance as ValidationAttribute;\\n\\n            if (validationAttribute != null)\\n            {\\n                var isValid = validationAttribute.IsValid(argument);\\n                if (!isValid)\\n                {\\n                    modelState.AddModelError(parameter.Name, validationAttribute.FormatErrorMessage(parameter.Name));\\n                }\\n            }\\n        }\\n    }\\n}\\n```\\n\\n(Note: this has been implemented using ASP.NET Core 1.0. In ASP.NET 4 the Api might be slightly different, but the same approach should work there as well.)\\n\\nIf we apply this filter to our action\\n\\n```csharp\\n[ValidateActionParameters]\\npublic IActionResult Post([FromBody][Required]Product product)\\n```\\n\\nand we send a POST request with an empty body, we'll get back the expected error response.\\n\\n```json\\n{\\\"product\\\":[\\\"The product field is required.\\\"]}\\n```\\n\\nThis approach will work nicely even if we have more than one action parameters with multiple different attributes, and also works with custom validation attributes implemented by us. The action I wanted to use this with looked like this, where one of the parameters come from the query string, and the other from the body.\\n\\n```csharp\\n[HttpPost(\\\"{categoryId}/products\\\")]\\nIActionResult Post([CustomNotEmptyGuid]Guid categoryId, [FromBody][Required]Product product)\\n```\\n\\nSince the above code iterates over all the parameters and attributes, we'll get a list of all the errors in the response.\\n\\nI uploaded the source code to this [Github respository](https://github.com/markvincze/rest-api-helpers/tree/master), and pushed the [package](https://www.nuget.org/packages/RestApiHelpers/) to NuGet. I plan to extend it with implementation of other cross-cutting concerns useful in a Rest Api in the future.\"}]],\"sections\":[[10,0]]}",
        "html": "<h2 id=\"modelvalidationinmvc\">Model validation in MVC</h2>\n<p>In both MVC and Web Api we can use the attributes provided in the <code>System.ComponentModel.DataAnnotations</code> namespace to specify validation rules for our models.</p>\n<p>Let's say we have a controller action with the following signature, accepting a single parameter populated from the request body.</p>\n<pre><code class=\"language-csharp\">public IActionResult Post([FromBody]Product product);\n</code></pre>\n<p>And we decorated our <code>Product</code> type with the following validation attributes (example taken from the <a href=\"http://www.asp.net/web-api/overview/formats-and-model-binding/model-validation-in-aspnet-web-api\">official ASP.NET documentation</a>).</p>\n<pre><code class=\"language-csharp\">public class Product\n{\n    public int Id { get; set; }\n    [Required]\n    public string Name { get; set; }\n    public decimal Price { get; set; }\n    [Range(0, 999)]\n    public double Weight { get; set; }\n}\n</code></pre>\n<p>When we call our endpoint by posting a product object in the body of our request, the framework is going to evaluate our validation attributes during the model binding process, and save its result (with possibly errors) in the <code>ModelState</code> property of our controller.<br>\nSo in the implementation of our action we can simply use the <code>ModelState</code> property to check whether the input is valid or not, and we can return its value as our response in case of an error.</p>\n<pre><code class=\"language-csharp\">public IActionResult Post(Product product)\n{\n    if (!ModelState.IsValid)\n    {\n        return HttpBadRequest(ModelState);\n    }\n\n    // Process the product...\n\n    return Ok();\n}\n</code></pre>\n<p>If we post the following Json to our endpoint (note that the required <code>Name</code> property is missing)</p>\n<pre><code class=\"language-json\">{ &quot;Id&quot;:4, &quot;Price&quot;:2.99, &quot;Weight&quot;:5 }\n</code></pre>\n<p>we get back an error response containing all the validation errors produced during the model binding.</p>\n<pre><code class=\"language-json\">{\n  &quot;name&quot;: [\n    &quot;The Name field is required.&quot;\n  ]\n}\n</code></pre>\n<h2 id=\"validationattributesonactionparameters\">Validation attributes on action parameters</h2>\n<p>There is nothing preventing us from putting validation attributes not on a model property, but on the method parameters of an action. We might want to enforce our callers to post a product object to this endpoint, so it seems logical to add the <code>Required</code> attribute on the method parameter itself.</p>\n<pre><code class=\"language-csharp\">public IActionResult Post([FromBody][Required]Product product)\n</code></pre>\n<p>The problem with this approach - which surprised me - is that it simply doesn't work. The framework does not seem to evaluate these attributes at all.</p>\n<p>If we call the endpoint with an empty request body, the value of the <code>product</code> argument will be <code>null</code>, but <code>ModelState.IsValid</code> will return true.</p>\n<h3 id=\"solution\">Solution</h3>\n<p>Luckily, it is not difficult to hook into the MVC pipeline with a custom filter attribute. With the following custom filter attribute we can iterate over all of the action parameters and evaluate all the validation attributes specified for them.</p>\n<pre><code class=\"language-csharp\">public class ValidateActionParametersAttribute : ActionFilterAttribute\n{\n    public override void OnActionExecuting(ActionExecutingContext context)\n    {\n        var descriptor = context.ActionDescriptor as ControllerActionDescriptor;\n\n        if (descriptor != null)\n        {\n            var parameters = descriptor.MethodInfo.GetParameters();\n\n            foreach (var parameter in parameters)\n            {\n                var argument = context.ActionArguments[parameter.Name];\n\n                EvaluateValidationAttributes(parameter, argument, context.ModelState);\n            }\n        }\n\n        base.OnActionExecuting(context);\n    }\n\n    private void EvaluateValidationAttributes(ParameterInfo parameter, object argument, ModelStateDictionary modelState)\n    {\n        var validationAttributes = parameter.CustomAttributes;\n\n        foreach (var attributeData in validationAttributes)\n        {\n            var attributeInstance = CustomAttributeExtensions.GetCustomAttribute(parameter, attributeData.AttributeType);\n\n            var validationAttribute = attributeInstance as ValidationAttribute;\n\n            if (validationAttribute != null)\n            {\n                var isValid = validationAttribute.IsValid(argument);\n                if (!isValid)\n                {\n                    modelState.AddModelError(parameter.Name, validationAttribute.FormatErrorMessage(parameter.Name));\n                }\n            }\n        }\n    }\n}\n</code></pre>\n<p>(Note: this has been implemented using ASP.NET Core 1.0. In ASP.NET 4 the Api might be slightly different, but the same approach should work there as well.)</p>\n<p>If we apply this filter to our action</p>\n<pre><code class=\"language-csharp\">[ValidateActionParameters]\npublic IActionResult Post([FromBody][Required]Product product)\n</code></pre>\n<p>and we send a POST request with an empty body, we'll get back the expected error response.</p>\n<pre><code class=\"language-json\">{&quot;product&quot;:[&quot;The product field is required.&quot;]}\n</code></pre>\n<p>This approach will work nicely even if we have more than one action parameters with multiple different attributes, and also works with custom validation attributes implemented by us. The action I wanted to use this with looked like this, where one of the parameters come from the query string, and the other from the body.</p>\n<pre><code class=\"language-csharp\">[HttpPost(&quot;{categoryId}/products&quot;)]\nIActionResult Post([CustomNotEmptyGuid]Guid categoryId, [FromBody][Required]Product product)\n</code></pre>\n<p>Since the above code iterates over all the parameters and attributes, we'll get a list of all the errors in the response.</p>\n<p>I uploaded the source code to this <a href=\"https://github.com/markvincze/rest-api-helpers/tree/master\">Github respository</a>, and pushed the <a href=\"https://www.nuget.org/packages/RestApiHelpers/\">package</a> to NuGet. I plan to extend it with implementation of other cross-cutting concerns useful in a Rest Api in the future.</p>\n",
        "comment_id": "13",
        "plaintext": "Model validation in MVC\nIn both MVC and Web Api we can use the attributes provided in the \nSystem.ComponentModel.DataAnnotations  namespace to specify validation rules for\nour models.\n\nLet's say we have a controller action with the following signature, accepting a\nsingle parameter populated from the request body.\n\npublic IActionResult Post([FromBody]Product product);\n\n\nAnd we decorated our Product  type with the following validation attributes\n(example taken from the official ASP.NET documentation\n[http://www.asp.net/web-api/overview/formats-and-model-binding/model-validation-in-aspnet-web-api]\n).\n\npublic class Product\n{\n    public int Id { get; set; }\n    [Required]\n    public string Name { get; set; }\n    public decimal Price { get; set; }\n    [Range(0, 999)]\n    public double Weight { get; set; }\n}\n\n\nWhen we call our endpoint by posting a product object in the body of our\nrequest, the framework is going to evaluate our validation attributes during the\nmodel binding process, and save its result (with possibly errors) in the \nModelState  property of our controller.\nSo in the implementation of our action we can simply use the ModelState \nproperty to check whether the input is valid or not, and we can return its value\nas our response in case of an error.\n\npublic IActionResult Post(Product product)\n{\n    if (!ModelState.IsValid)\n    {\n        return HttpBadRequest(ModelState);\n    }\n\n    // Process the product...\n\n    return Ok();\n}\n\n\nIf we post the following Json to our endpoint (note that the required Name \nproperty is missing)\n\n{ \"Id\":4, \"Price\":2.99, \"Weight\":5 }\n\n\nwe get back an error response containing all the validation errors produced\nduring the model binding.\n\n{\n  \"name\": [\n    \"The Name field is required.\"\n  ]\n}\n\n\nValidation attributes on action parameters\nThere is nothing preventing us from putting validation attributes not on a model\nproperty, but on the method parameters of an action. We might want to enforce\nour callers to post a product object to this endpoint, so it seems logical to\nadd the Required  attribute on the method parameter itself.\n\npublic IActionResult Post([FromBody][Required]Product product)\n\n\nThe problem with this approach - which surprised me - is that it simply doesn't\nwork. The framework does not seem to evaluate these attributes at all.\n\nIf we call the endpoint with an empty request body, the value of the product \nargument will be null, but ModelState.IsValid  will return true.\n\nSolution\nLuckily, it is not difficult to hook into the MVC pipeline with a custom filter\nattribute. With the following custom filter attribute we can iterate over all of\nthe action parameters and evaluate all the validation attributes specified for\nthem.\n\npublic class ValidateActionParametersAttribute : ActionFilterAttribute\n{\n    public override void OnActionExecuting(ActionExecutingContext context)\n    {\n        var descriptor = context.ActionDescriptor as ControllerActionDescriptor;\n\n        if (descriptor != null)\n        {\n            var parameters = descriptor.MethodInfo.GetParameters();\n\n            foreach (var parameter in parameters)\n            {\n                var argument = context.ActionArguments[parameter.Name];\n\n                EvaluateValidationAttributes(parameter, argument, context.ModelState);\n            }\n        }\n\n        base.OnActionExecuting(context);\n    }\n\n    private void EvaluateValidationAttributes(ParameterInfo parameter, object argument, ModelStateDictionary modelState)\n    {\n        var validationAttributes = parameter.CustomAttributes;\n\n        foreach (var attributeData in validationAttributes)\n        {\n            var attributeInstance = CustomAttributeExtensions.GetCustomAttribute(parameter, attributeData.AttributeType);\n\n            var validationAttribute = attributeInstance as ValidationAttribute;\n\n            if (validationAttribute != null)\n            {\n                var isValid = validationAttribute.IsValid(argument);\n                if (!isValid)\n                {\n                    modelState.AddModelError(parameter.Name, validationAttribute.FormatErrorMessage(parameter.Name));\n                }\n            }\n        }\n    }\n}\n\n\n(Note: this has been implemented using ASP.NET Core 1.0. In ASP.NET 4 the Api\nmight be slightly different, but the same approach should work there as well.)\n\nIf we apply this filter to our action\n\n[ValidateActionParameters]\npublic IActionResult Post([FromBody][Required]Product product)\n\n\nand we send a POST request with an empty body, we'll get back the expected error\nresponse.\n\n{\"product\":[\"The product field is required.\"]}\n\n\nThis approach will work nicely even if we have more than one action parameters\nwith multiple different attributes, and also works with custom validation\nattributes implemented by us. The action I wanted to use this with looked like\nthis, where one of the parameters come from the query string, and the other from\nthe body.\n\n[HttpPost(\"{categoryId}/products\")]\nIActionResult Post([CustomNotEmptyGuid]Guid categoryId, [FromBody][Required]Product product)\n\n\nSince the above code iterates over all the parameters and attributes, we'll get\na list of all the errors in the response.\n\nI uploaded the source code to this Github respository\n[https://github.com/markvincze/rest-api-helpers/tree/master], and pushed the \npackage [https://www.nuget.org/packages/RestApiHelpers/]  to NuGet. I plan to\nextend it with implementation of other cross-cutting concerns useful in a Rest\nApi in the future.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "A simple approach to evaluate DataAnnotation validation attributes not only on model properties, but on the action method parameters as well.",
        "author_id": "1",
        "created_at": "2016-02-28 14:23:08",
        "created_by": "1",
        "updated_at": "2016-09-09 19:26:43",
        "updated_by": "1",
        "published_at": "2016-02-28 14:23:00",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2968",
        "uuid": "5c2c3036-70fc-47fe-b5a2-82c9e3344baf",
        "title": "About me",
        "slug": "about-me",
        "mobiledoc": "{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I'm a software developer specialized in C# and modern C++, with extensive experience in using the Microsoft technology stack. I try to blog about things I find interesting during work or hobby projects.\\nI'm currently working at [Travix](http://www.travix.com) in Amsterdam.\\n\\nI worked with the following technologies.\\n\\n- C#: 6 years of experience, client and server side.\\n- C++: 3 years of developing modern, cross-platform code.\\n- XAML: Several projects for various platforms.\\n- ASP.NET: Used Web Forms, MVC and Web API.\\n- Other: WPF, Windows Phone, Azure, WinRT, WCF, JNI, TypeScript, MS SQL, Entity Framework, MS Mediaroom.\\n\\nI occasionly [tweet](https://twitter.com/mrkvincze), and you can find me on [Stack Overflow](http://stackoverflow.com/users/974733/mark-vincze) as well.\"}]],\"markups\":[],\"sections\":[[10,0]]}",
        "html": "<p>I'm a software developer specialized in C# and modern C++, with extensive experience in using the Microsoft technology stack. I try to blog about things I find interesting during work or hobby projects.<br>\nI'm currently working at <a href=\"http://www.travix.com\">Travix</a> in Amsterdam.</p>\n<p>I worked with the following technologies.</p>\n<ul>\n<li>C#: 6 years of experience, client and server side.</li>\n<li>C++: 3 years of developing modern, cross-platform code.</li>\n<li>XAML: Several projects for various platforms.</li>\n<li>ASP.NET: Used Web Forms, MVC and Web API.</li>\n<li>Other: WPF, Windows Phone, Azure, WinRT, WCF, JNI, TypeScript, MS SQL, Entity Framework, MS Mediaroom.</li>\n</ul>\n<p>I occasionly <a href=\"https://twitter.com/mrkvincze\">tweet</a>, and you can find me on <a href=\"http://stackoverflow.com/users/974733/mark-vincze\">Stack Overflow</a> as well.</p>\n",
        "comment_id": "7",
        "plaintext": "I'm a software developer specialized in C# and modern C++, with extensive\nexperience in using the Microsoft technology stack. I try to blog about things I\nfind interesting during work or hobby projects.\nI'm currently working at Travix [http://www.travix.com]  in Amsterdam.\n\nI worked with the following technologies.\n\n * C#: 6 years of experience, client and server side.\n * C++: 3 years of developing modern, cross-platform code.\n * XAML: Several projects for various platforms.\n * ASP.NET: Used Web Forms, MVC and Web API.\n * Other: WPF, Windows Phone, Azure, WinRT, WCF, JNI, TypeScript, MS SQL, Entity\n   Framework, MS Mediaroom.\n\nI occasionly tweet [https://twitter.com/mrkvincze], and you can find me on \nStack\nOverflow [http://stackoverflow.com/users/974733/mark-vincze]  as well.",
        "feature_image": null,
        "featured": 0,
        "page": 1,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": "Mark Vincze",
        "meta_description": "I???m a software developer specialized in C# and modern C++, with extensive experience in using the Microsoft technology stack.",
        "author_id": "1",
        "created_at": "2015-08-15 17:01:56",
        "created_by": "1",
        "updated_at": "2019-01-12 11:27:46",
        "updated_by": "1",
        "published_at": "2015-08-15 17:08:20",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2969",
        "uuid": "7a3c26e2-b55e-40a9-9a4a-5eaf71f8c84b",
        "title": "Setting up an AppVeyor pipeline for Golang",
        "slug": "setting-up-an-appveyor-pipeline-for-golang",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Recently at my day job I have been working on a [Golang-based application](https://github.com/Travix-International/Travix.Core.Adk), for which I wanted to set up an automated CD pipeline for building and releasing. Our application is a command line tool, so the release part is basically copying and uploading the binaries to a specified location.\\n\\nSince I have been using AppVeyor for my .NET Core projects ([Stubbery](https://github.com/markvincze/Stubbery) and [RestApiHelpers](https://github.com/markvincze/rest-api-helpers)), and it's working really nicely, it seemed to be the obvious choice for Golang as well.\\n\\nI did a quick Google search, and was surprised, because I didn't find any guides about setting this up. On the other hand, I found the `appveyor.yml` file of another [open-source Golang-project](https://github.com/oschwald/maxminddb-golang/blob/master/appveyor.yml), which looked rather simple, so I decided to try to set up my pipeline based on that. It turned out to be even simpler than what I expected.\\n\\n## Setting up the build\\n\\nIn this section I'll describe a sample AppVeyor configuration, which is enough to build a Golang application.\\n(I'll only focus on the parts relevant to Golang.)\\n\\nWe have two different options to configure AppVeyor projects: one is two use a management website to set the different properties of the project, and the other is to put an `appveyor.yml` file at the root of our repository, which describes the same settings, and is automatically processed by AppVeyor.\\n\\nI chose the latter approach, because it keeps the configuration close to the source code, this way it's easy to track its history, and it can be part of pull requests. Furthermore, the `yml` file contains only the settings which are relevant for our project, so it's often quite small and easy to understand.\\n\\n### The AppVeyor configuration\\n\\nI'll show the relevant parts of the `appveyor.yml` file and describe them in detail.\\n\\n### Basic settings\\n\\n```yaml\\nversion: 1.0.0.{build}\\n\\nplatform: x64\\n\\nbranches:\\n  only:\\n    - master\\n```\\n\\nThe version number can be customized to fit our needs, and it can be manually bumped when we are releasing a new version. The `{build}` argument is an internal environment variable of AppVeyor, which will be automatically bumped on every build.\\n\\nI configured my pipeline to be triggered only by pushes to the master branch, this can be modified in the `branches` section.\\n\\n### Cloning and prerequisites\\n\\n```yaml\\nclone_folder: c:\\\\gopath\\\\src\\\\github.com\\\\account\\\\myproject\\n\\nenvironment:\\n  GOPATH: c:\\\\gopath\\n\\ninstall:\\n  - echo %PATH%\\n  - echo %GOPATH%\\n  - set PATH=%GOPATH%\\\\bin;c:\\\\go\\\\bin;%PATH%\\n  - go version\\n  - go env\\n```\\n\\nThe `clone_folder` property specifies the location our repo gets cloned to. This is important in the Golang ecosystem, since we usually want our projects to reside under the folders specified by either the `GOROOT` or the `GOPATH` environment variables. Hence we are setting that root folder as the value of the `GOPATH` variable in the `environment` section.\\n\\nIt turns out that the default AppVeyor agent image  among [many other technologies](https://www.appveyor.com/docs/installed-software)  comes with Go preinstalled, so in the `install` section we don't have to do any real installation.  \\nHowever, it's still a good idea to print out some dianostic information about our Go environment, which can help troubleshooting if the build goes south. We also add `%GOPATH%\\\\bin` and `C:\\\\go\\\\bin` to the PATH, so we can use the go toolchain in our build scripts without specifying full paths.\\n\\n### Building\\n\\nDepending on how complicated our build is, we can either use a `go build` command directly in our `appveyor.yml`.\\n\\n```yaml\\nbuild_script:\\n  - go build -o buildOutput\\\\myapp -i .\\n```\\n\\nOr, if we have a more complicated build (for example in my case we are cross-compiling the application for three different platforms), it's a better idea to put a script doing the build in our repository (I used PowerShell), and execute that in the pipeline.\\n\\n```yaml\\nbuild_script:\\n  - ps: .\\\\build.ps1\\n```\\n\\nThe next thing to specify is the list of files to use during the deployment. These have to be added to the `artifacts` section.\\n\\n```yaml\\nartifacts:\\n  - path: buildOutput/myapp\\n    name: binary\\n```\\n\\nWe can list multiple files if we are generating more than one build output.\\n\\n### Deployment\\n\\nThe last part is doing the actual deployment of our artifacts.  \\nNow this part will completely depend on the way we want to release are application. I only investigated using GitHub Releases, since that was perfect for my purposes, but AppVeyor has built-in support for numerous other systems (Amazon S3, different Azure services, NuGet, etc.). And I'm sure we can roll our own deployment script as well.\\n\\nHere is an example for setting up the `deploy` section for GitHub Releases.\\n\\n```yaml\\ndeploy:\\n  release: myapp-$(appveyor_build_version)\\n  description: 'This is a release of my awesome application.'\\n  provider: GitHub\\n  auth_token:\\n    secure: ZkOJAiZBmapKpbiqovaofs+W0foBWaV9Jom4yBYzcRKlAk4Bee+5b7t+5LrQRVn8\\n  artifact: binary # This is the name we specified in the artifacts section.\\n  draft: false\\n  prerelease: false\\n  on:\\n    branch: master\\n```\\n\\nWhere the value of the `secure` property is a **Personal access token** we have to set up on the [settings page](https://github.com/settings/tokens) of GitHub.\\n\\nOne gotcha is that we shouldn't add the raw value of our token to the `appveyor.yml`, since then anyone would be able to read it from our repository (and GitHub is actually smart enough to notice that we erroneously added the plain value of a token to our repo and disables it automatically).\\n\\nWe should rather encrypt it with the **Encrypt data** option on the AppVeyor management site, and use that value instead. AppVeyor will recognize that its an encrypted token and decrypt it automatically.\\n\\nSo this is the complete `appveyor.yml` file we have to put at the root of our repository to get a basic build working. (More details about all the settings can be found in [official documentation](https://www.appveyor.com/docs).)\\n\\n```yaml\\nversion: 1.0.0.{build}\\n\\nplatform: x64\\n\\nbranches:\\n  only:\\n    - master\\n\\nclone_folder: c:\\\\gopath\\\\src\\\\github.com\\\\account\\\\myproject\\n\\nenvironment:\\n  GOPATH: c:\\\\gopath\\n\\ninstall:\\n  - echo %PATH%\\n  - echo %GOPATH%\\n  - set PATH=%GOPATH%\\\\bin;c:\\\\go\\\\bin;%PATH%\\n  - go version\\n  - go env\\n\\nbuild_script:\\n  - go build -o buildOutput\\\\myapp -i .\\n\\nbuild_script:\\n  - ps: .\\\\build.ps1\\n\\nartifacts:\\n  - path: buildOutput/myapp\\n    name: binary\\n\\ndeploy:\\n  release: myapp-$(appveyor_build_version)\\n  description: 'This is a release of my awesome application.'\\n  provider: GitHub\\n  auth_token:\\n    secure: FW3tJ3fMncxvs58/ifSP7w+kBl9BlxvRMr9liHmnBs14ALRG8Vfyol+sNhj9u2JA\\n  artifact: binary # This is the name we specified in the artifacts section.\\n  draft: false\\n  prerelease: false\\n  on:\\n    branch: master\\n```\\n\\nI think this example shows that setting up a build pipeline for a Golang project in AppVeyor is really simple, and I hope this post will save some time for people getting started with continuous delivery in Golang.\"}]],\"sections\":[[10,0]]}",
        "html": "<p>Recently at my day job I have been working on a <a href=\"https://github.com/Travix-International/Travix.Core.Adk\">Golang-based application</a>, for which I wanted to set up an automated CD pipeline for building and releasing. Our application is a command line tool, so the release part is basically copying and uploading the binaries to a specified location.</p>\n<p>Since I have been using AppVeyor for my .NET Core projects (<a href=\"https://github.com/markvincze/Stubbery\">Stubbery</a> and <a href=\"https://github.com/markvincze/rest-api-helpers\">RestApiHelpers</a>), and it's working really nicely, it seemed to be the obvious choice for Golang as well.</p>\n<p>I did a quick Google search, and was surprised, because I didn't find any guides about setting this up. On the other hand, I found the <code>appveyor.yml</code> file of another <a href=\"https://github.com/oschwald/maxminddb-golang/blob/master/appveyor.yml\">open-source Golang-project</a>, which looked rather simple, so I decided to try to set up my pipeline based on that. It turned out to be even simpler than what I expected.</p>\n<h2 id=\"settingupthebuild\">Setting up the build</h2>\n<p>In this section I'll describe a sample AppVeyor configuration, which is enough to build a Golang application.<br>\n(I'll only focus on the parts relevant to Golang.)</p>\n<p>We have two different options to configure AppVeyor projects: one is two use a management website to set the different properties of the project, and the other is to put an <code>appveyor.yml</code> file at the root of our repository, which describes the same settings, and is automatically processed by AppVeyor.</p>\n<p>I chose the latter approach, because it keeps the configuration close to the source code, this way it's easy to track its history, and it can be part of pull requests. Furthermore, the <code>yml</code> file contains only the settings which are relevant for our project, so it's often quite small and easy to understand.</p>\n<h3 id=\"theappveyorconfiguration\">The AppVeyor configuration</h3>\n<p>I'll show the relevant parts of the <code>appveyor.yml</code> file and describe them in detail.</p>\n<h3 id=\"basicsettings\">Basic settings</h3>\n<pre><code class=\"language-yaml\">version: 1.0.0.{build}\n\nplatform: x64\n\nbranches:\n  only:\n    - master\n</code></pre>\n<p>The version number can be customized to fit our needs, and it can be manually bumped when we are releasing a new version. The <code>{build}</code> argument is an internal environment variable of AppVeyor, which will be automatically bumped on every build.</p>\n<p>I configured my pipeline to be triggered only by pushes to the master branch, this can be modified in the <code>branches</code> section.</p>\n<h3 id=\"cloningandprerequisites\">Cloning and prerequisites</h3>\n<pre><code class=\"language-yaml\">clone_folder: c:\\gopath\\src\\github.com\\account\\myproject\n\nenvironment:\n  GOPATH: c:\\gopath\n\ninstall:\n  - echo %PATH%\n  - echo %GOPATH%\n  - set PATH=%GOPATH%\\bin;c:\\go\\bin;%PATH%\n  - go version\n  - go env\n</code></pre>\n<p>The <code>clone_folder</code> property specifies the location our repo gets cloned to. This is important in the Golang ecosystem, since we usually want our projects to reside under the folders specified by either the <code>GOROOT</code> or the <code>GOPATH</code> environment variables. Hence we are setting that root folder as the value of the <code>GOPATH</code> variable in the <code>environment</code> section.</p>\n<p>It turns out that the default AppVeyor agent image  among <a href=\"https://www.appveyor.com/docs/installed-software\">many other technologies</a>  comes with Go preinstalled, so in the <code>install</code> section we don't have to do any real installation.<br>\nHowever, it's still a good idea to print out some dianostic information about our Go environment, which can help troubleshooting if the build goes south. We also add <code>%GOPATH%\\bin</code> and <code>C:\\go\\bin</code> to the PATH, so we can use the go toolchain in our build scripts without specifying full paths.</p>\n<h3 id=\"building\">Building</h3>\n<p>Depending on how complicated our build is, we can either use a <code>go build</code> command directly in our <code>appveyor.yml</code>.</p>\n<pre><code class=\"language-yaml\">build_script:\n  - go build -o buildOutput\\myapp -i .\n</code></pre>\n<p>Or, if we have a more complicated build (for example in my case we are cross-compiling the application for three different platforms), it's a better idea to put a script doing the build in our repository (I used PowerShell), and execute that in the pipeline.</p>\n<pre><code class=\"language-yaml\">build_script:\n  - ps: .\\build.ps1\n</code></pre>\n<p>The next thing to specify is the list of files to use during the deployment. These have to be added to the <code>artifacts</code> section.</p>\n<pre><code class=\"language-yaml\">artifacts:\n  - path: buildOutput/myapp\n    name: binary\n</code></pre>\n<p>We can list multiple files if we are generating more than one build output.</p>\n<h3 id=\"deployment\">Deployment</h3>\n<p>The last part is doing the actual deployment of our artifacts.<br>\nNow this part will completely depend on the way we want to release are application. I only investigated using GitHub Releases, since that was perfect for my purposes, but AppVeyor has built-in support for numerous other systems (Amazon S3, different Azure services, NuGet, etc.). And I'm sure we can roll our own deployment script as well.</p>\n<p>Here is an example for setting up the <code>deploy</code> section for GitHub Releases.</p>\n<pre><code class=\"language-yaml\">deploy:\n  release: myapp-$(appveyor_build_version)\n  description: 'This is a release of my awesome application.'\n  provider: GitHub\n  auth_token:\n    secure: ZkOJAiZBmapKpbiqovaofs+W0foBWaV9Jom4yBYzcRKlAk4Bee+5b7t+5LrQRVn8\n  artifact: binary # This is the name we specified in the artifacts section.\n  draft: false\n  prerelease: false\n  on:\n    branch: master\n</code></pre>\n<p>Where the value of the <code>secure</code> property is a <strong>Personal access token</strong> we have to set up on the <a href=\"https://github.com/settings/tokens\">settings page</a> of GitHub.</p>\n<p>One gotcha is that we shouldn't add the raw value of our token to the <code>appveyor.yml</code>, since then anyone would be able to read it from our repository (and GitHub is actually smart enough to notice that we erroneously added the plain value of a token to our repo and disables it automatically).</p>\n<p>We should rather encrypt it with the <strong>Encrypt data</strong> option on the AppVeyor management site, and use that value instead. AppVeyor will recognize that its an encrypted token and decrypt it automatically.</p>\n<p>So this is the complete <code>appveyor.yml</code> file we have to put at the root of our repository to get a basic build working. (More details about all the settings can be found in <a href=\"https://www.appveyor.com/docs\">official documentation</a>.)</p>\n<pre><code class=\"language-yaml\">version: 1.0.0.{build}\n\nplatform: x64\n\nbranches:\n  only:\n    - master\n\nclone_folder: c:\\gopath\\src\\github.com\\account\\myproject\n\nenvironment:\n  GOPATH: c:\\gopath\n\ninstall:\n  - echo %PATH%\n  - echo %GOPATH%\n  - set PATH=%GOPATH%\\bin;c:\\go\\bin;%PATH%\n  - go version\n  - go env\n\nbuild_script:\n  - go build -o buildOutput\\myapp -i .\n\nbuild_script:\n  - ps: .\\build.ps1\n\nartifacts:\n  - path: buildOutput/myapp\n    name: binary\n\ndeploy:\n  release: myapp-$(appveyor_build_version)\n  description: 'This is a release of my awesome application.'\n  provider: GitHub\n  auth_token:\n    secure: FW3tJ3fMncxvs58/ifSP7w+kBl9BlxvRMr9liHmnBs14ALRG8Vfyol+sNhj9u2JA\n  artifact: binary # This is the name we specified in the artifacts section.\n  draft: false\n  prerelease: false\n  on:\n    branch: master\n</code></pre>\n<p>I think this example shows that setting up a build pipeline for a Golang project in AppVeyor is really simple, and I hope this post will save some time for people getting started with continuous delivery in Golang.</p>\n",
        "comment_id": "24",
        "plaintext": "Recently at my day job I have been working on a Golang-based application\n[https://github.com/Travix-International/Travix.Core.Adk], for which I wanted to\nset up an automated CD pipeline for building and releasing. Our application is a\ncommand line tool, so the release part is basically copying and uploading the\nbinaries to a specified location.\n\nSince I have been using AppVeyor for my .NET Core projects (Stubbery\n[https://github.com/markvincze/Stubbery]  and RestApiHelpers\n[https://github.com/markvincze/rest-api-helpers]), and it's working really\nnicely, it seemed to be the obvious choice for Golang as well.\n\nI did a quick Google search, and was surprised, because I didn't find any guides\nabout setting this up. On the other hand, I found the appveyor.yml  file of\nanother open-source Golang-project\n[https://github.com/oschwald/maxminddb-golang/blob/master/appveyor.yml], which\nlooked rather simple, so I decided to try to set up my pipeline based on that.\nIt turned out to be even simpler than what I expected.\n\nSetting up the build\nIn this section I'll describe a sample AppVeyor configuration, which is enough\nto build a Golang application.\n(I'll only focus on the parts relevant to Golang.)\n\nWe have two different options to configure AppVeyor projects: one is two use a\nmanagement website to set the different properties of the project, and the other\nis to put an appveyor.yml  file at the root of our repository, which describes\nthe same settings, and is automatically processed by AppVeyor.\n\nI chose the latter approach, because it keeps the configuration close to the\nsource code, this way it's easy to track its history, and it can be part of pull\nrequests. Furthermore, the yml  file contains only the settings which are\nrelevant for our project, so it's often quite small and easy to understand.\n\nThe AppVeyor configuration\nI'll show the relevant parts of the appveyor.yml  file and describe them in\ndetail.\n\nBasic settings\nversion: 1.0.0.{build}\n\nplatform: x64\n\nbranches:\n  only:\n    - master\n\n\nThe version number can be customized to fit our needs, and it can be manually\nbumped when we are releasing a new version. The {build}  argument is an internal\nenvironment variable of AppVeyor, which will be automatically bumped on every\nbuild.\n\nI configured my pipeline to be triggered only by pushes to the master branch,\nthis can be modified in the branches  section.\n\nCloning and prerequisites\nclone_folder: c:\\gopath\\src\\github.com\\account\\myproject\n\nenvironment:\n  GOPATH: c:\\gopath\n\ninstall:\n  - echo %PATH%\n  - echo %GOPATH%\n  - set PATH=%GOPATH%\\bin;c:\\go\\bin;%PATH%\n  - go version\n  - go env\n\n\nThe clone_folder  property specifies the location our repo gets cloned to. This\nis important in the Golang ecosystem, since we usually want our projects to\nreside under the folders specified by either the GOROOT  or the GOPATH \nenvironment variables. Hence we are setting that root folder as the value of the\n GOPATH  variable in the environment  section.\n\nIt turns out that the default AppVeyor agent image  among many other\ntechnologies [https://www.appveyor.com/docs/installed-software]   comes with Go\npreinstalled, so in the install  section we don't have to do any real\ninstallation.\nHowever, it's still a good idea to print out some dianostic information about\nour Go environment, which can help troubleshooting if the build goes south. We\nalso add %GOPATH%\\bin  and C:\\go\\bin  to the PATH, so we can use the go\ntoolchain in our build scripts without specifying full paths.\n\nBuilding\nDepending on how complicated our build is, we can either use a go build  command\ndirectly in our appveyor.yml.\n\nbuild_script:\n  - go build -o buildOutput\\myapp -i .\n\n\nOr, if we have a more complicated build (for example in my case we are\ncross-compiling the application for three different platforms), it's a better\nidea to put a script doing the build in our repository (I used PowerShell), and\nexecute that in the pipeline.\n\nbuild_script:\n  - ps: .\\build.ps1\n\n\nThe next thing to specify is the list of files to use during the deployment.\nThese have to be added to the artifacts  section.\n\nartifacts:\n  - path: buildOutput/myapp\n    name: binary\n\n\nWe can list multiple files if we are generating more than one build output.\n\nDeployment\nThe last part is doing the actual deployment of our artifacts.\nNow this part will completely depend on the way we want to release are\napplication. I only investigated using GitHub Releases, since that was perfect\nfor my purposes, but AppVeyor has built-in support for numerous other systems\n(Amazon S3, different Azure services, NuGet, etc.). And I'm sure we can roll our\nown deployment script as well.\n\nHere is an example for setting up the deploy  section for GitHub Releases.\n\ndeploy:\n  release: myapp-$(appveyor_build_version)\n  description: 'This is a release of my awesome application.'\n  provider: GitHub\n  auth_token:\n    secure: ZkOJAiZBmapKpbiqovaofs+W0foBWaV9Jom4yBYzcRKlAk4Bee+5b7t+5LrQRVn8\n  artifact: binary # This is the name we specified in the artifacts section.\n  draft: false\n  prerelease: false\n  on:\n    branch: master\n\n\nWhere the value of the secure  property is a Personal access token  we have to\nset up on the settings page [https://github.com/settings/tokens]  of GitHub.\n\nOne gotcha is that we shouldn't add the raw value of our token to the \nappveyor.yml, since then anyone would be able to read it from our repository\n(and GitHub is actually smart enough to notice that we erroneously added the\nplain value of a token to our repo and disables it automatically).\n\nWe should rather encrypt it with the Encrypt data  option on the AppVeyor\nmanagement site, and use that value instead. AppVeyor will recognize that its an\nencrypted token and decrypt it automatically.\n\nSo this is the complete appveyor.yml  file we have to put at the root of our\nrepository to get a basic build working. (More details about all the settings\ncan be found in official documentation [https://www.appveyor.com/docs].)\n\nversion: 1.0.0.{build}\n\nplatform: x64\n\nbranches:\n  only:\n    - master\n\nclone_folder: c:\\gopath\\src\\github.com\\account\\myproject\n\nenvironment:\n  GOPATH: c:\\gopath\n\ninstall:\n  - echo %PATH%\n  - echo %GOPATH%\n  - set PATH=%GOPATH%\\bin;c:\\go\\bin;%PATH%\n  - go version\n  - go env\n\nbuild_script:\n  - go build -o buildOutput\\myapp -i .\n\nbuild_script:\n  - ps: .\\build.ps1\n\nartifacts:\n  - path: buildOutput/myapp\n    name: binary\n\ndeploy:\n  release: myapp-$(appveyor_build_version)\n  description: 'This is a release of my awesome application.'\n  provider: GitHub\n  auth_token:\n    secure: FW3tJ3fMncxvs58/ifSP7w+kBl9BlxvRMr9liHmnBs14ALRG8Vfyol+sNhj9u2JA\n  artifact: binary # This is the name we specified in the artifacts section.\n  draft: false\n  prerelease: false\n  on:\n    branch: master\n\n\nI think this example shows that setting up a build pipeline for a Golang project\nin AppVeyor is really simple, and I hope this post will save some time for\npeople getting started with continuous delivery in Golang.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "This post gives and Introduction to setting up a continuous delivery pipeline for a Golang-based project in AppVeyor.",
        "author_id": "1",
        "created_at": "2016-07-16 15:32:50",
        "created_by": "1",
        "updated_at": "2016-10-30 14:56:10",
        "updated_by": "1",
        "published_at": "2016-07-16 15:47:28",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de296a",
        "uuid": "cd5f8191-a32d-4ada-a55d-0bf5e0745d69",
        "title": "Migrating a Ghost blog from SQLite to Postgre on OpenShift",
        "slug": "migrating-a-ghost-blog-from-sqlite-to-postgre-on-openshift",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"When I started this blog, I wanted to use the [Ghost](https://ghost.org/) blogging platform. I was looking for a free solution for hosting it, and I didn't mind a little tinkering in order to get it running.  \\nAt the time, the best approach I could find was hosting it on an Amazon EC2 instance, since Amazon offered a Free Tier, with which you could run a small Linux instance for free, for the duration of one year.\\n\\nThe one year duration of the Amazon Free Tier is coming to an end, so a couple of weeks ago I started to look for an alternative to host my blog. I found the [OpenShift](https://www.openshift.com/) platform by Red Hat, where you can run a small machine instance for free, which seemed like the perfect alternative to run my blog.  \\n\\nThe only problem was that on Amazon I was using the file-based SQLite approach to store my blog data, whereas on OpenShift the recommended method either MySql or Postgre. I decided to go with Postgre, so I needed to migrate all my blog data there from SQLite.\\n\\n## Getting started\\n\\nCreating a new Ghost blog on OpenShift is very simple, you can follow the guide on [howtoinstallghost](https://www.howtoinstallghost.com/how-to-install-ghost-on-openshift/).\\n\\nAfter you created the new blog, you need to clone its git repository, copy all your static content, namely the `content/apps` and `content/themes` directories overwriting the existing content, and push it to the remote.\\n\\nThe other, much bigger task is to migrate the actual data from SQLite to the Postgre database.\\n\\n## Connecting to the Postgre DB running on OpenShift\\n\\nConnecting to your new Postgre DB is simple, but it needs an extra step. Because of security reasons, you need a client side tool, which will open a secure tunnel between your computer and the DB server.\\n\\nFirst you have to install `rhc` by following the [official guide](https://developers.openshift.com/en/getting-started-windows.html#client-tools).\\n\\nAfter this, you have to initiate port-forwarding by executing the following command:\\n\\n```bash\\nrhc port-forward -a your-app-name\\n```\\n\\nIn the output of the command you'll find the port on localhost to which you have to connect to reach your Postgre DB.\\n\\n![Use the rhc command to open a port to your Postgre DB.](/content/images/2016/04/rhc-port-forward.png)\\n\\nSo I needed to connect to `localhost:62696`.  \\nFor actually connecting to the DB, I was using the official [pgAdmin](http://www.pgadmin.org/) client, but probably you can do the same thing with the command-line Postgre client too.\\n\\n## Clearing the DB\\n\\nWe want to migrate all our data and settings from the existing DB, however, the OpenShift Postgre DB contains some sample data by default. That data needs to be erased.  \\nWith PostgreSQL, you can erase all data from a table using the `TRUNCATE TABLE` command. Furthermore, you can find a helpful little procedure in this [SO answer](https://stackoverflow.com/questions/2829158/truncating-all-tables-in-a-postgres-database), with which you can truncate all your tables in one step. (But if you just manually clear all your tables, that works too.)\\n\\n## Migrate Sqlite data\\n\\nWith Sqlite3 it's easy to create a SQL script that creates an exact copy of our database and fills all the existing data into it. We can do this with the following command:\\n\\n```bash\\nsqlite3 ghost.db .dump | grep -v \\\"^CREATE\\\" | grep -v \\\"PRAGMA\\\" | grep -v \\\"sqlite_sequence\\\" > blog.dump.sql\\n```\\n\\nWith piping a couple of greps together we remove all the lines we don't want to use with Postgre. (I've found this command on [Leonardo Andrade's blog](https://andrade.io/ghost-blog-migrate-from-sqlite-to-postgres/).]\\n\\nIn order to be able to run this script on my Postgre DB, I needed to do a couple of tweaks on the script. Luckily the whole script is enclosed in one transaction, so you're free to try to run it again and again, it'll only insert anything into your DB after you successfully fixed all problems in the script.\\n\\nIf you end up in a situation where you accidentally inserted something into your DB anyway, simply purge all records again and start over.\\n\\n## Fix errors in the Sqlite dump script\\n\\nThere are a couple of incompatibilities between Sqlite and Postgre that we have to address if we want to successfully execute this script against our Postgre DB.  \\nI used a couple of very simple regexp replacements to fix all the different errors I encountered. You can use any text editor that supports regexp replacements, such as [Sublime Text](https://www.sublimetext.com/).  \\n\\n### Fix boolean literals\\n\\nSqlite [does not have a separate boolean data type](https://www.sqlite.org/datatype3.html), rather the booleans are represented with integers having either `0` or `1` as their value. In the Postgre schema, the same columns are represented with booleans, and the literals `0` and `1` are not acceptable as boolean values, so our script will report an error everywhere booleans are used. The solution is to replace all `0` values with `FALSE`, and all `1`s with `TRUE`.  \\nThis might be tricky to do for all different occurences, so I did it separately for all boolean columns. It's easy to do a replacement in a text editor based on the surrounding content of the literal. For example this was the first error I got, where the `0` value after `NULL` was invalid:\\n\\n```sql\\n<p>That should be enough to get you started. Have fun - and let us know what you think :)</p>',NULL,0,0,'draft','en_US',NULL,NULL,1,1429738370592,1,1430050851451,1,1429738370623,1);\\n                                                                                                    ^\\n```\\n\\nSo I could fix all occurrences of this column by replacing\\n\\n```sql\\n',NULL,0,(\\\\d),'\\n```\\n\\nwith\\n\\n```sql\\n',NULL,FALSE,$1,'\\n```\\n\\nThen I repeated the replacement from `',NULL,1,(\\\\d),'` to `',NULL,TUE,$1,'`, so we take care of true values as well. \\n\\nThe second error was in the same insert statement, with the boolean value just after the one we fixed. This can be adjusted with a simple replacement, we have to replace\\n\\n```sql\\n',NULL,(FALSE|TRUE),0,'\\n```\\n\\nwith\\n\\n```sql\\n',NULL,$1,FALSE,'\\n```\\n\\nThen I repeated this one too for true values, by replacing `',NULL,(FALSE|TRUE),1,'` with `',NULL,$1,TRUE,'`.\\n\\nAnother occurrence was in the `tags` table, I had to replace `',NULL,NULL,0,NULL,NULL,` with `',NULL,NULL,FALSE,NULL,NULL,`, and similarly for `TRUE`.\\n\\n### Fix timestamps\\n\\nThe next group of errors was caused by that in the Sqlite dump script timestamps are represented as bigints (Unix timestamps), which you need to convert to Postgre timestamps.  \\nConverting a Unix timestamp to a Postgre one is simple, you can use the following expression: `TO_CHAR(TO_TIMESTAMP(bigint_field / 1000), 'DD/MM/YYYY HH24:MI:SS')`\\n\\nAll occurrences can be fixed with a general replacement from\\n\\n```sql\\n,(\\\\d{13}),\\n```\\n\\nto\\n\\n```sql\\n,TO_TIMESTAMP($1/1000),\\n```\\n\\nOn the other hand, running a general replacement like this is dangerous, because if you have the same content in any of your posts, that will be replaced as well. If you want to avoid that, you should use a couple of more granular replacements that consider the surrounding content of the script.\\n\\n### Difference in the users table\\n\\nThis problem might have occurred to me only because I was using a different version of Ghost: in the `INSERT` statement for the `users` table the number of values was one less values than the number of columns, the column `tour_text` was missing.  \\nSo at the end of those lines, I had to replace\\n\\n```sql\\nNULL,NULL,TO_TIMESTAMP(1456936064255/1000),TO_TIMESTAMP(1429738371778/1000),1,TO_TIMESTAMP(1456936064255/1000),1);\\n```\\n\\nwith\\n\\n```sql\\nNULL,NULL,NULL,TO_TIMESTAMP(1456936064255/1000),TO_TIMESTAMP(1429738371778/1000),1,TO_TIMESTAMP(1456936064255/1000),1);\\n```\\n\\n### Missing columns in the clients table\\n\\nA couple of columns were missing from `clients`, I looked up their default value in a newly created DB, and based on that I had to replace\\n\\n```sql\\nINSERT INTO \\\"clients\\\" VALUES(1,'6d74790e-d879-484f-85bb-d1b608e46f52','Ghost Admin','ghost-admin','not_available',TO_TIMESTAMP(1429738370626/1000),1,TO_TIMESTAMP(1429738370626/1000),1);\\n```\\n\\nwith\\n\\n```sql\\nINSERT INTO \\\"clients\\\" VALUES(1,'6d74790e-d879-484f-85bb-d1b608e46f52','Ghost Admin','ghost-admin','','not_available','','enabled','ua','description',TO_TIMESTAMP(1429738370626/1000),1,TO_TIMESTAMP(1429738370626/1000),1);\\n```\\n\\nAfter all these tweaks, I was able to successfully run the script to load all the data into the DB.\\n\\n## Fix sequences\\n\\nThe last problem was that by purging all the data from the tables and inserting with explicit primary key values we messed up the current values of all our sequences, and insert statements would fail. This can be easily fixed by run the following commands.\\n\\n```sql\\nSELECT setval('accesstokens_id_seq', (SELECT MAX(id) FROM accesstokens)+1);\\nSELECT setval('app_fields_id_seq', (SELECT MAX(id) FROM app_fields)+1);\\nSELECT setval('app_settings_id_seq', (SELECT MAX(id) FROM app_settings)+1);\\nSELECT setval('apps_id_seq', (SELECT MAX(id) FROM apps)+1);\\nSELECT setval('client_trusted_domains_id_seq', (SELECT MAX(id) FROM client_trusted_domains)+1);\\nSELECT setval('clients_id_seq', (SELECT MAX(id) FROM clients)+1);\\nSELECT setval('permissions_apps_id_seq', (SELECT MAX(id) FROM permissions_apps)+1);\\nSELECT setval('permissions_id_seq', (SELECT MAX(id) FROM permissions)+1);\\nSELECT setval('permissions_roles_id_seq', (SELECT MAX(id) FROM permissions_roles)+1);\\nSELECT setval('permissions_users_id_seq', (SELECT MAX(id) FROM permissions_users)+1);\\nSELECT setval('posts_id_seq', (SELECT MAX(id) FROM posts)+1);\\nSELECT setval('posts_tags_id_seq', (SELECT MAX(id) FROM posts_tags)+1);\\nSELECT setval('refreshtokens_id_seq', (SELECT MAX(id) FROM refreshtokens)+1);\\nSELECT setval('roles_id_seq', (SELECT MAX(id) FROM roles)+1);\\nSELECT setval('roles_users_id_seq', (SELECT MAX(id) FROM roles_users)+1);\\nSELECT setval('settings_id_seq', (SELECT MAX(id) FROM settings)+1);\\nSELECT setval('tags_id_seq', (SELECT MAX(id) FROM tags)+1);\\nSELECT setval('users_id_seq', (SELECT MAX(id) FROM users)+1);\\n```\\n\\nAfter this last fix, your new Ghost blog on PostgreSQL should be up and running!\"}]],\"sections\":[[10,0]]}",
        "html": "<p>When I started this blog, I wanted to use the <a href=\"https://ghost.org/\">Ghost</a> blogging platform. I was looking for a free solution for hosting it, and I didn't mind a little tinkering in order to get it running.<br>\nAt the time, the best approach I could find was hosting it on an Amazon EC2 instance, since Amazon offered a Free Tier, with which you could run a small Linux instance for free, for the duration of one year.</p>\n<p>The one year duration of the Amazon Free Tier is coming to an end, so a couple of weeks ago I started to look for an alternative to host my blog. I found the <a href=\"https://www.openshift.com/\">OpenShift</a> platform by Red Hat, where you can run a small machine instance for free, which seemed like the perfect alternative to run my blog.</p>\n<p>The only problem was that on Amazon I was using the file-based SQLite approach to store my blog data, whereas on OpenShift the recommended method either MySql or Postgre. I decided to go with Postgre, so I needed to migrate all my blog data there from SQLite.</p>\n<h2 id=\"gettingstarted\">Getting started</h2>\n<p>Creating a new Ghost blog on OpenShift is very simple, you can follow the guide on <a href=\"https://www.howtoinstallghost.com/how-to-install-ghost-on-openshift/\">howtoinstallghost</a>.</p>\n<p>After you created the new blog, you need to clone its git repository, copy all your static content, namely the <code>content/apps</code> and <code>content/themes</code> directories overwriting the existing content, and push it to the remote.</p>\n<p>The other, much bigger task is to migrate the actual data from SQLite to the Postgre database.</p>\n<h2 id=\"connectingtothepostgredbrunningonopenshift\">Connecting to the Postgre DB running on OpenShift</h2>\n<p>Connecting to your new Postgre DB is simple, but it needs an extra step. Because of security reasons, you need a client side tool, which will open a secure tunnel between your computer and the DB server.</p>\n<p>First you have to install <code>rhc</code> by following the <a href=\"https://developers.openshift.com/en/getting-started-windows.html#client-tools\">official guide</a>.</p>\n<p>After this, you have to initiate port-forwarding by executing the following command:</p>\n<pre><code class=\"language-bash\">rhc port-forward -a your-app-name\n</code></pre>\n<p>In the output of the command you'll find the port on localhost to which you have to connect to reach your Postgre DB.</p>\n<p><img src=\"/content/images/2016/04/rhc-port-forward.png\" alt=\"Use the rhc command to open a port to your Postgre DB.\"></p>\n<p>So I needed to connect to <code>localhost:62696</code>.<br>\nFor actually connecting to the DB, I was using the official <a href=\"http://www.pgadmin.org/\">pgAdmin</a> client, but probably you can do the same thing with the command-line Postgre client too.</p>\n<h2 id=\"clearingthedb\">Clearing the DB</h2>\n<p>We want to migrate all our data and settings from the existing DB, however, the OpenShift Postgre DB contains some sample data by default. That data needs to be erased.<br>\nWith PostgreSQL, you can erase all data from a table using the <code>TRUNCATE TABLE</code> command. Furthermore, you can find a helpful little procedure in this <a href=\"https://stackoverflow.com/questions/2829158/truncating-all-tables-in-a-postgres-database\">SO answer</a>, with which you can truncate all your tables in one step. (But if you just manually clear all your tables, that works too.)</p>\n<h2 id=\"migratesqlitedata\">Migrate Sqlite data</h2>\n<p>With Sqlite3 it's easy to create a SQL script that creates an exact copy of our database and fills all the existing data into it. We can do this with the following command:</p>\n<pre><code class=\"language-bash\">sqlite3 ghost.db .dump | grep -v &quot;^CREATE&quot; | grep -v &quot;PRAGMA&quot; | grep -v &quot;sqlite_sequence&quot; &gt; blog.dump.sql\n</code></pre>\n<p>With piping a couple of greps together we remove all the lines we don't want to use with Postgre. (I've found this command on <a href=\"https://andrade.io/ghost-blog-migrate-from-sqlite-to-postgres/\">Leonardo Andrade's blog</a>.]</p>\n<p>In order to be able to run this script on my Postgre DB, I needed to do a couple of tweaks on the script. Luckily the whole script is enclosed in one transaction, so you're free to try to run it again and again, it'll only insert anything into your DB after you successfully fixed all problems in the script.</p>\n<p>If you end up in a situation where you accidentally inserted something into your DB anyway, simply purge all records again and start over.</p>\n<h2 id=\"fixerrorsinthesqlitedumpscript\">Fix errors in the Sqlite dump script</h2>\n<p>There are a couple of incompatibilities between Sqlite and Postgre that we have to address if we want to successfully execute this script against our Postgre DB.<br>\nI used a couple of very simple regexp replacements to fix all the different errors I encountered. You can use any text editor that supports regexp replacements, such as <a href=\"https://www.sublimetext.com/\">Sublime Text</a>.</p>\n<h3 id=\"fixbooleanliterals\">Fix boolean literals</h3>\n<p>Sqlite <a href=\"https://www.sqlite.org/datatype3.html\">does not have a separate boolean data type</a>, rather the booleans are represented with integers having either <code>0</code> or <code>1</code> as their value. In the Postgre schema, the same columns are represented with booleans, and the literals <code>0</code> and <code>1</code> are not acceptable as boolean values, so our script will report an error everywhere booleans are used. The solution is to replace all <code>0</code> values with <code>FALSE</code>, and all <code>1</code>s with <code>TRUE</code>.<br>\nThis might be tricky to do for all different occurences, so I did it separately for all boolean columns. It's easy to do a replacement in a text editor based on the surrounding content of the literal. For example this was the first error I got, where the <code>0</code> value after <code>NULL</code> was invalid:</p>\n<pre><code class=\"language-sql\">&lt;p&gt;That should be enough to get you started. Have fun - and let us know what you think :)&lt;/p&gt;',NULL,0,0,'draft','en_US',NULL,NULL,1,1429738370592,1,1430050851451,1,1429738370623,1);\n                                                                                                    ^\n</code></pre>\n<p>So I could fix all occurrences of this column by replacing</p>\n<pre><code class=\"language-sql\">',NULL,0,(\\d),'\n</code></pre>\n<p>with</p>\n<pre><code class=\"language-sql\">',NULL,FALSE,$1,'\n</code></pre>\n<p>Then I repeated the replacement from <code>',NULL,1,(\\d),'</code> to <code>',NULL,TUE,$1,'</code>, so we take care of true values as well.</p>\n<p>The second error was in the same insert statement, with the boolean value just after the one we fixed. This can be adjusted with a simple replacement, we have to replace</p>\n<pre><code class=\"language-sql\">',NULL,(FALSE|TRUE),0,'\n</code></pre>\n<p>with</p>\n<pre><code class=\"language-sql\">',NULL,$1,FALSE,'\n</code></pre>\n<p>Then I repeated this one too for true values, by replacing <code>',NULL,(FALSE|TRUE),1,'</code> with <code>',NULL,$1,TRUE,'</code>.</p>\n<p>Another occurrence was in the <code>tags</code> table, I had to replace <code>',NULL,NULL,0,NULL,NULL,</code> with <code>',NULL,NULL,FALSE,NULL,NULL,</code>, and similarly for <code>TRUE</code>.</p>\n<h3 id=\"fixtimestamps\">Fix timestamps</h3>\n<p>The next group of errors was caused by that in the Sqlite dump script timestamps are represented as bigints (Unix timestamps), which you need to convert to Postgre timestamps.<br>\nConverting a Unix timestamp to a Postgre one is simple, you can use the following expression: <code>TO_CHAR(TO_TIMESTAMP(bigint_field / 1000), 'DD/MM/YYYY HH24:MI:SS')</code></p>\n<p>All occurrences can be fixed with a general replacement from</p>\n<pre><code class=\"language-sql\">,(\\d{13}),\n</code></pre>\n<p>to</p>\n<pre><code class=\"language-sql\">,TO_TIMESTAMP($1/1000),\n</code></pre>\n<p>On the other hand, running a general replacement like this is dangerous, because if you have the same content in any of your posts, that will be replaced as well. If you want to avoid that, you should use a couple of more granular replacements that consider the surrounding content of the script.</p>\n<h3 id=\"differenceintheuserstable\">Difference in the users table</h3>\n<p>This problem might have occurred to me only because I was using a different version of Ghost: in the <code>INSERT</code> statement for the <code>users</code> table the number of values was one less values than the number of columns, the column <code>tour_text</code> was missing.<br>\nSo at the end of those lines, I had to replace</p>\n<pre><code class=\"language-sql\">NULL,NULL,TO_TIMESTAMP(1456936064255/1000),TO_TIMESTAMP(1429738371778/1000),1,TO_TIMESTAMP(1456936064255/1000),1);\n</code></pre>\n<p>with</p>\n<pre><code class=\"language-sql\">NULL,NULL,NULL,TO_TIMESTAMP(1456936064255/1000),TO_TIMESTAMP(1429738371778/1000),1,TO_TIMESTAMP(1456936064255/1000),1);\n</code></pre>\n<h3 id=\"missingcolumnsintheclientstable\">Missing columns in the clients table</h3>\n<p>A couple of columns were missing from <code>clients</code>, I looked up their default value in a newly created DB, and based on that I had to replace</p>\n<pre><code class=\"language-sql\">INSERT INTO &quot;clients&quot; VALUES(1,'6d74790e-d879-484f-85bb-d1b608e46f52','Ghost Admin','ghost-admin','not_available',TO_TIMESTAMP(1429738370626/1000),1,TO_TIMESTAMP(1429738370626/1000),1);\n</code></pre>\n<p>with</p>\n<pre><code class=\"language-sql\">INSERT INTO &quot;clients&quot; VALUES(1,'6d74790e-d879-484f-85bb-d1b608e46f52','Ghost Admin','ghost-admin','','not_available','','enabled','ua','description',TO_TIMESTAMP(1429738370626/1000),1,TO_TIMESTAMP(1429738370626/1000),1);\n</code></pre>\n<p>After all these tweaks, I was able to successfully run the script to load all the data into the DB.</p>\n<h2 id=\"fixsequences\">Fix sequences</h2>\n<p>The last problem was that by purging all the data from the tables and inserting with explicit primary key values we messed up the current values of all our sequences, and insert statements would fail. This can be easily fixed by run the following commands.</p>\n<pre><code class=\"language-sql\">SELECT setval('accesstokens_id_seq', (SELECT MAX(id) FROM accesstokens)+1);\nSELECT setval('app_fields_id_seq', (SELECT MAX(id) FROM app_fields)+1);\nSELECT setval('app_settings_id_seq', (SELECT MAX(id) FROM app_settings)+1);\nSELECT setval('apps_id_seq', (SELECT MAX(id) FROM apps)+1);\nSELECT setval('client_trusted_domains_id_seq', (SELECT MAX(id) FROM client_trusted_domains)+1);\nSELECT setval('clients_id_seq', (SELECT MAX(id) FROM clients)+1);\nSELECT setval('permissions_apps_id_seq', (SELECT MAX(id) FROM permissions_apps)+1);\nSELECT setval('permissions_id_seq', (SELECT MAX(id) FROM permissions)+1);\nSELECT setval('permissions_roles_id_seq', (SELECT MAX(id) FROM permissions_roles)+1);\nSELECT setval('permissions_users_id_seq', (SELECT MAX(id) FROM permissions_users)+1);\nSELECT setval('posts_id_seq', (SELECT MAX(id) FROM posts)+1);\nSELECT setval('posts_tags_id_seq', (SELECT MAX(id) FROM posts_tags)+1);\nSELECT setval('refreshtokens_id_seq', (SELECT MAX(id) FROM refreshtokens)+1);\nSELECT setval('roles_id_seq', (SELECT MAX(id) FROM roles)+1);\nSELECT setval('roles_users_id_seq', (SELECT MAX(id) FROM roles_users)+1);\nSELECT setval('settings_id_seq', (SELECT MAX(id) FROM settings)+1);\nSELECT setval('tags_id_seq', (SELECT MAX(id) FROM tags)+1);\nSELECT setval('users_id_seq', (SELECT MAX(id) FROM users)+1);\n</code></pre>\n<p>After this last fix, your new Ghost blog on PostgreSQL should be up and running!</p>\n",
        "comment_id": "18",
        "plaintext": "When I started this blog, I wanted to use the Ghost [https://ghost.org/] \nblogging platform. I was looking for a free solution for hosting it, and I\ndidn't mind a little tinkering in order to get it running.\nAt the time, the best approach I could find was hosting it on an Amazon EC2\ninstance, since Amazon offered a Free Tier, with which you could run a small\nLinux instance for free, for the duration of one year.\n\nThe one year duration of the Amazon Free Tier is coming to an end, so a couple\nof weeks ago I started to look for an alternative to host my blog. I found the \nOpenShift [https://www.openshift.com/]  platform by Red Hat, where you can run a\nsmall machine instance for free, which seemed like the perfect alternative to\nrun my blog.\n\nThe only problem was that on Amazon I was using the file-based SQLite approach\nto store my blog data, whereas on OpenShift the recommended method either MySql\nor Postgre. I decided to go with Postgre, so I needed to migrate all my blog\ndata there from SQLite.\n\nGetting started\nCreating a new Ghost blog on OpenShift is very simple, you can follow the guide\non howtoinstallghost\n[https://www.howtoinstallghost.com/how-to-install-ghost-on-openshift/].\n\nAfter you created the new blog, you need to clone its git repository, copy all\nyour static content, namely the content/apps  and content/themes  directories\noverwriting the existing content, and push it to the remote.\n\nThe other, much bigger task is to migrate the actual data from SQLite to the\nPostgre database.\n\nConnecting to the Postgre DB running on OpenShift\nConnecting to your new Postgre DB is simple, but it needs an extra step. Because\nof security reasons, you need a client side tool, which will open a secure\ntunnel between your computer and the DB server.\n\nFirst you have to install rhc  by following the official guide.\n\nAfter this, you have to initiate port-forwarding by executing the following\ncommand:\n\nrhc port-forward -a your-app-name\n\n\nIn the output of the command you'll find the port on localhost to which you have\nto connect to reach your Postgre DB.\n\n\n\nSo I needed to connect to localhost:62696.\nFor actually connecting to the DB, I was using the official pgAdmin\n[http://www.pgadmin.org/]  client, but probably you can do the same thing with\nthe command-line Postgre client too.\n\nClearing the DB\nWe want to migrate all our data and settings from the existing DB, however, the\nOpenShift Postgre DB contains some sample data by default. That data needs to be\nerased.\nWith PostgreSQL, you can erase all data from a table using the TRUNCATE TABLE \ncommand. Furthermore, you can find a helpful little procedure in this SO answer\n[https://stackoverflow.com/questions/2829158/truncating-all-tables-in-a-postgres-database]\n, with which you can truncate all your tables in one step. (But if you just\nmanually clear all your tables, that works too.)\n\nMigrate Sqlite data\nWith Sqlite3 it's easy to create a SQL script that creates an exact copy of our\ndatabase and fills all the existing data into it. We can do this with the\nfollowing command:\n\nsqlite3 ghost.db .dump | grep -v \"^CREATE\" | grep -v \"PRAGMA\" | grep -v \"sqlite_sequence\" > blog.dump.sql\n\n\nWith piping a couple of greps together we remove all the lines we don't want to\nuse with Postgre. (I've found this command on Leonardo Andrade's blog\n[https://andrade.io/ghost-blog-migrate-from-sqlite-to-postgres/].]\n\nIn order to be able to run this script on my Postgre DB, I needed to do a couple\nof tweaks on the script. Luckily the whole script is enclosed in one\ntransaction, so you're free to try to run it again and again, it'll only insert\nanything into your DB after you successfully fixed all problems in the script.\n\nIf you end up in a situation where you accidentally inserted something into your\nDB anyway, simply purge all records again and start over.\n\nFix errors in the Sqlite dump script\nThere are a couple of incompatibilities between Sqlite and Postgre that we have\nto address if we want to successfully execute this script against our Postgre\nDB.\nI used a couple of very simple regexp replacements to fix all the different\nerrors I encountered. You can use any text editor that supports regexp\nreplacements, such as Sublime Text [https://www.sublimetext.com/].\n\nFix boolean literals\nSqlite does not have a separate boolean data type\n[https://www.sqlite.org/datatype3.html], rather the booleans are represented\nwith integers having either 0  or 1  as their value. In the Postgre schema, the\nsame columns are represented with booleans, and the literals 0  and 1  are not\nacceptable as boolean values, so our script will report an error everywhere\nbooleans are used. The solution is to replace all 0  values with FALSE, and all \n1s with TRUE.\nThis might be tricky to do for all different occurences, so I did it separately\nfor all boolean columns. It's easy to do a replacement in a text editor based on\nthe surrounding content of the literal. For example this was the first error I\ngot, where the 0  value after NULL  was invalid:\n\n<p>That should be enough to get you started. Have fun - and let us know what you think :)</p>',NULL,0,0,'draft','en_US',NULL,NULL,1,1429738370592,1,1430050851451,1,1429738370623,1);\n                                                                                                    ^\n\n\nSo I could fix all occurrences of this column by replacing\n\n',NULL,0,(\\d),'\n\n\nwith\n\n',NULL,FALSE,$1,'\n\n\nThen I repeated the replacement from ',NULL,1,(\\d),'  to ',NULL,TUE,$1,', so we\ntake care of true values as well.\n\nThe second error was in the same insert statement, with the boolean value just\nafter the one we fixed. This can be adjusted with a simple replacement, we have\nto replace\n\n',NULL,(FALSE|TRUE),0,'\n\n\nwith\n\n',NULL,$1,FALSE,'\n\n\nThen I repeated this one too for true values, by replacing \n',NULL,(FALSE|TRUE),1,'  with ',NULL,$1,TRUE,'.\n\nAnother occurrence was in the tags  table, I had to replace \n',NULL,NULL,0,NULL,NULL,  with ',NULL,NULL,FALSE,NULL,NULL,, and similarly for \nTRUE.\n\nFix timestamps\nThe next group of errors was caused by that in the Sqlite dump script timestamps\nare represented as bigints (Unix timestamps), which you need to convert to\nPostgre timestamps.\nConverting a Unix timestamp to a Postgre one is simple, you can use the\nfollowing expression: TO_CHAR(TO_TIMESTAMP(bigint_field / 1000), 'DD/MM/YYYY\nHH24:MI:SS')\n\nAll occurrences can be fixed with a general replacement from\n\n,(\\d{13}),\n\n\nto\n\n,TO_TIMESTAMP($1/1000),\n\n\nOn the other hand, running a general replacement like this is dangerous, because\nif you have the same content in any of your posts, that will be replaced as\nwell. If you want to avoid that, you should use a couple of more granular\nreplacements that consider the surrounding content of the script.\n\nDifference in the users table\nThis problem might have occurred to me only because I was using a different\nversion of Ghost: in the INSERT  statement for the users  table the number of\nvalues was one less values than the number of columns, the column tour_text  was\nmissing.\nSo at the end of those lines, I had to replace\n\nNULL,NULL,TO_TIMESTAMP(1456936064255/1000),TO_TIMESTAMP(1429738371778/1000),1,TO_TIMESTAMP(1456936064255/1000),1);\n\n\nwith\n\nNULL,NULL,NULL,TO_TIMESTAMP(1456936064255/1000),TO_TIMESTAMP(1429738371778/1000),1,TO_TIMESTAMP(1456936064255/1000),1);\n\n\nMissing columns in the clients table\nA couple of columns were missing from clients, I looked up their default value\nin a newly created DB, and based on that I had to replace\n\nINSERT INTO \"clients\" VALUES(1,'6d74790e-d879-484f-85bb-d1b608e46f52','Ghost Admin','ghost-admin','not_available',TO_TIMESTAMP(1429738370626/1000),1,TO_TIMESTAMP(1429738370626/1000),1);\n\n\nwith\n\nINSERT INTO \"clients\" VALUES(1,'6d74790e-d879-484f-85bb-d1b608e46f52','Ghost Admin','ghost-admin','','not_available','','enabled','ua','description',TO_TIMESTAMP(1429738370626/1000),1,TO_TIMESTAMP(1429738370626/1000),1);\n\n\nAfter all these tweaks, I was able to successfully run the script to load all\nthe data into the DB.\n\nFix sequences\nThe last problem was that by purging all the data from the tables and inserting\nwith explicit primary key values we messed up the current values of all our\nsequences, and insert statements would fail. This can be easily fixed by run the\nfollowing commands.\n\nSELECT setval('accesstokens_id_seq', (SELECT MAX(id) FROM accesstokens)+1);\nSELECT setval('app_fields_id_seq', (SELECT MAX(id) FROM app_fields)+1);\nSELECT setval('app_settings_id_seq', (SELECT MAX(id) FROM app_settings)+1);\nSELECT setval('apps_id_seq', (SELECT MAX(id) FROM apps)+1);\nSELECT setval('client_trusted_domains_id_seq', (SELECT MAX(id) FROM client_trusted_domains)+1);\nSELECT setval('clients_id_seq', (SELECT MAX(id) FROM clients)+1);\nSELECT setval('permissions_apps_id_seq', (SELECT MAX(id) FROM permissions_apps)+1);\nSELECT setval('permissions_id_seq', (SELECT MAX(id) FROM permissions)+1);\nSELECT setval('permissions_roles_id_seq', (SELECT MAX(id) FROM permissions_roles)+1);\nSELECT setval('permissions_users_id_seq', (SELECT MAX(id) FROM permissions_users)+1);\nSELECT setval('posts_id_seq', (SELECT MAX(id) FROM posts)+1);\nSELECT setval('posts_tags_id_seq', (SELECT MAX(id) FROM posts_tags)+1);\nSELECT setval('refreshtokens_id_seq', (SELECT MAX(id) FROM refreshtokens)+1);\nSELECT setval('roles_id_seq', (SELECT MAX(id) FROM roles)+1);\nSELECT setval('roles_users_id_seq', (SELECT MAX(id) FROM roles_users)+1);\nSELECT setval('settings_id_seq', (SELECT MAX(id) FROM settings)+1);\nSELECT setval('tags_id_seq', (SELECT MAX(id) FROM tags)+1);\nSELECT setval('users_id_seq', (SELECT MAX(id) FROM users)+1);\n\n\nAfter this last fix, your new Ghost blog on PostgreSQL should be up and running!",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "A guide about how to migrate a Ghost blog using SQLite to PostgreSQL running on Openshift.",
        "author_id": "1",
        "created_at": "2016-04-10 16:36:15",
        "created_by": "1",
        "updated_at": "2016-04-23 18:43:29",
        "updated_by": "1",
        "published_at": "2016-04-10 16:48:05",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de296b",
        "uuid": "61290b8d-0297-4985-b813-d38e81758da7",
        "title": "Stubbing service dependencies in .NET using Stubbery",
        "slug": "stubbing-service-dependencies-in-net-using-stubbery",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"# Introduction\\n\\nWhen writing integration tests for a service (especially if we are running a long, end-to-end test), it often causes a problem that the external dependencies of our service fail.\\n\\nLet's say we have a service handling customer payments and the actual payments are handled by an external provider.\\n\\n![Diagram illustrating integration testing an api with an external dependency.](/content/images/2016/06/payment-integration-test-1.png)\\n\\nVery often, we cannot use the actual production system of our external dependency. For example, when we want to test payments, we cannot make real purchases every time we run our integration test suite.\\n\\nThus, we resort to using the test system of our dependency (if there is one).  \\nThe problem with this is that the test system of any provider is usually not 100% reliable. There will be occasional downtimes, performance problems, or other problems due to configuration changes or general tinkering.\\n\\nBecause of this, from time to time our tests will fail due to the failure of the dependency.\\nThis means that if we have a substantial number of tests in our suite, one or two of them will fail in every run, which will cause our test suite to be always red.\\n\\nWhen I first faced this problem I was surprised to see there was no .NET library for programmatically creating Api stubs, so I decided to  create a simple library for this purpose.\\n\\n# Stubbery\\n\\n[Stubbery](http://markvincze.github.io/Stubbery/) is a library with which we can simply configure and start a web server that responds on particular routes with the configured results. It supports .NET Core and the full .NET Framework up from .NET 4.5.1.\\n\\n## How to use\\n\\nThe central class of the library is `ApiStub`. In order to start a new server we have to create an instance of `ApiStub`, set up some routes using the methods `Get`, `Post`, `Put` and `Delete`, and start the server by calling `Start`.\\n\\nThe server listens on localhost on a randomly picked free port. The full address is returned by the `Address` property.\\n\\nAfter usage the server should be stopped to free up the TCP port. This can be done by calling `Dispose` (or using the stub in a `using` block).\\n\\n### Basic usage\\n\\n```csharp\\nusing (var stub = new ApiStub())\\n{\\n    stub.Get(\\n        \\\"/testget\\\",\\n        (req, args) => \\\"testresponse\\\");\\n\\n    stub.Start();\\n\\n    var result = await httpClient.GetAsync(new UriBuilder(new Uri(stub.Address)) { Path = \\\"/testget\\\" }.Uri);\\n\\n    // resultString will contain \\\"testresponse\\\"\\n    var resultString = await result.Content.ReadAsStringAsync();\\n}\\n```\\n\\nMore examples can be found on the [GitHub page](https://github.com/markvincze/Stubbery).\\n\\n# End to end example\\n\\n## The dependency\\n\\nLet's say we have an endpoint in our Api that depends on getting the name of an UK county based on a postcode, for which we are using the [Postcodes.io](http://postcodes.io/) api.\\n\\nThe endpoint I'm interested in returns the details of a certain postcode. Given the request\\n\\n```\\nGET http://api.postcodes.io/postcodes/OX49%205NU\\n```\\n\\nit returns the details in the form:\\n\\n```javascript\\n{\\n    \\\"status\\\": 200,\\n    \\\"result\\\": {\\n        \\\"postcode\\\": \\\"OX49 5NU\\\",\\n        \\\"admin_county\\\": \\\"Oxfordshire\\\"\\n        ...\\n    }\\n}\\n```\\n\\nThis is the dependency we'd like to replace in our test with a stub.\\n\\n## The api under test\\n\\nOur own Api is an ASP.NET Core application, which implements an endpoint that returns the name of the county. Given the request\\n\\n```\\nGET http://our-own-api/countyname/OX49%205NU\\n```\\n\\nit returns the name of the county as a raw string:\\n\\n```\\nOxfordshire\\n```\\n\\nInternally, our API is calling out to `postcodes.io` to get the details of the county. If our application is calling an external dependency, it's introducing the following problems if we want to implement integration tests.\\n\\n - We have no control over the external API, it might be down any time due to an outage or maintanance, which would break our integration tests.\\n - The exact data returned by our dependency might change over time. In the case of `postcodes.io`, this change will be infrequent, only when the set of UK postcodes change. On the other hand, with other APIs (imagine using a weather data provider), the dataset returned will be different every day.  \\nThis will make it difficult to implement reliable assertions in our tests, since the data we're expecting can change any time.\\n - Since we're using a real API, it's going to be difficult to test non-happy flows. Maybe we want to test what happens if the dependency is returning a `404`, or a `500`, or if it doesn't respond at all. Or we might want to test what happens if it works, but it responds very slowly. If our API is calling directly the real dependency, we have no way to emulate these scenarios.\\n\\nThese problem can all be solved by replacing the dependecy during our integration test with a stub.\\n\\n## Starting the stub\\n\\nWe have to add reference to the [Stubbery library](https://www.nuget.org/packages/Stubbery/).\\n\\nThe stub for the post code api can be created and set up in constructor of the unit test class. I'll configure a response for the case when the postcode is found, one for when it's not found, and one for when some other error happens. (An additional benefit of using a stub is to be able to imitate responses which cannot be reproduced with the real system.)\\n\\n```csharp\\nprivate ApiStub StartStub()\\n{\\n    var postcodeApiStub = new ApiStub();\\n\\n    postcodeApiStub.Get(\\n        \\\"/postcodes/{postcode}\\\",\\n        (request, args) =>\\n        {\\n            if (args.Route.postcode == \\\"postcodeOk\\\")\\n            {\\n                return \\\"{ \\\\\\\"status\\\\\\\": 200, \\\\\\\"result\\\\\\\": { \\\\\\\"admin_county\\\\\\\": \\\\\\\"CountyName\\\\\\\" } }\\\";\\n            }\\n\\n            if (args.Route.postcode == \\\"postcodeNotFound\\\")\\n            {\\n                return \\\"{ \\\\\\\"status\\\\\\\": 404 }\\\";\\n            }\\n\\n            return \\\"{ \\\\\\\"status\\\\\\\": 500 }\\\";\\n        });\\n\\n    postcodeApiStub.Start();\\n\\n    return postcodeApiStub;\\n}\\n```\\n\\n(The `postcodes.io` API returns the status in a field in the body, and we're depending on that in our implementation, so that's what we configure on the stub.  \\nThe API is also returning proper status codes. If we depended on those, we could set up the stub to respond with the specific status codes by calling the `StatusCode` method.)\\n\\n## Starting the api under test\\n\\nThen we start our web application for testing (there is a little more ceremony to it than this, the details can be found in [the documentation](https://docs.asp.net/en/latest/testing/integration-testing.html), or the [sample project](https://github.com/markvincze/Stubbery/tree/master/samples/Stubbery.Samples.BasicSample)).\\n\\n```csharp\\nprivate TestServer StartApiUnderTest(ApiStub postcodeApiStub)\\n{\\n    var server = new TestServer(\\n        WebHost.CreateDefaultBuilder()\\n            .UseStartup<Startup>()\\n            .ConfigureAppConfiguration((ctx, b) =>\\n            {\\n                b.Add(new MemoryConfigurationSource\\n                {\\n                    InitialData = new Dictionary<string, string>\\n                    {\\n                        // Replace the real api URL with the stub.\\n                        [\\\"PostCodeApiUrl\\\"] = postcodeApiStub.Address\\n                    }\\n                });\\n            }));\\n\\n    return server;\\n}\\n```\\n\\n## Implement the test\\n\\nFor implementing the test, I'll use xUnit as the test runner.  \\nWe can implement the actual test and verify that we're getting the responses that we expect.\\n\\n```csharp\\n[Fact]\\npublic async Task GetCountyName_CountyFound_CountyNameReturned()\\n{\\n    using (var stub = StartStub())\\n    using (var server = StartApiUnderTest(stub))\\n    using (var client = server.CreateClient())\\n    {\\n        var response = await client.GetAsync(\\\"/countyname/postcodeOk\\\");\\n\\n        var responseString = await response.Content.ReadAsStringAsync();\\n\\n        Assert.Equal(\\\"CountyName\\\", responseString);\\n    }\\n}\\n```\\n\\nThe complete example can be found in the [sample project](https://github.com/markvincze/Stubbery/tree/master/samples/Stubbery.Samples.BasicSample).\\n\\n# Conclusion\\n\\nThanks to the changes in how hosting works in ASP.NET Core, writing integration tests became much easier.  \\nExternal dependencies can make our integration tests brittle, which can be mitigated by replacing them with a hardwired, reliable stubwhich is easy to do with the pluggable proramming model of ASP.NET Core.\\n\\nAnd the [Stubbery](https://markvincze.github.io/Stubbery/) library makes it more convenient to set up and start such stubs for HTTP APIs.\"}]],\"sections\":[[10,0]]}",
        "html": "<h1 id=\"introduction\">Introduction</h1>\n<p>When writing integration tests for a service (especially if we are running a long, end-to-end test), it often causes a problem that the external dependencies of our service fail.</p>\n<p>Let's say we have a service handling customer payments and the actual payments are handled by an external provider.</p>\n<p><img src=\"/content/images/2016/06/payment-integration-test-1.png\" alt=\"Diagram illustrating integration testing an api with an external dependency.\"></p>\n<p>Very often, we cannot use the actual production system of our external dependency. For example, when we want to test payments, we cannot make real purchases every time we run our integration test suite.</p>\n<p>Thus, we resort to using the test system of our dependency (if there is one).<br>\nThe problem with this is that the test system of any provider is usually not 100% reliable. There will be occasional downtimes, performance problems, or other problems due to configuration changes or general tinkering.</p>\n<p>Because of this, from time to time our tests will fail due to the failure of the dependency.<br>\nThis means that if we have a substantial number of tests in our suite, one or two of them will fail in every run, which will cause our test suite to be always red.</p>\n<p>When I first faced this problem I was surprised to see there was no .NET library for programmatically creating Api stubs, so I decided to  create a simple library for this purpose.</p>\n<h1 id=\"stubbery\">Stubbery</h1>\n<p><a href=\"http://markvincze.github.io/Stubbery/\">Stubbery</a> is a library with which we can simply configure and start a web server that responds on particular routes with the configured results. It supports .NET Core and the full .NET Framework up from .NET 4.5.1.</p>\n<h2 id=\"howtouse\">How to use</h2>\n<p>The central class of the library is <code>ApiStub</code>. In order to start a new server we have to create an instance of <code>ApiStub</code>, set up some routes using the methods <code>Get</code>, <code>Post</code>, <code>Put</code> and <code>Delete</code>, and start the server by calling <code>Start</code>.</p>\n<p>The server listens on localhost on a randomly picked free port. The full address is returned by the <code>Address</code> property.</p>\n<p>After usage the server should be stopped to free up the TCP port. This can be done by calling <code>Dispose</code> (or using the stub in a <code>using</code> block).</p>\n<h3 id=\"basicusage\">Basic usage</h3>\n<pre><code class=\"language-csharp\">using (var stub = new ApiStub())\n{\n    stub.Get(\n        &quot;/testget&quot;,\n        (req, args) =&gt; &quot;testresponse&quot;);\n\n    stub.Start();\n\n    var result = await httpClient.GetAsync(new UriBuilder(new Uri(stub.Address)) { Path = &quot;/testget&quot; }.Uri);\n\n    // resultString will contain &quot;testresponse&quot;\n    var resultString = await result.Content.ReadAsStringAsync();\n}\n</code></pre>\n<p>More examples can be found on the <a href=\"https://github.com/markvincze/Stubbery\">GitHub page</a>.</p>\n<h1 id=\"endtoendexample\">End to end example</h1>\n<h2 id=\"thedependency\">The dependency</h2>\n<p>Let's say we have an endpoint in our Api that depends on getting the name of an UK county based on a postcode, for which we are using the <a href=\"http://postcodes.io/\">Postcodes.io</a> api.</p>\n<p>The endpoint I'm interested in returns the details of a certain postcode. Given the request</p>\n<pre><code>GET http://api.postcodes.io/postcodes/OX49%205NU\n</code></pre>\n<p>it returns the details in the form:</p>\n<pre><code class=\"language-javascript\">{\n    &quot;status&quot;: 200,\n    &quot;result&quot;: {\n        &quot;postcode&quot;: &quot;OX49 5NU&quot;,\n        &quot;admin_county&quot;: &quot;Oxfordshire&quot;\n        ...\n    }\n}\n</code></pre>\n<p>This is the dependency we'd like to replace in our test with a stub.</p>\n<h2 id=\"theapiundertest\">The api under test</h2>\n<p>Our own Api is an ASP.NET Core application, which implements an endpoint that returns the name of the county. Given the request</p>\n<pre><code>GET http://our-own-api/countyname/OX49%205NU\n</code></pre>\n<p>it returns the name of the county as a raw string:</p>\n<pre><code>Oxfordshire\n</code></pre>\n<p>Internally, our API is calling out to <code>postcodes.io</code> to get the details of the county. If our application is calling an external dependency, it's introducing the following problems if we want to implement integration tests.</p>\n<ul>\n<li>We have no control over the external API, it might be down any time due to an outage or maintanance, which would break our integration tests.</li>\n<li>The exact data returned by our dependency might change over time. In the case of <code>postcodes.io</code>, this change will be infrequent, only when the set of UK postcodes change. On the other hand, with other APIs (imagine using a weather data provider), the dataset returned will be different every day.<br>\nThis will make it difficult to implement reliable assertions in our tests, since the data we're expecting can change any time.</li>\n<li>Since we're using a real API, it's going to be difficult to test non-happy flows. Maybe we want to test what happens if the dependency is returning a <code>404</code>, or a <code>500</code>, or if it doesn't respond at all. Or we might want to test what happens if it works, but it responds very slowly. If our API is calling directly the real dependency, we have no way to emulate these scenarios.</li>\n</ul>\n<p>These problem can all be solved by replacing the dependecy during our integration test with a stub.</p>\n<h2 id=\"startingthestub\">Starting the stub</h2>\n<p>We have to add reference to the <a href=\"https://www.nuget.org/packages/Stubbery/\">Stubbery library</a>.</p>\n<p>The stub for the post code api can be created and set up in constructor of the unit test class. I'll configure a response for the case when the postcode is found, one for when it's not found, and one for when some other error happens. (An additional benefit of using a stub is to be able to imitate responses which cannot be reproduced with the real system.)</p>\n<pre><code class=\"language-csharp\">private ApiStub StartStub()\n{\n    var postcodeApiStub = new ApiStub();\n\n    postcodeApiStub.Get(\n        &quot;/postcodes/{postcode}&quot;,\n        (request, args) =&gt;\n        {\n            if (args.Route.postcode == &quot;postcodeOk&quot;)\n            {\n                return &quot;{ \\&quot;status\\&quot;: 200, \\&quot;result\\&quot;: { \\&quot;admin_county\\&quot;: \\&quot;CountyName\\&quot; } }&quot;;\n            }\n\n            if (args.Route.postcode == &quot;postcodeNotFound&quot;)\n            {\n                return &quot;{ \\&quot;status\\&quot;: 404 }&quot;;\n            }\n\n            return &quot;{ \\&quot;status\\&quot;: 500 }&quot;;\n        });\n\n    postcodeApiStub.Start();\n\n    return postcodeApiStub;\n}\n</code></pre>\n<p>(The <code>postcodes.io</code> API returns the status in a field in the body, and we're depending on that in our implementation, so that's what we configure on the stub.<br>\nThe API is also returning proper status codes. If we depended on those, we could set up the stub to respond with the specific status codes by calling the <code>StatusCode</code> method.)</p>\n<h2 id=\"startingtheapiundertest\">Starting the api under test</h2>\n<p>Then we start our web application for testing (there is a little more ceremony to it than this, the details can be found in <a href=\"https://docs.asp.net/en/latest/testing/integration-testing.html\">the documentation</a>, or the <a href=\"https://github.com/markvincze/Stubbery/tree/master/samples/Stubbery.Samples.BasicSample\">sample project</a>).</p>\n<pre><code class=\"language-csharp\">private TestServer StartApiUnderTest(ApiStub postcodeApiStub)\n{\n    var server = new TestServer(\n        WebHost.CreateDefaultBuilder()\n            .UseStartup&lt;Startup&gt;()\n            .ConfigureAppConfiguration((ctx, b) =&gt;\n            {\n                b.Add(new MemoryConfigurationSource\n                {\n                    InitialData = new Dictionary&lt;string, string&gt;\n                    {\n                        // Replace the real api URL with the stub.\n                        [&quot;PostCodeApiUrl&quot;] = postcodeApiStub.Address\n                    }\n                });\n            }));\n\n    return server;\n}\n</code></pre>\n<h2 id=\"implementthetest\">Implement the test</h2>\n<p>For implementing the test, I'll use xUnit as the test runner.<br>\nWe can implement the actual test and verify that we're getting the responses that we expect.</p>\n<pre><code class=\"language-csharp\">[Fact]\npublic async Task GetCountyName_CountyFound_CountyNameReturned()\n{\n    using (var stub = StartStub())\n    using (var server = StartApiUnderTest(stub))\n    using (var client = server.CreateClient())\n    {\n        var response = await client.GetAsync(&quot;/countyname/postcodeOk&quot;);\n\n        var responseString = await response.Content.ReadAsStringAsync();\n\n        Assert.Equal(&quot;CountyName&quot;, responseString);\n    }\n}\n</code></pre>\n<p>The complete example can be found in the <a href=\"https://github.com/markvincze/Stubbery/tree/master/samples/Stubbery.Samples.BasicSample\">sample project</a>.</p>\n<h1 id=\"conclusion\">Conclusion</h1>\n<p>Thanks to the changes in how hosting works in ASP.NET Core, writing integration tests became much easier.<br>\nExternal dependencies can make our integration tests brittle, which can be mitigated by replacing them with a hardwired, reliable stubwhich is easy to do with the pluggable proramming model of ASP.NET Core.</p>\n<p>And the <a href=\"https://markvincze.github.io/Stubbery/\">Stubbery</a> library makes it more convenient to set up and start such stubs for HTTP APIs.</p>\n",
        "comment_id": "20",
        "plaintext": "Introduction\nWhen writing integration tests for a service (especially if we are running a\nlong, end-to-end test), it often causes a problem that the external dependencies\nof our service fail.\n\nLet's say we have a service handling customer payments and the actual payments\nare handled by an external provider.\n\n\n\nVery often, we cannot use the actual production system of our external\ndependency. For example, when we want to test payments, we cannot make real\npurchases every time we run our integration test suite.\n\nThus, we resort to using the test system of our dependency (if there is one).\nThe problem with this is that the test system of any provider is usually not\n100% reliable. There will be occasional downtimes, performance problems, or\nother problems due to configuration changes or general tinkering.\n\nBecause of this, from time to time our tests will fail due to the failure of the\ndependency.\nThis means that if we have a substantial number of tests in our suite, one or\ntwo of them will fail in every run, which will cause our test suite to be always\nred.\n\nWhen I first faced this problem I was surprised to see there was no .NET library\nfor programmatically creating Api stubs, so I decided to create a simple library\nfor this purpose.\n\nStubbery\nStubbery [http://markvincze.github.io/Stubbery/]  is a library with which we can\nsimply configure and start a web server that responds on particular routes with\nthe configured results. It supports .NET Core and the full .NET Framework up\nfrom .NET 4.5.1.\n\nHow to use\nThe central class of the library is ApiStub. In order to start a new server we\nhave to create an instance of ApiStub, set up some routes using the methods Get,\n Post, Put  and Delete, and start the server by calling Start.\n\nThe server listens on localhost on a randomly picked free port. The full address\nis returned by the Address  property.\n\nAfter usage the server should be stopped to free up the TCP port. This can be\ndone by calling Dispose  (or using the stub in a using  block).\n\nBasic usage\nusing (var stub = new ApiStub())\n{\n    stub.Get(\n        \"/testget\",\n        (req, args) => \"testresponse\");\n\n    stub.Start();\n\n    var result = await httpClient.GetAsync(new UriBuilder(new Uri(stub.Address)) { Path = \"/testget\" }.Uri);\n\n    // resultString will contain \"testresponse\"\n    var resultString = await result.Content.ReadAsStringAsync();\n}\n\n\nMore examples can be found on the GitHub page\n[https://github.com/markvincze/Stubbery].\n\nEnd to end example\nThe dependency\nLet's say we have an endpoint in our Api that depends on getting the name of an\nUK county based on a postcode, for which we are using the Postcodes.io\n[http://postcodes.io/]  api.\n\nThe endpoint I'm interested in returns the details of a certain postcode. Given\nthe request\n\nGET http://api.postcodes.io/postcodes/OX49%205NU\n\n\nit returns the details in the form:\n\n{\n    \"status\": 200,\n    \"result\": {\n        \"postcode\": \"OX49 5NU\",\n        \"admin_county\": \"Oxfordshire\"\n        ...\n    }\n}\n\n\nThis is the dependency we'd like to replace in our test with a stub.\n\nThe api under test\nOur own Api is an ASP.NET Core application, which implements an endpoint that\nreturns the name of the county. Given the request\n\nGET http://our-own-api/countyname/OX49%205NU\n\n\nit returns the name of the county as a raw string:\n\nOxfordshire\n\n\nInternally, our API is calling out to postcodes.io  to get the details of the\ncounty. If our application is calling an external dependency, it's introducing\nthe following problems if we want to implement integration tests.\n\n * We have no control over the external API, it might be down any time due to an\n   outage or maintanance, which would break our integration tests.\n * The exact data returned by our dependency might change over time. In the case\n   of postcodes.io, this change will be infrequent, only when the set of UK\n   postcodes change. On the other hand, with other APIs (imagine using a weather\n   data provider), the dataset returned will be different every day.\n   This will make it difficult to implement reliable assertions in our tests,\n   since the data we're expecting can change any time.\n * Since we're using a real API, it's going to be difficult to test non-happy\n   flows. Maybe we want to test what happens if the dependency is returning a \n   404, or a 500, or if it doesn't respond at all. Or we might want to test what\n   happens if it works, but it responds very slowly. If our API is calling\n   directly the real dependency, we have no way to emulate these scenarios.\n\nThese problem can all be solved by replacing the dependecy during our\nintegration test with a stub.\n\nStarting the stub\nWe have to add reference to the Stubbery library\n[https://www.nuget.org/packages/Stubbery/].\n\nThe stub for the post code api can be created and set up in constructor of the\nunit test class. I'll configure a response for the case when the postcode is\nfound, one for when it's not found, and one for when some other error happens.\n(An additional benefit of using a stub is to be able to imitate responses which\ncannot be reproduced with the real system.)\n\nprivate ApiStub StartStub()\n{\n    var postcodeApiStub = new ApiStub();\n\n    postcodeApiStub.Get(\n        \"/postcodes/{postcode}\",\n        (request, args) =>\n        {\n            if (args.Route.postcode == \"postcodeOk\")\n            {\n                return \"{ \\\"status\\\": 200, \\\"result\\\": { \\\"admin_county\\\": \\\"CountyName\\\" } }\";\n            }\n\n            if (args.Route.postcode == \"postcodeNotFound\")\n            {\n                return \"{ \\\"status\\\": 404 }\";\n            }\n\n            return \"{ \\\"status\\\": 500 }\";\n        });\n\n    postcodeApiStub.Start();\n\n    return postcodeApiStub;\n}\n\n\n(The postcodes.io  API returns the status in a field in the body, and we're\ndepending on that in our implementation, so that's what we configure on the\nstub.\nThe API is also returning proper status codes. If we depended on those, we could\nset up the stub to respond with the specific status codes by calling the \nStatusCode  method.)\n\nStarting the api under test\nThen we start our web application for testing (there is a little more ceremony\nto it than this, the details can be found in the documentation\n[https://docs.asp.net/en/latest/testing/integration-testing.html], or the \nsample\nproject\n[https://github.com/markvincze/Stubbery/tree/master/samples/Stubbery.Samples.BasicSample]\n).\n\nprivate TestServer StartApiUnderTest(ApiStub postcodeApiStub)\n{\n    var server = new TestServer(\n        WebHost.CreateDefaultBuilder()\n            .UseStartup<Startup>()\n            .ConfigureAppConfiguration((ctx, b) =>\n            {\n                b.Add(new MemoryConfigurationSource\n                {\n                    InitialData = new Dictionary<string, string>\n                    {\n                        // Replace the real api URL with the stub.\n                        [\"PostCodeApiUrl\"] = postcodeApiStub.Address\n                    }\n                });\n            }));\n\n    return server;\n}\n\n\nImplement the test\nFor implementing the test, I'll use xUnit as the test runner.\nWe can implement the actual test and verify that we're getting the responses\nthat we expect.\n\n[Fact]\npublic async Task GetCountyName_CountyFound_CountyNameReturned()\n{\n    using (var stub = StartStub())\n    using (var server = StartApiUnderTest(stub))\n    using (var client = server.CreateClient())\n    {\n        var response = await client.GetAsync(\"/countyname/postcodeOk\");\n\n        var responseString = await response.Content.ReadAsStringAsync();\n\n        Assert.Equal(\"CountyName\", responseString);\n    }\n}\n\n\nThe complete example can be found in the sample project\n[https://github.com/markvincze/Stubbery/tree/master/samples/Stubbery.Samples.BasicSample]\n.\n\nConclusion\nThanks to the changes in how hosting works in ASP.NET Core, writing integration\ntests became much easier.\nExternal dependencies can make our integration tests brittle, which can be\nmitigated by replacing them with a hardwired, reliable stubwhich is easy to do\nwith the pluggable proramming model of ASP.NET Core.\n\nAnd the Stubbery [https://markvincze.github.io/Stubbery/]  library makes it more\nconvenient to set up and start such stubs for HTTP APIs.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "Stubbery is a library for creating and running Api stubs in .NET. The post shows how it can be used to stub service dependencies during integration tests.",
        "author_id": "1",
        "created_at": "2016-06-11 22:36:00",
        "created_by": "1",
        "updated_at": "2017-10-17 08:43:23",
        "updated_by": "1",
        "published_at": "2016-06-12 15:27:28",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de296c",
        "uuid": "184ec2d0-2e69-4e78-a0b8-d22f5b9cf6b9",
        "title": "NDC Oslo 2016 recap",
        "slug": "ndc-oslo-2016-recap",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In the beginning of June I was lucky enough to be able to travel to the [NDC conference in Oslo](http://ndcoslo.com). (Hats off to my employer [Travix](http://travix.com) for providing the budget!)\\n\\nThe NDC has always been one of my favourite conferences besides Build and CppCon (and earlier the PDC and MIX), but so far I have only been following it online by watching the sessions uploaded to [Vimeo](http://www.vimeo.com/ndcconferences). This was the first year I had the chance to actually travel to the conference, and it was a splendid experience.\\n\\nIn this post I'd like to tell my impressions, and share the things I learned and found the most interesting.\\n\\n## Venue, organization, etc.\\n\\nThe conference was held at a central location in Oslo, in a big conference building called Spektrum.The large arena in the middle was used to host the booths of the sponsor companies, serve the food, and to provide a casual space for people to hang out between talks. This was also the venue for the two keynote talks during the conference.\\n\\n![Central venue at NDC Oslo](/content/images/2016/07/ndc-main-venue.jpg)\\n\\nThe food also deserves a mention: there was breakfast in the morning, and then lunch was served continuously throughout the whole day. There were several caterers, each serving one or two dishes, which was really cool, because there were many different dishes you could try out, such as sushi, taco, risotto, etc. (And also, the providers were different caterer companies who wanted to show off their product, so I think they all brought their A-game.)\\nSomeone even put together a poll to decide which provider was the best, it seems that people enjoyed the GitHub-sponsored tacos the most :)\\n\\n[![NDC Oslo food poll](/content/images/2016/07/foodvote.png)](https://poll.lab.io/ndc/results)\\n\\nAnd what else would help more to concentrate on tech talks all day than some good coffee? Just like the food, coffee was also much better than what is typical at conferences. You could always get a nice espress or a latte with properly steamed milk foam. (I haven't seen any of those dreaded coffee creamer thingies).\\n\\nAnd even some of the baristas were [famous tech people](https://robdoescoffee.com/) :).\\n\\n![Rob Ashton serving coffe](/content/images/2016/07/robs-coffee.png)\\n\\nI can sum up the work the NDC guys put into organizing the conference with one word: perfect. Every talk started on time, all the tech worked, I didn't see a single problem with microphones, projectors, etc. The main venue was spacious and well designed, it was never too crowded, even during the busiest periods.\\nThe organizers clearly have a lot of experience in putting a conferenc together, they made a great job.\\n\\n## The talks\\n\\nAlthough it was pretty good to hang out, eat nice food and drink hipster coffee all day, the most important part of the conference for me was the actual talks. They didn't disappoint, every talk I went to was high quality. I learned a lot, and I came home from the conference inspired.\\n\\nIn the rest of this post I'd like to share some interesting details I learned at the talks I attended.\\n\\n### Keynote: Yesterdays Technology is Dead, Todays is on Life Support (Troy Hunt)\\n\\nI'm usually not a big fan of keynotes, they tend to be not very informational, and are often very marketingy, especially when they are tied to a particular company.\\n\\nHowever, I enjoyed this one a lot. [Troy Hunt](https://www.troyhunt.com/) is a Microsoft MVP, consultant and Pluralsight author, mostly focusing on security. He always delivers amusingly entertaining talks, and he's also an exceptionally charismatic presenter.\\nThis talk wasn't an exception, it was very interesting and thoroughly entertaining. I recommend giving it a watch if you want a good laugh and would like to learn some interesting trivia about the past, present and future of computing.\\n\\n### ASP.NET Core 1.0 deep dive (Damian Edwards, David Fowler)\\n\\nI was really looking forward to this talk, since the main technology I use nowadays is .NET Core, so I was really interested in what the PM and the architect on the project will share.\\nThe talk was mostly about interesting details from under the hood of .NET, like where the framework is stored on the machine, how to see where the binaries are stored, and even about how someone could create a custom edition of the .NET framework.\\n\\nThere were also some funky demos, like how ASP.NET Core can be run inside of a WinForms application, which - let's face it - not something you see every day ;).\\n\\nWhile the talk was really entertaining, to me it felt a bit random and underprepared. (They even had to troubleshoot on stage when the demos weren't working at first.)\\n\\nDespite these complaints it was a good talk, and it's always nice to see how enthusiastic these guys are about the platform.\\n\\n### ASP.NET Core Kestrel: Adventures in building a fast web server (Damian Edwards, David Fowler)\\n\\nThe second talk by the guys from the ASP.NET team. This was about interesting tricks about optimizing [Kestrel](https://github.com/aspnet/KestrelHttpServer), the new cross-platform web server specifically developed for .NET Core.\\n\\nFirst they introduced the [TechEmpower Benchmarks](https://www.techempower.com/benchmarks/), which is an independent initiative for comparing the performance of different web frameworks. Although these benchmarks can be somewhat misleading, since low-level, very thin, specialized frameworks can have much higher throughput than all-around, general purpose web frameworks. On the other hand they might be so low-level, that they are not a practical choice for developing complex web applications.\\n\\nSo it might be a bit hard to measure the importance of these measurements, it's still cool to see how certain frameworks compare to each other. Especially since throughput, response time, and the ability to handle many concurrent requests at the same time is getting more and more important, especially with the advent of technolgies like WebSockets.\\n\\nEarly versions of Kestrel didn't perform too well on this Benchmark, but since then many things were optimized and in the next round of the TechEmpower benchmarks .NET Core should land at a much better position.\\n\\nThis was a really interesting talks, we've seen some tricks and techniques which the average developer definitely doesn't see in the everyday work, especially when working with high-level web applications.\\n\\nThere are two tricks I found particularly interesting.\\nIt seems that at high scale, most of the performance problems in .NET are due to object allocations and garbage collection. So one of the things they always kept in mind during the development of Kestrel was keeping the number of objects allocated as little as possible. (This is something that the Stack Overflow guys have also been blogging a lot about.)\\n\\nOne of the cool optimizations is happening when the beginning of HTTP requests are processed, and the web server is extracting the HTTP verb from the request.\\nThe naive solution would be to read in the first couple of characters into a string, and compare it to the expected methods, such as `\\\"GET \\\"`, `\\\"POST \\\"`, etc. They figured out that the longest of these strings we have to take into account is `\\\"OPTIONS \\\"`, which is exactly eight characters, which, in the case of UTF-8 encoding fits exactly into a single long.\\n\\nTo utilize this fact, Kestrel defines a couple of `static readonly long` fields, which hold the values of these expected strings, if we interpret their memory representations as a long value.\\n\\n```csharp\\nprivate readonly static long _httpGetMethodLong = GetAsciiStringAsLong(\\\"GET \\\\0\\\\0\\\\0\\\\0\\\");\\nprivate readonly static long _httpHeadMethodLong = GetAsciiStringAsLong(\\\"HEAD \\\\0\\\\0\\\\0\\\");\\n...\\n```\\n\\nAt the start of processing an HTTP request, the first 8 bytes of the request is read, interpreted as a long, and compared with these predefined values.\\n\\n```csharp\\nlong value = begin.PeekLong();\\n\\nforeach (var x in _knownMethods)\\n{\\n    if ((value & x.Item1) == x.Item2)\\n    {\\n        knownMethod = x.Item3;\\n        return true;\\n    }\\n}\\n\\nreturn false;\\n```\\n\\nThis approach brings two benefits.\\n\\n - We save on object allocation, since we're reading in 8 bytes into an existing buffer, and we don't have to create a `String` object.\\n - The other advantage is that comparing two longs is much faster than comparing two strings.\\n\\nThe other thing that stick with me was a really funky trick. I won't go into the details, but the gist is that in a stream of bytes we want to find the position of the first non-zero byte. The naive solution would be to iterate over the stream one byte at a time and compare them with zero.\\nOn the other hand, the CPU works with longs - 8 bytes - anyway, so in a single operation, we are able to compare not one, but 8 bytes. However, this comparison doesn't give us the *position* of the non-zero value, but it lets us do the following.\\n\\n```csharp\\nreturn (i << 3) +\\n    ((longValue & 0x00000000ffffffff) > 0\\n        ? (longValue & 0x000000000000ffff) > 0\\n            ? (longValue & 0x00000000000000ff) > 0 ? 0 : 1\\n            : (longValue & 0x0000000000ff0000) > 0 ? 2 : 3\\n        : (longValue & 0x0000ffff00000000) > 0\\n            ? (longValue & 0x000000ff00000000) > 0 ? 4 : 5\\n            : (longValue & 0x00ff000000000000) > 0 ? 6 : 7);\\n```\\n\\nWhat the above code is doing is basically a binary search in a bit pattern. When we're comparing our value with `0x00000000ffffffff` we get the information whether there is a non-zero value in the left, or in the right side of the 8 bytes. By doing this we can eliminate half of the 8 bytes in a single operation, so in worts case, we will execute 3 comparisons instead of 8.\\n\\nThis stick with me, because I was surprised the simplicity and elegance of this trick. I was always familiar with how binary search worked and how it achieved logarithmic runtime instead of linear, but I don't know if I could came up with using the same trick not in a sorted collection, but on a bit pattern.\\n\\n### The C++ and CLR Memory Models (Sasha Goldshtein)\\n\\nI went to this talk expecting to learn about what's happening in memory when we're passing objects between the .NET Framework and native Win32 components, for example when using `PInvoke` or C++/CLI.\\n\\nAlthough the talk was really good, its topic was a bit different. It was particularly about the optimizations happening in the compiler and the CPU which can cause our operations to be reordered. It gave clear explanations about the type of bugs this can cause, and what are our options to prevent these, for instance the `volatile` keyword and the synchronization primitives in the C++ STL.\\nIt was a well-rounded talk with a speaker clearly having confident knowledge about the topic, so definitely give it a look if you're interested in the above topics.\\n\\n### Functional Programming Lab Hour\\n\\nThis session was unlike the others at the conference. It wasn't a talk, but rather a freestyle session to which all the Functional Programmer speakers came, the audience could propose some topics which were picked up by the presenters, and then the people attending could chat with the speakers in small groups about anything FP-related.\\n\\nI learned a lot about the basics of functional programming, and now feel much more confident in jumping in to try F# than I was before. And I think that the concepts we discussed (for instance the importance of immutability, or the problems with nullable variables) can be used to some extent in imperative languages as well.\\nIt was interesting that most of the people were coming from a C# background, and almost everybody was interested in F#. (I even felt bad for the Erlang/Haskell/Elixir experts who were nice enough to come and share their knowledge, yet everybody wanted to hear about F# :).)\\n\\nThis session was repeated on all three days, and it worked very nicely. More and more people came on the second and third day, and interesting discussions emerged organically.\\n\\nThis was definitely one of the higlights of the conference for me, I went away from these sessions inspired and motivated. Big kudos to [Mathias Brandewinder](http://brandewinder.com/) and [Bryan Hunter](http://codeswamp.com/) for organizing!\\n\\n### Fastware (Andrei Alexandrescu)\\n\\n(On vimeo, this can be found with the title Theres treasure everywhere.)\\nI have been following the work of [Andrei Alexandrescu](http://erdani.com/) for a long time, and have watched many of his conference talks. He's an energetic, confident and thoroughly entertaining speaker, and is also intimidatingly smart.\\nHe used to be a research scientist at Facebook, working on - among other things - optimizing Facebook's in-house developed [just-in-time compiler for PHP](https://github.com/facebook/hhvm).\\nRecently he left Facebook to work full-time on the [D language] he created together with Walter Bright.\\n\\nThe talk was about some new improvements Andrei [came up with recently](https://twitter.com/incomputable/status/738707845953294336) related to the `quickselect` algorithm found in many standard libraries, and also about some general optimization tricks using [sentinels](https://en.wikipedia.org/wiki/Sentinel_node).\\n\\nThe talk was highly entertaining and inspiring, the takeway was never to make assumptions or take things for granted, but always work with an open pair of eyes. To which the improvement of `quickselect` is a good example, since it's a rather simple algorithm, yet it hasn't been this significantly improved in 50 years.\\n\\n### Generic Locking in C++ (Andrei Alexandrescu)\\n\\nThe second talk was about an interesting C++ construct implemented as part of [folly](https://github.com/facebook/folly), an open source C++ library maintained by Facebook.\\n\\nThe C++11 standard promises that all the `const` methods of the containers in the standard C++ librar are thread safe, but the `non-const` functions are not promised to be thread safe, so we should call them from only a single thread at a time.\\n\\nIn the STL the type `std::shared_mutex` provides us a way to use a reader-writer lock, with which we can allow multiple readers into a critical section, but only a single writer (without any readers).\\n\\nThe way how the reader-writer lock works has a nice simmetry with the promise of the C++ standard, so we can use a lock to control concurrent access to the containers in the STL.\\n\\n```c++\\nstd::shared_mutex mutex;\\nstd::vector<int> vect;\\n\\n...\\n\\n{\\n    // Reading, need a shared lock\\n    std::shared_lock<std::shared_mutex> lock(mutex);\\n    int length = vect.size();\\n}\\n\\n{\\n    // Modifying, need an exclusive lock\\n    std::unique_lock<std::shared_mutex> lock(mutex);\\n    vect.clear();\\n}\\n```\\n\\nThe type introduced in folly is called `Synchronized`. It is a class template, which we can use to wrap other classes in it, and it makes all the methods of the underlying class available on the wrapper. Upon calling functions on `Synchronized`, it uses the proper synchronization mechanism before forwarding the call to the wrapped object. Thus we can safely call the methods on `Synchronized` concurrently without worrying about manually doing any locking.\\n\\n```c++\\nSynchronized<std::vector<int>> syncVect;\\n\\n...\\n\\n// Calling a const method, uses shared lock automatically\\nint length = syncVect->size();\\n\\n// Calling a non-const method, uses exclusive lock automatically\\nsyncVect->clear();\\n```\\n\\nNote that the functions `size()` and `clear()` are not defined on the type `Synchronized`, yet, we can call those on the `syncVect` object. This is made possible by the code-generation capabilities of C++ through template metaprogramming.\\nTo me this was fascinating, especially since I mostly use C# in my everyday work, with which - as far as I know - something like this is impossible.\\n\\nSometimes we tend to think that C++ is an old and outdated language, which is only used today for low-level things like OS-development or device drivers. The takeaway from this talk for me is the fact that C++ is still alive; very exciting and interesting things are happening in its community, and really smart people are working on improving it continuously.\\n\\nThese two talks by Andrei were one of the highlights of NDC to me. If you only watch one thing from NDC, I definitley recommend these.\\n\\n### C# Today and Tomorrow (Mads Torgersen)\\n\\nThis was the standard talk that Mads - the lead designer of C# - gives at almost all big conferences, and it's always updated with the new features coming to the C# language.\\n\\nSince I'm mostly working with C#, I was looking forwad to this session, and it lived up to my expectations. It was the right combination of entertaining, interesting and informational. Also, the new features coming to C# seem like really nice additions.\\n\\nWhat was interesting to me is that most of the upcoming features seem to be borrowed from functional languages, or related to functional programming and immutability.\\n\\nOne of the bigger new features is pattern matching, which makes the `switch` statement much more powerful, by removing the limitation that we can only have compile-time constants in the `case` clauses, but we will be able to use dynamic conditions, and we can also use a new thing introduced called patterns.\\n\\n```csharp\\nGeometry g = new Square(5);\\nswitch (g)\\n{\\n    case Triangle(int Width, int Height, int Base):\\n        WriteLine($\\\"{Width} {Height} {Base}\\\");\\n        break;\\n    case Rectangle(int Width, int Height):\\n        WriteLine($\\\"{Width} {Height}\\\");\\n        break;\\n    case Square(int Width):\\n        WriteLine($\\\"{Width}\\\");\\n        break;\\n    default:\\n        WriteLine(\\\"<other>\\\");\\n        break;\\n}\\n```\\n\\nThe other significant addition to the language will be tuple types, which will make it easier to work with methods that have to return multiple values.\\nIn the current version of C#, if we need to return multiple values from a method, we have a couple of options, of which none of is too good.\\n\\nWe can use `out` parameters.\\n\\n```csharp\\npublic void Tally(IEnumerable<int> values, out int sum, out int count) { ... }\\n```\\n\\nWe generally don't like this, since it's syntactically weird, especially if we want to use the result of the method inside an `if` condition.\\n\\nWe can use the `Tuple` type.\\n\\n```csharp\\npublic Tuple<int, int> Tally(IEnumerable<int> values) { ... }\\n```\\n\\nThis is not ideal either, since the type `Tuple<int, int>` makes the signature of the method hard to understand, and at the call site we will always forget which one is `Item1` and `Item2`.\\n\\nAnd we can explicitly define a return type.\\n\\n```csharp\\npublic struct TallyResult { public int Sum; public int Count; }\\npublic TallyResult Tally(IEnumerable<int> values) { ... }\\n```\\n\\nThis is an ideal solution when we are calling the method, but these helper types are boilerplate cluttering up our codebase.\\n\\nC# 7 will make it possible to return multiple values from a method, which can be than accessed outside on a single variable, as if the variable had a type containing those properties.\\n\\n```csharp\\npublic (int sum, int count) Tally(IEnumerable<int> values) { ... }\\n\\nvar t = Tally(myValues);\\nConsole.WriteLine($\\\"Sum: {t.sum}, count: {t.count}\\\");\\n```\\n\\n(Basically this is a syntactical sugar, which generates a type during compilation. And of course, this is not a novel idea, Golang and Swift - to name two examplex - have this too.)\\nA technical detail: the object `t` is going to be a value type.\\n\\nThe best thing about this talk was that Mads hung around at the Microsoft booth after the talk for more than an hour and had a chat with the \\\"fans\\\" :). He was really nice and approachable, and it was interesting to hear his personal opinions.\\n\\nWe asked him about the possible introduction of tail recursion to C#, he confirmed that it's not on the roadmap, so probably won't be introduced in the coming years (although there is no technical reason preventing it). We also discussed having a notion of *purity*, in terms of identifying methods which are mutating state and which don't, similarly to `const` methods in C++. This is not on the roadmap either. Actually, this was part of a research project at Microsoft called **Midori**, which was discontinued, and this purity concept had consequences which caused more problems than what it solved.\\n\\nOn the other hand, the thing which they are actually thinking about is providing a way to do compile-time code generation. The example he gave is the implementation of the `INotifyPropertyChanged` interface.\\nWhen implementing an MVVM application with WPF, Windows Phone or WinRT, in order to implement data binding, you have to implement all your view model properties the following way.\\n\\n```csharp\\nprivate string customerName;\\n\\npublic string CustomerName\\n{\\n    get\\n    {\\n        return this.customerName;\\n    }\\n\\n    set\\n    {\\n        if (value != this.customerName)\\n        {\\n            this.customerName = value;\\n            NotifyPropertyChanged();\\n        }\\n    }\\n}\\n```\\n\\nThis is quite a lot of boilerplate code, and this is what they would like to make generatable. The actual syntax to do it is not fleshed out yet, but one of the concepts is to be able to define a simple auto-generated property, and put an attribute on top of it to say that custom code should be generated for it.\\n\\n```csharp\\n[GenerateNotifyPropertyChangedProperty]\\npublic string CustomerName { get; set; }\\n```\\n\\n(This code example is made up by me based on what Mads told us.) And there would be a way to specify the template which generates the actual code, and then either through a manual action, or upon saving the file, the code based on the template would be generated.\\n\\nThis feature sounds very exciting to me, but probably we are quite far from actually seeing it becoming a part of C#, since it's very early in the design process.\\n\\n## Conclusion\\n\\nThis was the first time I went to NDC, and it was a great experience. I learned a lot, I came home inspired and motivated, and the overall vibe of the conference was tremendously good.\\nAll the sessions are going to be uploaded to [Vimeo](https://vimeo.com/ndcconferences) (most of them are already up), I recommend everyone to check them out.\\nI hope I will have the opportunity to go back next year, I will definitely try to do so.\\n\"}]],\"sections\":[[10,0]]}",
        "html": "<p>In the beginning of June I was lucky enough to be able to travel to the <a href=\"http://ndcoslo.com\">NDC conference in Oslo</a>. (Hats off to my employer <a href=\"http://travix.com\">Travix</a> for providing the budget!)</p>\n<p>The NDC has always been one of my favourite conferences besides Build and CppCon (and earlier the PDC and MIX), but so far I have only been following it online by watching the sessions uploaded to <a href=\"http://www.vimeo.com/ndcconferences\">Vimeo</a>. This was the first year I had the chance to actually travel to the conference, and it was a splendid experience.</p>\n<p>In this post I'd like to tell my impressions, and share the things I learned and found the most interesting.</p>\n<h2 id=\"venueorganizationetc\">Venue, organization, etc.</h2>\n<p>The conference was held at a central location in Oslo, in a big conference building called Spektrum.The large arena in the middle was used to host the booths of the sponsor companies, serve the food, and to provide a casual space for people to hang out between talks. This was also the venue for the two keynote talks during the conference.</p>\n<p><img src=\"/content/images/2016/07/ndc-main-venue.jpg\" alt=\"Central venue at NDC Oslo\"></p>\n<p>The food also deserves a mention: there was breakfast in the morning, and then lunch was served continuously throughout the whole day. There were several caterers, each serving one or two dishes, which was really cool, because there were many different dishes you could try out, such as sushi, taco, risotto, etc. (And also, the providers were different caterer companies who wanted to show off their product, so I think they all brought their A-game.)<br>\nSomeone even put together a poll to decide which provider was the best, it seems that people enjoyed the GitHub-sponsored tacos the most :)</p>\n<p><a href=\"https://poll.lab.io/ndc/results\"><img src=\"/content/images/2016/07/foodvote.png\" alt=\"NDC Oslo food poll\"></a></p>\n<p>And what else would help more to concentrate on tech talks all day than some good coffee? Just like the food, coffee was also much better than what is typical at conferences. You could always get a nice espress or a latte with properly steamed milk foam. (I haven't seen any of those dreaded coffee creamer thingies).</p>\n<p>And even some of the baristas were <a href=\"https://robdoescoffee.com/\">famous tech people</a> :).</p>\n<p><img src=\"/content/images/2016/07/robs-coffee.png\" alt=\"Rob Ashton serving coffe\"></p>\n<p>I can sum up the work the NDC guys put into organizing the conference with one word: perfect. Every talk started on time, all the tech worked, I didn't see a single problem with microphones, projectors, etc. The main venue was spacious and well designed, it was never too crowded, even during the busiest periods.<br>\nThe organizers clearly have a lot of experience in putting a conferenc together, they made a great job.</p>\n<h2 id=\"thetalks\">The talks</h2>\n<p>Although it was pretty good to hang out, eat nice food and drink hipster coffee all day, the most important part of the conference for me was the actual talks. They didn't disappoint, every talk I went to was high quality. I learned a lot, and I came home from the conference inspired.</p>\n<p>In the rest of this post I'd like to share some interesting details I learned at the talks I attended.</p>\n<h3 id=\"keynoteyesterdaystechnologyisdeadtodaysisonlifesupporttroyhunt\">Keynote: Yesterdays Technology is Dead, Todays is on Life Support (Troy Hunt)</h3>\n<p>I'm usually not a big fan of keynotes, they tend to be not very informational, and are often very marketingy, especially when they are tied to a particular company.</p>\n<p>However, I enjoyed this one a lot. <a href=\"https://www.troyhunt.com/\">Troy Hunt</a> is a Microsoft MVP, consultant and Pluralsight author, mostly focusing on security. He always delivers amusingly entertaining talks, and he's also an exceptionally charismatic presenter.<br>\nThis talk wasn't an exception, it was very interesting and thoroughly entertaining. I recommend giving it a watch if you want a good laugh and would like to learn some interesting trivia about the past, present and future of computing.</p>\n<h3 id=\"aspnetcore10deepdivedamianedwardsdavidfowler\">ASP.NET Core 1.0 deep dive (Damian Edwards, David Fowler)</h3>\n<p>I was really looking forward to this talk, since the main technology I use nowadays is .NET Core, so I was really interested in what the PM and the architect on the project will share.<br>\nThe talk was mostly about interesting details from under the hood of .NET, like where the framework is stored on the machine, how to see where the binaries are stored, and even about how someone could create a custom edition of the .NET framework.</p>\n<p>There were also some funky demos, like how ASP.NET Core can be run inside of a WinForms application, which - let's face it - not something you see every day ;).</p>\n<p>While the talk was really entertaining, to me it felt a bit random and underprepared. (They even had to troubleshoot on stage when the demos weren't working at first.)</p>\n<p>Despite these complaints it was a good talk, and it's always nice to see how enthusiastic these guys are about the platform.</p>\n<h3 id=\"aspnetcorekestreladventuresinbuildingafastwebserverdamianedwardsdavidfowler\">ASP.NET Core Kestrel: Adventures in building a fast web server (Damian Edwards, David Fowler)</h3>\n<p>The second talk by the guys from the ASP.NET team. This was about interesting tricks about optimizing <a href=\"https://github.com/aspnet/KestrelHttpServer\">Kestrel</a>, the new cross-platform web server specifically developed for .NET Core.</p>\n<p>First they introduced the <a href=\"https://www.techempower.com/benchmarks/\">TechEmpower Benchmarks</a>, which is an independent initiative for comparing the performance of different web frameworks. Although these benchmarks can be somewhat misleading, since low-level, very thin, specialized frameworks can have much higher throughput than all-around, general purpose web frameworks. On the other hand they might be so low-level, that they are not a practical choice for developing complex web applications.</p>\n<p>So it might be a bit hard to measure the importance of these measurements, it's still cool to see how certain frameworks compare to each other. Especially since throughput, response time, and the ability to handle many concurrent requests at the same time is getting more and more important, especially with the advent of technolgies like WebSockets.</p>\n<p>Early versions of Kestrel didn't perform too well on this Benchmark, but since then many things were optimized and in the next round of the TechEmpower benchmarks .NET Core should land at a much better position.</p>\n<p>This was a really interesting talks, we've seen some tricks and techniques which the average developer definitely doesn't see in the everyday work, especially when working with high-level web applications.</p>\n<p>There are two tricks I found particularly interesting.<br>\nIt seems that at high scale, most of the performance problems in .NET are due to object allocations and garbage collection. So one of the things they always kept in mind during the development of Kestrel was keeping the number of objects allocated as little as possible. (This is something that the Stack Overflow guys have also been blogging a lot about.)</p>\n<p>One of the cool optimizations is happening when the beginning of HTTP requests are processed, and the web server is extracting the HTTP verb from the request.<br>\nThe naive solution would be to read in the first couple of characters into a string, and compare it to the expected methods, such as <code>&quot;GET &quot;</code>, <code>&quot;POST &quot;</code>, etc. They figured out that the longest of these strings we have to take into account is <code>&quot;OPTIONS &quot;</code>, which is exactly eight characters, which, in the case of UTF-8 encoding fits exactly into a single long.</p>\n<p>To utilize this fact, Kestrel defines a couple of <code>static readonly long</code> fields, which hold the values of these expected strings, if we interpret their memory representations as a long value.</p>\n<pre><code class=\"language-csharp\">private readonly static long _httpGetMethodLong = GetAsciiStringAsLong(&quot;GET \\0\\0\\0\\0&quot;);\nprivate readonly static long _httpHeadMethodLong = GetAsciiStringAsLong(&quot;HEAD \\0\\0\\0&quot;);\n...\n</code></pre>\n<p>At the start of processing an HTTP request, the first 8 bytes of the request is read, interpreted as a long, and compared with these predefined values.</p>\n<pre><code class=\"language-csharp\">long value = begin.PeekLong();\n\nforeach (var x in _knownMethods)\n{\n    if ((value &amp; x.Item1) == x.Item2)\n    {\n        knownMethod = x.Item3;\n        return true;\n    }\n}\n\nreturn false;\n</code></pre>\n<p>This approach brings two benefits.</p>\n<ul>\n<li>We save on object allocation, since we're reading in 8 bytes into an existing buffer, and we don't have to create a <code>String</code> object.</li>\n<li>The other advantage is that comparing two longs is much faster than comparing two strings.</li>\n</ul>\n<p>The other thing that stick with me was a really funky trick. I won't go into the details, but the gist is that in a stream of bytes we want to find the position of the first non-zero byte. The naive solution would be to iterate over the stream one byte at a time and compare them with zero.<br>\nOn the other hand, the CPU works with longs - 8 bytes - anyway, so in a single operation, we are able to compare not one, but 8 bytes. However, this comparison doesn't give us the <em>position</em> of the non-zero value, but it lets us do the following.</p>\n<pre><code class=\"language-csharp\">return (i &lt;&lt; 3) +\n    ((longValue &amp; 0x00000000ffffffff) &gt; 0\n        ? (longValue &amp; 0x000000000000ffff) &gt; 0\n            ? (longValue &amp; 0x00000000000000ff) &gt; 0 ? 0 : 1\n            : (longValue &amp; 0x0000000000ff0000) &gt; 0 ? 2 : 3\n        : (longValue &amp; 0x0000ffff00000000) &gt; 0\n            ? (longValue &amp; 0x000000ff00000000) &gt; 0 ? 4 : 5\n            : (longValue &amp; 0x00ff000000000000) &gt; 0 ? 6 : 7);\n</code></pre>\n<p>What the above code is doing is basically a binary search in a bit pattern. When we're comparing our value with <code>0x00000000ffffffff</code> we get the information whether there is a non-zero value in the left, or in the right side of the 8 bytes. By doing this we can eliminate half of the 8 bytes in a single operation, so in worts case, we will execute 3 comparisons instead of 8.</p>\n<p>This stick with me, because I was surprised the simplicity and elegance of this trick. I was always familiar with how binary search worked and how it achieved logarithmic runtime instead of linear, but I don't know if I could came up with using the same trick not in a sorted collection, but on a bit pattern.</p>\n<h3 id=\"thecandclrmemorymodelssashagoldshtein\">The C++ and CLR Memory Models (Sasha Goldshtein)</h3>\n<p>I went to this talk expecting to learn about what's happening in memory when we're passing objects between the .NET Framework and native Win32 components, for example when using <code>PInvoke</code> or C++/CLI.</p>\n<p>Although the talk was really good, its topic was a bit different. It was particularly about the optimizations happening in the compiler and the CPU which can cause our operations to be reordered. It gave clear explanations about the type of bugs this can cause, and what are our options to prevent these, for instance the <code>volatile</code> keyword and the synchronization primitives in the C++ STL.<br>\nIt was a well-rounded talk with a speaker clearly having confident knowledge about the topic, so definitely give it a look if you're interested in the above topics.</p>\n<h3 id=\"functionalprogramminglabhour\">Functional Programming Lab Hour</h3>\n<p>This session was unlike the others at the conference. It wasn't a talk, but rather a freestyle session to which all the Functional Programmer speakers came, the audience could propose some topics which were picked up by the presenters, and then the people attending could chat with the speakers in small groups about anything FP-related.</p>\n<p>I learned a lot about the basics of functional programming, and now feel much more confident in jumping in to try F# than I was before. And I think that the concepts we discussed (for instance the importance of immutability, or the problems with nullable variables) can be used to some extent in imperative languages as well.<br>\nIt was interesting that most of the people were coming from a C# background, and almost everybody was interested in F#. (I even felt bad for the Erlang/Haskell/Elixir experts who were nice enough to come and share their knowledge, yet everybody wanted to hear about F# :).)</p>\n<p>This session was repeated on all three days, and it worked very nicely. More and more people came on the second and third day, and interesting discussions emerged organically.</p>\n<p>This was definitely one of the higlights of the conference for me, I went away from these sessions inspired and motivated. Big kudos to <a href=\"http://brandewinder.com/\">Mathias Brandewinder</a> and <a href=\"http://codeswamp.com/\">Bryan Hunter</a> for organizing!</p>\n<h3 id=\"fastwareandreialexandrescu\">Fastware (Andrei Alexandrescu)</h3>\n<p>(On vimeo, this can be found with the title Theres treasure everywhere.)<br>\nI have been following the work of <a href=\"http://erdani.com/\">Andrei Alexandrescu</a> for a long time, and have watched many of his conference talks. He's an energetic, confident and thoroughly entertaining speaker, and is also intimidatingly smart.<br>\nHe used to be a research scientist at Facebook, working on - among other things - optimizing Facebook's in-house developed <a href=\"https://github.com/facebook/hhvm\">just-in-time compiler for PHP</a>.<br>\nRecently he left Facebook to work full-time on the [D language] he created together with Walter Bright.</p>\n<p>The talk was about some new improvements Andrei <a href=\"https://twitter.com/incomputable/status/738707845953294336\">came up with recently</a> related to the <code>quickselect</code> algorithm found in many standard libraries, and also about some general optimization tricks using <a href=\"https://en.wikipedia.org/wiki/Sentinel_node\">sentinels</a>.</p>\n<p>The talk was highly entertaining and inspiring, the takeway was never to make assumptions or take things for granted, but always work with an open pair of eyes. To which the improvement of <code>quickselect</code> is a good example, since it's a rather simple algorithm, yet it hasn't been this significantly improved in 50 years.</p>\n<h3 id=\"genericlockingincandreialexandrescu\">Generic Locking in C++ (Andrei Alexandrescu)</h3>\n<p>The second talk was about an interesting C++ construct implemented as part of <a href=\"https://github.com/facebook/folly\">folly</a>, an open source C++ library maintained by Facebook.</p>\n<p>The C++11 standard promises that all the <code>const</code> methods of the containers in the standard C++ librar are thread safe, but the <code>non-const</code> functions are not promised to be thread safe, so we should call them from only a single thread at a time.</p>\n<p>In the STL the type <code>std::shared_mutex</code> provides us a way to use a reader-writer lock, with which we can allow multiple readers into a critical section, but only a single writer (without any readers).</p>\n<p>The way how the reader-writer lock works has a nice simmetry with the promise of the C++ standard, so we can use a lock to control concurrent access to the containers in the STL.</p>\n<pre><code class=\"language-c++\">std::shared_mutex mutex;\nstd::vector&lt;int&gt; vect;\n\n...\n\n{\n    // Reading, need a shared lock\n    std::shared_lock&lt;std::shared_mutex&gt; lock(mutex);\n    int length = vect.size();\n}\n\n{\n    // Modifying, need an exclusive lock\n    std::unique_lock&lt;std::shared_mutex&gt; lock(mutex);\n    vect.clear();\n}\n</code></pre>\n<p>The type introduced in folly is called <code>Synchronized</code>. It is a class template, which we can use to wrap other classes in it, and it makes all the methods of the underlying class available on the wrapper. Upon calling functions on <code>Synchronized</code>, it uses the proper synchronization mechanism before forwarding the call to the wrapped object. Thus we can safely call the methods on <code>Synchronized</code> concurrently without worrying about manually doing any locking.</p>\n<pre><code class=\"language-c++\">Synchronized&lt;std::vector&lt;int&gt;&gt; syncVect;\n\n...\n\n// Calling a const method, uses shared lock automatically\nint length = syncVect-&gt;size();\n\n// Calling a non-const method, uses exclusive lock automatically\nsyncVect-&gt;clear();\n</code></pre>\n<p>Note that the functions <code>size()</code> and <code>clear()</code> are not defined on the type <code>Synchronized</code>, yet, we can call those on the <code>syncVect</code> object. This is made possible by the code-generation capabilities of C++ through template metaprogramming.<br>\nTo me this was fascinating, especially since I mostly use C# in my everyday work, with which - as far as I know - something like this is impossible.</p>\n<p>Sometimes we tend to think that C++ is an old and outdated language, which is only used today for low-level things like OS-development or device drivers. The takeaway from this talk for me is the fact that C++ is still alive; very exciting and interesting things are happening in its community, and really smart people are working on improving it continuously.</p>\n<p>These two talks by Andrei were one of the highlights of NDC to me. If you only watch one thing from NDC, I definitley recommend these.</p>\n<h3 id=\"ctodayandtomorrowmadstorgersen\">C# Today and Tomorrow (Mads Torgersen)</h3>\n<p>This was the standard talk that Mads - the lead designer of C# - gives at almost all big conferences, and it's always updated with the new features coming to the C# language.</p>\n<p>Since I'm mostly working with C#, I was looking forwad to this session, and it lived up to my expectations. It was the right combination of entertaining, interesting and informational. Also, the new features coming to C# seem like really nice additions.</p>\n<p>What was interesting to me is that most of the upcoming features seem to be borrowed from functional languages, or related to functional programming and immutability.</p>\n<p>One of the bigger new features is pattern matching, which makes the <code>switch</code> statement much more powerful, by removing the limitation that we can only have compile-time constants in the <code>case</code> clauses, but we will be able to use dynamic conditions, and we can also use a new thing introduced called patterns.</p>\n<pre><code class=\"language-csharp\">Geometry g = new Square(5);\nswitch (g)\n{\n    case Triangle(int Width, int Height, int Base):\n        WriteLine($&quot;{Width} {Height} {Base}&quot;);\n        break;\n    case Rectangle(int Width, int Height):\n        WriteLine($&quot;{Width} {Height}&quot;);\n        break;\n    case Square(int Width):\n        WriteLine($&quot;{Width}&quot;);\n        break;\n    default:\n        WriteLine(&quot;&lt;other&gt;&quot;);\n        break;\n}\n</code></pre>\n<p>The other significant addition to the language will be tuple types, which will make it easier to work with methods that have to return multiple values.<br>\nIn the current version of C#, if we need to return multiple values from a method, we have a couple of options, of which none of is too good.</p>\n<p>We can use <code>out</code> parameters.</p>\n<pre><code class=\"language-csharp\">public void Tally(IEnumerable&lt;int&gt; values, out int sum, out int count) { ... }\n</code></pre>\n<p>We generally don't like this, since it's syntactically weird, especially if we want to use the result of the method inside an <code>if</code> condition.</p>\n<p>We can use the <code>Tuple</code> type.</p>\n<pre><code class=\"language-csharp\">public Tuple&lt;int, int&gt; Tally(IEnumerable&lt;int&gt; values) { ... }\n</code></pre>\n<p>This is not ideal either, since the type <code>Tuple&lt;int, int&gt;</code> makes the signature of the method hard to understand, and at the call site we will always forget which one is <code>Item1</code> and <code>Item2</code>.</p>\n<p>And we can explicitly define a return type.</p>\n<pre><code class=\"language-csharp\">public struct TallyResult { public int Sum; public int Count; }\npublic TallyResult Tally(IEnumerable&lt;int&gt; values) { ... }\n</code></pre>\n<p>This is an ideal solution when we are calling the method, but these helper types are boilerplate cluttering up our codebase.</p>\n<p>C# 7 will make it possible to return multiple values from a method, which can be than accessed outside on a single variable, as if the variable had a type containing those properties.</p>\n<pre><code class=\"language-csharp\">public (int sum, int count) Tally(IEnumerable&lt;int&gt; values) { ... }\n\nvar t = Tally(myValues);\nConsole.WriteLine($&quot;Sum: {t.sum}, count: {t.count}&quot;);\n</code></pre>\n<p>(Basically this is a syntactical sugar, which generates a type during compilation. And of course, this is not a novel idea, Golang and Swift - to name two examplex - have this too.)<br>\nA technical detail: the object <code>t</code> is going to be a value type.</p>\n<p>The best thing about this talk was that Mads hung around at the Microsoft booth after the talk for more than an hour and had a chat with the &quot;fans&quot; :). He was really nice and approachable, and it was interesting to hear his personal opinions.</p>\n<p>We asked him about the possible introduction of tail recursion to C#, he confirmed that it's not on the roadmap, so probably won't be introduced in the coming years (although there is no technical reason preventing it). We also discussed having a notion of <em>purity</em>, in terms of identifying methods which are mutating state and which don't, similarly to <code>const</code> methods in C++. This is not on the roadmap either. Actually, this was part of a research project at Microsoft called <strong>Midori</strong>, which was discontinued, and this purity concept had consequences which caused more problems than what it solved.</p>\n<p>On the other hand, the thing which they are actually thinking about is providing a way to do compile-time code generation. The example he gave is the implementation of the <code>INotifyPropertyChanged</code> interface.<br>\nWhen implementing an MVVM application with WPF, Windows Phone or WinRT, in order to implement data binding, you have to implement all your view model properties the following way.</p>\n<pre><code class=\"language-csharp\">private string customerName;\n\npublic string CustomerName\n{\n    get\n    {\n        return this.customerName;\n    }\n\n    set\n    {\n        if (value != this.customerName)\n        {\n            this.customerName = value;\n            NotifyPropertyChanged();\n        }\n    }\n}\n</code></pre>\n<p>This is quite a lot of boilerplate code, and this is what they would like to make generatable. The actual syntax to do it is not fleshed out yet, but one of the concepts is to be able to define a simple auto-generated property, and put an attribute on top of it to say that custom code should be generated for it.</p>\n<pre><code class=\"language-csharp\">[GenerateNotifyPropertyChangedProperty]\npublic string CustomerName { get; set; }\n</code></pre>\n<p>(This code example is made up by me based on what Mads told us.) And there would be a way to specify the template which generates the actual code, and then either through a manual action, or upon saving the file, the code based on the template would be generated.</p>\n<p>This feature sounds very exciting to me, but probably we are quite far from actually seeing it becoming a part of C#, since it's very early in the design process.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>This was the first time I went to NDC, and it was a great experience. I learned a lot, I came home inspired and motivated, and the overall vibe of the conference was tremendously good.<br>\nAll the sessions are going to be uploaded to <a href=\"https://vimeo.com/ndcconferences\">Vimeo</a> (most of them are already up), I recommend everyone to check them out.<br>\nI hope I will have the opportunity to go back next year, I will definitely try to do so.</p>\n",
        "comment_id": "22",
        "plaintext": "In the beginning of June I was lucky enough to be able to travel to the NDC\nconference in Oslo [http://ndcoslo.com]. (Hats off to my employer Travix\n[http://travix.com]  for providing the budget!)\n\nThe NDC has always been one of my favourite conferences besides Build and CppCon\n(and earlier the PDC and MIX), but so far I have only been following it online\nby watching the sessions uploaded to Vimeo [http://www.vimeo.com/ndcconferences]\n. This was the first year I had the chance to actually travel to the conference,\nand it was a splendid experience.\n\nIn this post I'd like to tell my impressions, and share the things I learned and\nfound the most interesting.\n\nVenue, organization, etc.\nThe conference was held at a central location in Oslo, in a big conference\nbuilding called Spektrum.The large arena in the middle was used to host the\nbooths of the sponsor companies, serve the food, and to provide a casual space\nfor people to hang out between talks. This was also the venue for the two\nkeynote talks during the conference.\n\n\n\nThe food also deserves a mention: there was breakfast in the morning, and then\nlunch was served continuously throughout the whole day. There were several\ncaterers, each serving one or two dishes, which was really cool, because there\nwere many different dishes you could try out, such as sushi, taco, risotto, etc.\n(And also, the providers were different caterer companies who wanted to show off\ntheir product, so I think they all brought their A-game.)\nSomeone even put together a poll to decide which provider was the best, it seems\nthat people enjoyed the GitHub-sponsored tacos the most :)\n\n  [https://poll.lab.io/ndc/results]\n\nAnd what else would help more to concentrate on tech talks all day than some\ngood coffee? Just like the food, coffee was also much better than what is\ntypical at conferences. You could always get a nice espress or a latte with\nproperly steamed milk foam. (I haven't seen any of those dreaded coffee creamer\nthingies).\n\nAnd even some of the baristas were famous tech people\n[https://robdoescoffee.com/]  :).\n\n\n\nI can sum up the work the NDC guys put into organizing the conference with one\nword: perfect. Every talk started on time, all the tech worked, I didn't see a\nsingle problem with microphones, projectors, etc. The main venue was spacious\nand well designed, it was never too crowded, even during the busiest periods.\nThe organizers clearly have a lot of experience in putting a conferenc together,\nthey made a great job.\n\nThe talks\nAlthough it was pretty good to hang out, eat nice food and drink hipster coffee\nall day, the most important part of the conference for me was the actual talks.\nThey didn't disappoint, every talk I went to was high quality. I learned a lot,\nand I came home from the conference inspired.\n\nIn the rest of this post I'd like to share some interesting details I learned at\nthe talks I attended.\n\nKeynote: Yesterdays Technology is Dead, Todays is on Life Support (Troy Hunt)\nI'm usually not a big fan of keynotes, they tend to be not very informational,\nand are often very marketingy, especially when they are tied to a particular\ncompany.\n\nHowever, I enjoyed this one a lot. Troy Hunt [https://www.troyhunt.com/]  is a\nMicrosoft MVP, consultant and Pluralsight author, mostly focusing on security.\nHe always delivers amusingly entertaining talks, and he's also an exceptionally\ncharismatic presenter.\nThis talk wasn't an exception, it was very interesting and thoroughly\nentertaining. I recommend giving it a watch if you want a good laugh and would\nlike to learn some interesting trivia about the past, present and future of\ncomputing.\n\nASP.NET Core 1.0 deep dive (Damian Edwards, David Fowler)\nI was really looking forward to this talk, since the main technology I use\nnowadays is .NET Core, so I was really interested in what the PM and the\narchitect on the project will share.\nThe talk was mostly about interesting details from under the hood of .NET, like\nwhere the framework is stored on the machine, how to see where the binaries are\nstored, and even about how someone could create a custom edition of the .NET\nframework.\n\nThere were also some funky demos, like how ASP.NET Core can be run inside of a\nWinForms application, which - let's face it - not something you see every day\n;).\n\nWhile the talk was really entertaining, to me it felt a bit random and\nunderprepared. (They even had to troubleshoot on stage when the demos weren't\nworking at first.)\n\nDespite these complaints it was a good talk, and it's always nice to see how\nenthusiastic these guys are about the platform.\n\nASP.NET Core Kestrel: Adventures in building a fast web server (Damian Edwards,\nDavid Fowler)\nThe second talk by the guys from the ASP.NET team. This was about interesting\ntricks about optimizing Kestrel [https://github.com/aspnet/KestrelHttpServer],\nthe new cross-platform web server specifically developed for .NET Core.\n\nFirst they introduced the TechEmpower Benchmarks\n[https://www.techempower.com/benchmarks/], which is an independent initiative\nfor comparing the performance of different web frameworks. Although these\nbenchmarks can be somewhat misleading, since low-level, very thin, specialized\nframeworks can have much higher throughput than all-around, general purpose web\nframeworks. On the other hand they might be so low-level, that they are not a\npractical choice for developing complex web applications.\n\nSo it might be a bit hard to measure the importance of these measurements, it's\nstill cool to see how certain frameworks compare to each other. Especially since\nthroughput, response time, and the ability to handle many concurrent requests at\nthe same time is getting more and more important, especially with the advent of\ntechnolgies like WebSockets.\n\nEarly versions of Kestrel didn't perform too well on this Benchmark, but since\nthen many things were optimized and in the next round of the TechEmpower\nbenchmarks .NET Core should land at a much better position.\n\nThis was a really interesting talks, we've seen some tricks and techniques which\nthe average developer definitely doesn't see in the everyday work, especially\nwhen working with high-level web applications.\n\nThere are two tricks I found particularly interesting.\nIt seems that at high scale, most of the performance problems in .NET are due to\nobject allocations and garbage collection. So one of the things they always kept\nin mind during the development of Kestrel was keeping the number of objects\nallocated as little as possible. (This is something that the Stack Overflow guys\nhave also been blogging a lot about.)\n\nOne of the cool optimizations is happening when the beginning of HTTP requests\nare processed, and the web server is extracting the HTTP verb from the request.\nThe naive solution would be to read in the first couple of characters into a\nstring, and compare it to the expected methods, such as \"GET \", \"POST \", etc.\nThey figured out that the longest of these strings we have to take into account\nis \"OPTIONS \", which is exactly eight characters, which, in the case of UTF-8\nencoding fits exactly into a single long.\n\nTo utilize this fact, Kestrel defines a couple of static readonly long  fields,\nwhich hold the values of these expected strings, if we interpret their memory\nrepresentations as a long value.\n\nprivate readonly static long _httpGetMethodLong = GetAsciiStringAsLong(\"GET \\0\\0\\0\\0\");\nprivate readonly static long _httpHeadMethodLong = GetAsciiStringAsLong(\"HEAD \\0\\0\\0\");\n...\n\n\nAt the start of processing an HTTP request, the first 8 bytes of the request is\nread, interpreted as a long, and compared with these predefined values.\n\nlong value = begin.PeekLong();\n\nforeach (var x in _knownMethods)\n{\n    if ((value & x.Item1) == x.Item2)\n    {\n        knownMethod = x.Item3;\n        return true;\n    }\n}\n\nreturn false;\n\n\nThis approach brings two benefits.\n\n * We save on object allocation, since we're reading in 8 bytes into an existing\n   buffer, and we don't have to create a String  object.\n * The other advantage is that comparing two longs is much faster than comparing\n   two strings.\n\nThe other thing that stick with me was a really funky trick. I won't go into the\ndetails, but the gist is that in a stream of bytes we want to find the position\nof the first non-zero byte. The naive solution would be to iterate over the\nstream one byte at a time and compare them with zero.\nOn the other hand, the CPU works with longs - 8 bytes - anyway, so in a single\noperation, we are able to compare not one, but 8 bytes. However, this comparison\ndoesn't give us the position  of the non-zero value, but it lets us do the\nfollowing.\n\nreturn (i << 3) +\n    ((longValue & 0x00000000ffffffff) > 0\n        ? (longValue & 0x000000000000ffff) > 0\n            ? (longValue & 0x00000000000000ff) > 0 ? 0 : 1\n            : (longValue & 0x0000000000ff0000) > 0 ? 2 : 3\n        : (longValue & 0x0000ffff00000000) > 0\n            ? (longValue & 0x000000ff00000000) > 0 ? 4 : 5\n            : (longValue & 0x00ff000000000000) > 0 ? 6 : 7);\n\n\nWhat the above code is doing is basically a binary search in a bit pattern. When\nwe're comparing our value with 0x00000000ffffffff  we get the information\nwhether there is a non-zero value in the left, or in the right side of the 8\nbytes. By doing this we can eliminate half of the 8 bytes in a single operation,\nso in worts case, we will execute 3 comparisons instead of 8.\n\nThis stick with me, because I was surprised the simplicity and elegance of this\ntrick. I was always familiar with how binary search worked and how it achieved\nlogarithmic runtime instead of linear, but I don't know if I could came up with\nusing the same trick not in a sorted collection, but on a bit pattern.\n\nThe C++ and CLR Memory Models (Sasha Goldshtein)\nI went to this talk expecting to learn about what's happening in memory when\nwe're passing objects between the .NET Framework and native Win32 components,\nfor example when using PInvoke  or C++/CLI.\n\nAlthough the talk was really good, its topic was a bit different. It was\nparticularly about the optimizations happening in the compiler and the CPU which\ncan cause our operations to be reordered. It gave clear explanations about the\ntype of bugs this can cause, and what are our options to prevent these, for\ninstance the volatile  keyword and the synchronization primitives in the C++\nSTL.\nIt was a well-rounded talk with a speaker clearly having confident knowledge\nabout the topic, so definitely give it a look if you're interested in the above\ntopics.\n\nFunctional Programming Lab Hour\nThis session was unlike the others at the conference. It wasn't a talk, but\nrather a freestyle session to which all the Functional Programmer speakers came,\nthe audience could propose some topics which were picked up by the presenters,\nand then the people attending could chat with the speakers in small groups about\nanything FP-related.\n\nI learned a lot about the basics of functional programming, and now feel much\nmore confident in jumping in to try F# than I was before. And I think that the\nconcepts we discussed (for instance the importance of immutability, or the\nproblems with nullable variables) can be used to some extent in imperative\nlanguages as well.\nIt was interesting that most of the people were coming from a C# background, and\nalmost everybody was interested in F#. (I even felt bad for the\nErlang/Haskell/Elixir experts who were nice enough to come and share their\nknowledge, yet everybody wanted to hear about F# :).)\n\nThis session was repeated on all three days, and it worked very nicely. More and\nmore people came on the second and third day, and interesting discussions\nemerged organically.\n\nThis was definitely one of the higlights of the conference for me, I went away\nfrom these sessions inspired and motivated. Big kudos to Mathias Brandewinder\n[http://brandewinder.com/]  and Bryan Hunter [http://codeswamp.com/]  for\norganizing!\n\nFastware (Andrei Alexandrescu)\n(On vimeo, this can be found with the title Theres treasure everywhere.)\nI have been following the work of Andrei Alexandrescu [http://erdani.com/]  for\na long time, and have watched many of his conference talks. He's an energetic,\nconfident and thoroughly entertaining speaker, and is also intimidatingly smart.\nHe used to be a research scientist at Facebook, working on - among other things\n- optimizing Facebook's in-house developed just-in-time compiler for PHP\n[https://github.com/facebook/hhvm].\nRecently he left Facebook to work full-time on the [D language] he created\ntogether with Walter Bright.\n\nThe talk was about some new improvements Andrei came up with recently\n[https://twitter.com/incomputable/status/738707845953294336]  related to the \nquickselect  algorithm found in many standard libraries, and also about some\ngeneral optimization tricks using sentinels\n[https://en.wikipedia.org/wiki/Sentinel_node].\n\nThe talk was highly entertaining and inspiring, the takeway was never to make\nassumptions or take things for granted, but always work with an open pair of\neyes. To which the improvement of quickselect  is a good example, since it's a\nrather simple algorithm, yet it hasn't been this significantly improved in 50\nyears.\n\nGeneric Locking in C++ (Andrei Alexandrescu)\nThe second talk was about an interesting C++ construct implemented as part of \nfolly [https://github.com/facebook/folly], an open source C++ library maintained\nby Facebook.\n\nThe C++11 standard promises that all the const  methods of the containers in the\nstandard C++ librar are thread safe, but the non-const  functions are not\npromised to be thread safe, so we should call them from only a single thread at\na time.\n\nIn the STL the type std::shared_mutex  provides us a way to use a reader-writer\nlock, with which we can allow multiple readers into a critical section, but only\na single writer (without any readers).\n\nThe way how the reader-writer lock works has a nice simmetry with the promise of\nthe C++ standard, so we can use a lock to control concurrent access to the\ncontainers in the STL.\n\nstd::shared_mutex mutex;\nstd::vector<int> vect;\n\n...\n\n{\n    // Reading, need a shared lock\n    std::shared_lock<std::shared_mutex> lock(mutex);\n    int length = vect.size();\n}\n\n{\n    // Modifying, need an exclusive lock\n    std::unique_lock<std::shared_mutex> lock(mutex);\n    vect.clear();\n}\n\n\nThe type introduced in folly is called Synchronized. It is a class template,\nwhich we can use to wrap other classes in it, and it makes all the methods of\nthe underlying class available on the wrapper. Upon calling functions on \nSynchronized, it uses the proper synchronization mechanism before forwarding the\ncall to the wrapped object. Thus we can safely call the methods on Synchronized \nconcurrently without worrying about manually doing any locking.\n\nSynchronized<std::vector<int>> syncVect;\n\n...\n\n// Calling a const method, uses shared lock automatically\nint length = syncVect->size();\n\n// Calling a non-const method, uses exclusive lock automatically\nsyncVect->clear();\n\n\nNote that the functions size()  and clear()  are not defined on the type \nSynchronized, yet, we can call those on the syncVect  object. This is made\npossible by the code-generation capabilities of C++ through template\nmetaprogramming.\nTo me this was fascinating, especially since I mostly use C# in my everyday\nwork, with which - as far as I know - something like this is impossible.\n\nSometimes we tend to think that C++ is an old and outdated language, which is\nonly used today for low-level things like OS-development or device drivers. The\ntakeaway from this talk for me is the fact that C++ is still alive; very\nexciting and interesting things are happening in its community, and really smart\npeople are working on improving it continuously.\n\nThese two talks by Andrei were one of the highlights of NDC to me. If you only\nwatch one thing from NDC, I definitley recommend these.\n\nC# Today and Tomorrow (Mads Torgersen)\nThis was the standard talk that Mads - the lead designer of C# - gives at almost\nall big conferences, and it's always updated with the new features coming to the\nC# language.\n\nSince I'm mostly working with C#, I was looking forwad to this session, and it\nlived up to my expectations. It was the right combination of entertaining,\ninteresting and informational. Also, the new features coming to C# seem like\nreally nice additions.\n\nWhat was interesting to me is that most of the upcoming features seem to be\nborrowed from functional languages, or related to functional programming and\nimmutability.\n\nOne of the bigger new features is pattern matching, which makes the switch \nstatement much more powerful, by removing the limitation that we can only have\ncompile-time constants in the case  clauses, but we will be able to use dynamic\nconditions, and we can also use a new thing introduced called patterns.\n\nGeometry g = new Square(5);\nswitch (g)\n{\n    case Triangle(int Width, int Height, int Base):\n        WriteLine($\"{Width} {Height} {Base}\");\n        break;\n    case Rectangle(int Width, int Height):\n        WriteLine($\"{Width} {Height}\");\n        break;\n    case Square(int Width):\n        WriteLine($\"{Width}\");\n        break;\n    default:\n        WriteLine(\"<other>\");\n        break;\n}\n\n\nThe other significant addition to the language will be tuple types, which will\nmake it easier to work with methods that have to return multiple values.\nIn the current version of C#, if we need to return multiple values from a\nmethod, we have a couple of options, of which none of is too good.\n\nWe can use out  parameters.\n\npublic void Tally(IEnumerable<int> values, out int sum, out int count) { ... }\n\n\nWe generally don't like this, since it's syntactically weird, especially if we\nwant to use the result of the method inside an if  condition.\n\nWe can use the Tuple  type.\n\npublic Tuple<int, int> Tally(IEnumerable<int> values) { ... }\n\n\nThis is not ideal either, since the type Tuple<int, int>  makes the signature of\nthe method hard to understand, and at the call site we will always forget which\none is Item1  and Item2.\n\nAnd we can explicitly define a return type.\n\npublic struct TallyResult { public int Sum; public int Count; }\npublic TallyResult Tally(IEnumerable<int> values) { ... }\n\n\nThis is an ideal solution when we are calling the method, but these helper types\nare boilerplate cluttering up our codebase.\n\nC# 7 will make it possible to return multiple values from a method, which can be\nthan accessed outside on a single variable, as if the variable had a type\ncontaining those properties.\n\npublic (int sum, int count) Tally(IEnumerable<int> values) { ... }\n\nvar t = Tally(myValues);\nConsole.WriteLine($\"Sum: {t.sum}, count: {t.count}\");\n\n\n(Basically this is a syntactical sugar, which generates a type during\ncompilation. And of course, this is not a novel idea, Golang and Swift - to name\ntwo examplex - have this too.)\nA technical detail: the object t  is going to be a value type.\n\nThe best thing about this talk was that Mads hung around at the Microsoft booth\nafter the talk for more than an hour and had a chat with the \"fans\" :). He was\nreally nice and approachable, and it was interesting to hear his personal\nopinions.\n\nWe asked him about the possible introduction of tail recursion to C#, he\nconfirmed that it's not on the roadmap, so probably won't be introduced in the\ncoming years (although there is no technical reason preventing it). We also\ndiscussed having a notion of purity, in terms of identifying methods which are\nmutating state and which don't, similarly to const  methods in C++. This is not\non the roadmap either. Actually, this was part of a research project at\nMicrosoft called Midori, which was discontinued, and this purity concept had\nconsequences which caused more problems than what it solved.\n\nOn the other hand, the thing which they are actually thinking about is providing\na way to do compile-time code generation. The example he gave is the\nimplementation of the INotifyPropertyChanged  interface.\nWhen implementing an MVVM application with WPF, Windows Phone or WinRT, in order\nto implement data binding, you have to implement all your view model properties\nthe following way.\n\nprivate string customerName;\n\npublic string CustomerName\n{\n    get\n    {\n        return this.customerName;\n    }\n\n    set\n    {\n        if (value != this.customerName)\n        {\n            this.customerName = value;\n            NotifyPropertyChanged();\n        }\n    }\n}\n\n\nThis is quite a lot of boilerplate code, and this is what they would like to\nmake generatable. The actual syntax to do it is not fleshed out yet, but one of\nthe concepts is to be able to define a simple auto-generated property, and put\nan attribute on top of it to say that custom code should be generated for it.\n\n[GenerateNotifyPropertyChangedProperty]\npublic string CustomerName { get; set; }\n\n\n(This code example is made up by me based on what Mads told us.) And there would\nbe a way to specify the template which generates the actual code, and then\neither through a manual action, or upon saving the file, the code based on the\ntemplate would be generated.\n\nThis feature sounds very exciting to me, but probably we are quite far from\nactually seeing it becoming a part of C#, since it's very early in the design\nprocess.\n\nConclusion\nThis was the first time I went to NDC, and it was a great experience. I learned\na lot, I came home inspired and motivated, and the overall vibe of the\nconference was tremendously good.\nAll the sessions are going to be uploaded to Vimeo\n[https://vimeo.com/ndcconferences]  (most of them are already up), I recommend\neveryone to check them out.\nI hope I will have the opportunity to go back next year, I will definitely try\nto do so.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "A summary of my experiences at NDC Oslo 2016, and excerpts from the sessions I found the most interesting.",
        "author_id": "1",
        "created_at": "2016-07-02 18:14:27",
        "created_by": "1",
        "updated_at": "2016-10-30 14:56:55",
        "updated_by": "1",
        "published_at": "2016-07-02 18:19:12",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de296d",
        "uuid": "9f2d112d-0d38-4110-9fd3-8714e4ad5407",
        "title": "How to fix the empty SpecFlow HTML report problem with vstest.console.exe",
        "slug": "how-to-fix-the-empty-specflow-html-report-problem-with-vstest-console-exe",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"## Introduction\\n\\nThere are multiple ways to run an MsTest test suite from the command line. The older, now deprecated tool is `mstest.exe`. It executes the test suite and produces an output in an XML-based format called TRX.  \\nOther tools, including the SpecFlow HTML report generation build upon that TRX format.\\n\\nThe newer, current way to execute the unit tests in a project is `vstest.console.exe`. This new tool has a pluggable logging system, it supports specifying different \\\"loggers\\\", which can produce different outputs.  \\nOne of the built-in loggers is called `trx`, which is supposed to produce the output in the same format as mstest.exe did.\\n\\nThe problem with this TRX logger is that it produces a slightly different output than mstest.exe, which beaks the SpecFlow report generation. If you try to generate an HTML report from the new TRX file, the report will be empty, all the test names and descriptions will be missing.\\n\\n![Empty SpecFlow HTML report.](/content/images/2016/04/empty-specflow-report.png)\\n\\n## The problem\\n\\nThe issue was discussed in the a [GitHub issue](https://github.com/techtalk/SpecFlow/issues/278) in the SpecFlow repository. Paul Rohorzka identified the differences between the two TRX outputs causing the problems with the report:\\n\\n>1. `//TestRun/TestDefinitions/UnitTest/TestMethod/@className` renders not the fully qualified name, but just the namespace.  \\nEffect for report generation: **No relevant textual output at all**\\n\\n>2. The `TestDescriptionAttribute` is not rendered, but should be in `//TestRun/TestDefinitions/UnitTest/Description`.  \\nEffect for report generation: **No scenario titles**\\n\\n>3. `TestPropertyAttributes` are not rendered, but should be in `//TestRun/TestDefinitions/UnitTest/Properties`.  \\nEffect for report generation: **No feature titles**\\n\\n## The solution\\n\\nLuckily thanks to the pluggable nature of the `vstest.console.exe` logging, it's possible to hook in a custom Logger implementation when executing the test suite.\\n\\nIn order to do this we need to implement the interface `Microsoft.VisualStudio.TestPlatform.ObjectModel.Client.ITestLogger`, where we can implement our fully custom code generating the output (instructions to get started can be found [here](https://blogs.msdn.microsoft.com/vikramagrawal/2012/07/26/writing-loggers-for-command-line-test-runner-vstest-console-exe/)).\\n\\nThen the compiled assembly should be copied into `C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 12.0\\\\Common7\\\\IDE\\\\CommonExtensions\\\\Microsoft\\\\TestWindow\\\\Extensions` so that `vstest.console.exe` can pick it up.  \\nThe last thing to do is to specify that we want to use that particular logger. That can be done with the `Logger` parameter of the test runner. Example:\\n\\n```bash\\nvstest.console.exe ... /Logger:MyCustomLogger\\n```\\n\\nThe name of the logger has to be equal to the value specified in the `FriendlyName` attribute on our implementation class.\\n\\nI implemented a custom logger called `MsTestTrxLogger` in which I replicated to format of the old `mstest.exe`. I tried to make it generate an output as close to the original as possible, but I had to make some assumptions. A [post](https://blogs.msdn.microsoft.com/dhopton/2008/06/13/helpful-internals-of-trx-and-vsmdi-files/) by Dominic Hopton about TRX internals helped in filling in some of the details.\\n\\nI uploaded the solution with instructions about how to use it to this [GitHub repository](https://github.com/markvincze/MsTestTrxLogger).\\n\\nIt was an interesting experience to to reverse engineer the `mstest.exe` output and try to replicate that using the data model provided by `Microsoft.VisualStudio.TestPlatform.ObjectModel`, and it's nice to see that with some tinkering we can fully customize the output of `vstest.console.exe`.\\n\\nSo far it's working perfectly with the SpecFlow report of a test suite containing ~300 tests. However, due to the assumptions I had to make, it might now work perfectly in every situation. So issue reports (or pull requests :)) are welcome if you find any problems.\"}]],\"sections\":[[10,0]]}",
        "html": "<h2 id=\"introduction\">Introduction</h2>\n<p>There are multiple ways to run an MsTest test suite from the command line. The older, now deprecated tool is <code>mstest.exe</code>. It executes the test suite and produces an output in an XML-based format called TRX.<br>\nOther tools, including the SpecFlow HTML report generation build upon that TRX format.</p>\n<p>The newer, current way to execute the unit tests in a project is <code>vstest.console.exe</code>. This new tool has a pluggable logging system, it supports specifying different &quot;loggers&quot;, which can produce different outputs.<br>\nOne of the built-in loggers is called <code>trx</code>, which is supposed to produce the output in the same format as mstest.exe did.</p>\n<p>The problem with this TRX logger is that it produces a slightly different output than mstest.exe, which beaks the SpecFlow report generation. If you try to generate an HTML report from the new TRX file, the report will be empty, all the test names and descriptions will be missing.</p>\n<p><img src=\"/content/images/2016/04/empty-specflow-report.png\" alt=\"Empty SpecFlow HTML report.\"></p>\n<h2 id=\"theproblem\">The problem</h2>\n<p>The issue was discussed in the a <a href=\"https://github.com/techtalk/SpecFlow/issues/278\">GitHub issue</a> in the SpecFlow repository. Paul Rohorzka identified the differences between the two TRX outputs causing the problems with the report:</p>\n<blockquote>\n<ol>\n<li><code>//TestRun/TestDefinitions/UnitTest/TestMethod/@className</code> renders not the fully qualified name, but just the namespace.<br>\nEffect for report generation: <strong>No relevant textual output at all</strong></li>\n</ol>\n</blockquote>\n<blockquote>\n<ol start=\"2\">\n<li>The <code>TestDescriptionAttribute</code> is not rendered, but should be in <code>//TestRun/TestDefinitions/UnitTest/Description</code>.<br>\nEffect for report generation: <strong>No scenario titles</strong></li>\n</ol>\n</blockquote>\n<blockquote>\n<ol start=\"3\">\n<li><code>TestPropertyAttributes</code> are not rendered, but should be in <code>//TestRun/TestDefinitions/UnitTest/Properties</code>.<br>\nEffect for report generation: <strong>No feature titles</strong></li>\n</ol>\n</blockquote>\n<h2 id=\"thesolution\">The solution</h2>\n<p>Luckily thanks to the pluggable nature of the <code>vstest.console.exe</code> logging, it's possible to hook in a custom Logger implementation when executing the test suite.</p>\n<p>In order to do this we need to implement the interface <code>Microsoft.VisualStudio.TestPlatform.ObjectModel.Client.ITestLogger</code>, where we can implement our fully custom code generating the output (instructions to get started can be found <a href=\"https://blogs.msdn.microsoft.com/vikramagrawal/2012/07/26/writing-loggers-for-command-line-test-runner-vstest-console-exe/\">here</a>).</p>\n<p>Then the compiled assembly should be copied into <code>C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow\\Extensions</code> so that <code>vstest.console.exe</code> can pick it up.<br>\nThe last thing to do is to specify that we want to use that particular logger. That can be done with the <code>Logger</code> parameter of the test runner. Example:</p>\n<pre><code class=\"language-bash\">vstest.console.exe ... /Logger:MyCustomLogger\n</code></pre>\n<p>The name of the logger has to be equal to the value specified in the <code>FriendlyName</code> attribute on our implementation class.</p>\n<p>I implemented a custom logger called <code>MsTestTrxLogger</code> in which I replicated to format of the old <code>mstest.exe</code>. I tried to make it generate an output as close to the original as possible, but I had to make some assumptions. A <a href=\"https://blogs.msdn.microsoft.com/dhopton/2008/06/13/helpful-internals-of-trx-and-vsmdi-files/\">post</a> by Dominic Hopton about TRX internals helped in filling in some of the details.</p>\n<p>I uploaded the solution with instructions about how to use it to this <a href=\"https://github.com/markvincze/MsTestTrxLogger\">GitHub repository</a>.</p>\n<p>It was an interesting experience to to reverse engineer the <code>mstest.exe</code> output and try to replicate that using the data model provided by <code>Microsoft.VisualStudio.TestPlatform.ObjectModel</code>, and it's nice to see that with some tinkering we can fully customize the output of <code>vstest.console.exe</code>.</p>\n<p>So far it's working perfectly with the SpecFlow report of a test suite containing ~300 tests. However, due to the assumptions I had to make, it might now work perfectly in every situation. So issue reports (or pull requests :)) are welcome if you find any problems.</p>\n",
        "comment_id": "19",
        "plaintext": "Introduction\nThere are multiple ways to run an MsTest test suite from the command line. The\nolder, now deprecated tool is mstest.exe. It executes the test suite and\nproduces an output in an XML-based format called TRX.\nOther tools, including the SpecFlow HTML report generation build upon that TRX\nformat.\n\nThe newer, current way to execute the unit tests in a project is \nvstest.console.exe. This new tool has a pluggable logging system, it supports\nspecifying different \"loggers\", which can produce different outputs.\nOne of the built-in loggers is called trx, which is supposed to produce the\noutput in the same format as mstest.exe did.\n\nThe problem with this TRX logger is that it produces a slightly different output\nthan mstest.exe, which beaks the SpecFlow report generation. If you try to\ngenerate an HTML report from the new TRX file, the report will be empty, all the\ntest names and descriptions will be missing.\n\n\n\nThe problem\nThe issue was discussed in the a GitHub issue\n[https://github.com/techtalk/SpecFlow/issues/278]  in the SpecFlow repository.\nPaul Rohorzka identified the differences between the two TRX outputs causing the\nproblems with the report:\n\n 1. //TestRun/TestDefinitions/UnitTest/TestMethod/@className  renders not the\n    fully qualified name, but just the namespace.\n    Effect for report generation: No relevant textual output at all\n\n 2. The TestDescriptionAttribute  is not rendered, but should be in \n    //TestRun/TestDefinitions/UnitTest/Description.\n    Effect for report generation: No scenario titles\n\n 3. TestPropertyAttributes  are not rendered, but should be in \n    //TestRun/TestDefinitions/UnitTest/Properties.\n    Effect for report generation: No feature titles\n\nThe solution\nLuckily thanks to the pluggable nature of the vstest.console.exe  logging, it's\npossible to hook in a custom Logger implementation when executing the test\nsuite.\n\nIn order to do this we need to implement the interface \nMicrosoft.VisualStudio.TestPlatform.ObjectModel.Client.ITestLogger, where we can\nimplement our fully custom code generating the output (instructions to get\nstarted can be found here\n[https://blogs.msdn.microsoft.com/vikramagrawal/2012/07/26/writing-loggers-for-command-line-test-runner-vstest-console-exe/]\n).\n\nThen the compiled assembly should be copied into C:\\Program Files\n(x86)\\Microsoft Visual Studio\n12.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow\\Extensions  so that \nvstest.console.exe  can pick it up.\nThe last thing to do is to specify that we want to use that particular logger.\nThat can be done with the Logger  parameter of the test runner. Example:\n\nvstest.console.exe ... /Logger:MyCustomLogger\n\n\nThe name of the logger has to be equal to the value specified in the \nFriendlyName  attribute on our implementation class.\n\nI implemented a custom logger called MsTestTrxLogger  in which I replicated to\nformat of the old mstest.exe. I tried to make it generate an output as close to\nthe original as possible, but I had to make some assumptions. A post\n[https://blogs.msdn.microsoft.com/dhopton/2008/06/13/helpful-internals-of-trx-and-vsmdi-files/] \n by Dominic Hopton about TRX internals helped in filling in some of the details.\n\nI uploaded the solution with instructions about how to use it to this GitHub\nrepository [https://github.com/markvincze/MsTestTrxLogger].\n\nIt was an interesting experience to to reverse engineer the mstest.exe  output\nand try to replicate that using the data model provided by \nMicrosoft.VisualStudio.TestPlatform.ObjectModel, and it's nice to see that with\nsome tinkering we can fully customize the output of vstest.console.exe.\n\nSo far it's working perfectly with the SpecFlow report of a test suite\ncontaining ~300 tests. However, due to the assumptions I had to make, it might\nnow work perfectly in every situation. So issue reports (or pull requests :))\nare welcome if you find any problems.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": "Fix for empty SpecFlow HTML reports with vstest.console.exe",
        "meta_description": "The slightly changed TRX generation in vstest.console.exe causes SpecFlow to generate empty HTML reports. This can be fixed with a custom Logger.",
        "author_id": "1",
        "created_at": "2016-04-23 15:45:38",
        "created_by": "1",
        "updated_at": "2016-10-30 14:57:44",
        "updated_by": "1",
        "published_at": "2016-04-23 15:57:05",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de296e",
        "uuid": "79870bf2-1d94-422e-9c14-4ebbeeae5763",
        "title": "Matching route templates manually in ASP.NET Core",
        "slug": "matching-route-templates-manually-in-asp-net-core",
        "mobiledoc": "{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"We can use routing in ASP.NET to define paths on which we want to respond to HTTP requests. In ASP.NET Core we have two common ways to specify routing in our application.\\n\\nWe can use the `Route` attribute on the action methods:\\n\\n```csharp\\n[HttpGet(\\\"test/{myParam}\\\"]\\npublic IActionResult Get(int myParam)\\n{\\n    // ...\\n}\\n```\\n\\nOr if we don't want to use MVC, we can directly set up some responses in our `Startup` class by creating a `RouteBuilder` and adding it to the pipeline with the `UseRouter` method.\\n\\n```csharp\\nRouteBuilder builder = new RouteBuilder(app);\\n\\nbuilder.MapVerb(\\n    HttpMethod.Get.Method,\\n    \\\"test/{myParam}\\\",\\n    async context =>\\n    {\\n        var arg = context.GetRouteData().Values[\\\"myParam\\\"];\\n\\n        // ...\\n    });\\n\\napp.UseRouter(builder.Build());\\n```\\n\\n## Using route matching manually\\n\\nIn usual development the methods above are enough, since most often we'll use MVC to implement our application. On the other hand sometimes it can be handy if we're able to manually match a URL path to a template and extract the arguments.\\n\\nRecently I needed to do this in the [Stubbery](https://github.com/markvincze/Stubbery) library, to set up stubbed replies for various request paths.\\n\\nI looked around in the ASP.NET Routing codebase quite a while in an attempt to figure out how the routing worked internally and whether the classes doing the template matching are  `public`, so I can use them in my code.\\n\\n### TLDR version\\n\\nYou can manually do the template matching and extract the route arguments with this [little helper class](https://github.com/markvincze/Stubbery/blob/master/src/Stubbery/RequestMatching/RouteMatcher.cs).\\n\\n```csharp\\npublic class RouteMatcher\\n{\\n    public RouteValueDictionary Match(string routeTemplate, string requestPath)\\n    {\\n        var template = TemplateParser.Parse(routeTemplate);\\n\\n        var matcher = new TemplateMatcher(template, GetDefaults(template));\\n\\n        var values = matcher.Match(requestPath);\\n\\n        return values;\\n    }\\n\\n    // This method extracts the default argument values from the template.\\n    private RouteValueDictionary GetDefaults(RouteTemplate parsedTemplate)\\n    {\\n        var result = new RouteValueDictionary();\\n\\n        foreach (var parameter in parsedTemplate.Parameters)\\n        {\\n            if (parameter.DefaultValue != null)\\n            {\\n                result.Add(parameter.Name, parameter.DefaultValue);\\n            }\\n        }\\n\\n        return result;\\n    }\\n}\\n```\\n\\nFortunately, the necessary framework classes (`TemplateParser` and `TemplateMatcher`) are public.\\n\\n### Some more detail\\n\\nThere are quite a handful of classes participating in the routing process, I needed to debug the code and step over the different calls to wrap my head around them. On a high level this is what happening during setting up our routes.\\n\\n![Set up routing in ASP.NET](/content/images/2016/06/aspnet-routing.png)\\n\\nWhen we specify a route by calling `RouteBuilder.MapVerb` a new `Route` is created, and added to the builder's collection. The `Route` object has a delegate to the handler processing the request and producing the response.\\nIf we use `UseMvc`, then the routes are automatically created based on the action methods we have (this is done by a method called `CreateAttributeMegaRoute` :)), and then `UseRouter` is called.\\n  \\nWhen we call the `UseRouter` extension method, a middleware is registered called `RouterMiddleware`, to which a `RouteCollection` is passed with the configured routes.\\n\\nThis middleware is added to the pipeline, and its `Invoke` method is called during request processing. This method checks if any of the route templates match, and then calls the appropriate handler.\\n\\nAt this point it wasn't much more work to find the classes doing the actual route matching and extract them into my own codebase.\"}]],\"markups\":[],\"sections\":[[10,0]]}",
        "html": "<p>We can use routing in ASP.NET to define paths on which we want to respond to HTTP requests. In ASP.NET Core we have two common ways to specify routing in our application.</p>\n<p>We can use the <code>Route</code> attribute on the action methods:</p>\n<pre><code class=\"language-csharp\">[HttpGet(&quot;test/{myParam}&quot;]\npublic IActionResult Get(int myParam)\n{\n    // ...\n}\n</code></pre>\n<p>Or if we don't want to use MVC, we can directly set up some responses in our <code>Startup</code> class by creating a <code>RouteBuilder</code> and adding it to the pipeline with the <code>UseRouter</code> method.</p>\n<pre><code class=\"language-csharp\">RouteBuilder builder = new RouteBuilder(app);\n\nbuilder.MapVerb(\n    HttpMethod.Get.Method,\n    &quot;test/{myParam}&quot;,\n    async context =&gt;\n    {\n        var arg = context.GetRouteData().Values[&quot;myParam&quot;];\n\n        // ...\n    });\n\napp.UseRouter(builder.Build());\n</code></pre>\n<h2 id=\"usingroutematchingmanually\">Using route matching manually</h2>\n<p>In usual development the methods above are enough, since most often we'll use MVC to implement our application. On the other hand sometimes it can be handy if we're able to manually match a URL path to a template and extract the arguments.</p>\n<p>Recently I needed to do this in the <a href=\"https://github.com/markvincze/Stubbery\">Stubbery</a> library, to set up stubbed replies for various request paths.</p>\n<p>I looked around in the ASP.NET Routing codebase quite a while in an attempt to figure out how the routing worked internally and whether the classes doing the template matching are  <code>public</code>, so I can use them in my code.</p>\n<h3 id=\"tldrversion\">TLDR version</h3>\n<p>You can manually do the template matching and extract the route arguments with this <a href=\"https://github.com/markvincze/Stubbery/blob/master/src/Stubbery/RequestMatching/RouteMatcher.cs\">little helper class</a>.</p>\n<pre><code class=\"language-csharp\">public class RouteMatcher\n{\n    public RouteValueDictionary Match(string routeTemplate, string requestPath)\n    {\n        var template = TemplateParser.Parse(routeTemplate);\n\n        var matcher = new TemplateMatcher(template, GetDefaults(template));\n\n        var values = matcher.Match(requestPath);\n\n        return values;\n    }\n\n    // This method extracts the default argument values from the template.\n    private RouteValueDictionary GetDefaults(RouteTemplate parsedTemplate)\n    {\n        var result = new RouteValueDictionary();\n\n        foreach (var parameter in parsedTemplate.Parameters)\n        {\n            if (parameter.DefaultValue != null)\n            {\n                result.Add(parameter.Name, parameter.DefaultValue);\n            }\n        }\n\n        return result;\n    }\n}\n</code></pre>\n<p>Fortunately, the necessary framework classes (<code>TemplateParser</code> and <code>TemplateMatcher</code>) are public.</p>\n<h3 id=\"somemoredetail\">Some more detail</h3>\n<p>There are quite a handful of classes participating in the routing process, I needed to debug the code and step over the different calls to wrap my head around them. On a high level this is what happening during setting up our routes.</p>\n<p><img src=\"/content/images/2016/06/aspnet-routing.png\" alt=\"Set up routing in ASP.NET\"></p>\n<p>When we specify a route by calling <code>RouteBuilder.MapVerb</code> a new <code>Route</code> is created, and added to the builder's collection. The <code>Route</code> object has a delegate to the handler processing the request and producing the response.<br>\nIf we use <code>UseMvc</code>, then the routes are automatically created based on the action methods we have (this is done by a method called <code>CreateAttributeMegaRoute</code> :)), and then <code>UseRouter</code> is called.</p>\n<p>When we call the <code>UseRouter</code> extension method, a middleware is registered called <code>RouterMiddleware</code>, to which a <code>RouteCollection</code> is passed with the configured routes.</p>\n<p>This middleware is added to the pipeline, and its <code>Invoke</code> method is called during request processing. This method checks if any of the route templates match, and then calls the appropriate handler.</p>\n<p>At this point it wasn't much more work to find the classes doing the actual route matching and extract them into my own codebase.</p>\n",
        "comment_id": "21",
        "plaintext": "We can use routing in ASP.NET to define paths on which we want to respond to\nHTTP requests. In ASP.NET Core we have two common ways to specify routing in our\napplication.\n\nWe can use the Route  attribute on the action methods:\n\n[HttpGet(\"test/{myParam}\"]\npublic IActionResult Get(int myParam)\n{\n    // ...\n}\n\n\nOr if we don't want to use MVC, we can directly set up some responses in our \nStartup  class by creating a RouteBuilder  and adding it to the pipeline with\nthe UseRouter  method.\n\nRouteBuilder builder = new RouteBuilder(app);\n\nbuilder.MapVerb(\n    HttpMethod.Get.Method,\n    \"test/{myParam}\",\n    async context =>\n    {\n        var arg = context.GetRouteData().Values[\"myParam\"];\n\n        // ...\n    });\n\napp.UseRouter(builder.Build());\n\n\nUsing route matching manually\nIn usual development the methods above are enough, since most often we'll use\nMVC to implement our application. On the other hand sometimes it can be handy if\nwe're able to manually match a URL path to a template and extract the arguments.\n\nRecently I needed to do this in the Stubbery\n[https://github.com/markvincze/Stubbery]  library, to set up stubbed replies for\nvarious request paths.\n\nI looked around in the ASP.NET Routing codebase quite a while in an attempt to\nfigure out how the routing worked internally and whether the classes doing the\ntemplate matching are public, so I can use them in my code.\n\nTLDR version\nYou can manually do the template matching and extract the route arguments with\nthis little helper class\n[https://github.com/markvincze/Stubbery/blob/master/src/Stubbery/RequestMatching/RouteMatcher.cs]\n.\n\npublic class RouteMatcher\n{\n    public RouteValueDictionary Match(string routeTemplate, string requestPath)\n    {\n        var template = TemplateParser.Parse(routeTemplate);\n\n        var matcher = new TemplateMatcher(template, GetDefaults(template));\n\n        var values = matcher.Match(requestPath);\n\n        return values;\n    }\n\n    // This method extracts the default argument values from the template.\n    private RouteValueDictionary GetDefaults(RouteTemplate parsedTemplate)\n    {\n        var result = new RouteValueDictionary();\n\n        foreach (var parameter in parsedTemplate.Parameters)\n        {\n            if (parameter.DefaultValue != null)\n            {\n                result.Add(parameter.Name, parameter.DefaultValue);\n            }\n        }\n\n        return result;\n    }\n}\n\n\nFortunately, the necessary framework classes (TemplateParser  and \nTemplateMatcher) are public.\n\nSome more detail\nThere are quite a handful of classes participating in the routing process, I\nneeded to debug the code and step over the different calls to wrap my head\naround them. On a high level this is what happening during setting up our\nroutes.\n\n\n\nWhen we specify a route by calling RouteBuilder.MapVerb  a new Route  is\ncreated, and added to the builder's collection. The Route  object has a delegate\nto the handler processing the request and producing the response.\nIf we use UseMvc, then the routes are automatically created based on the action\nmethods we have (this is done by a method called CreateAttributeMegaRoute  :)),\nand then UseRouter  is called.\n\nWhen we call the UseRouter  extension method, a middleware is registered called \nRouterMiddleware, to which a RouteCollection  is passed with the configured\nroutes.\n\nThis middleware is added to the pipeline, and its Invoke  method is called\nduring request processing. This method checks if any of the route templates\nmatch, and then calls the appropriate handler.\n\nAt this point it wasn't much more work to find the classes doing the actual\nroute matching and extract them into my own codebase.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "It can come handy to manually match a request path to route templates, and extract the arguments. This post describes how it can be done with ASP.NET Core.",
        "author_id": "1",
        "created_at": "2016-06-18 18:17:47",
        "created_by": "1",
        "updated_at": "2019-01-12 11:37:38",
        "updated_by": "1",
        "published_at": "2016-06-18 21:17:15",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de296f",
        "uuid": "68fa7576-4ff1-445a-8e12-435429fca961",
        "title": "Setting up a Travis-CI pipeline for Golang",
        "slug": "setting-up-a-travis-ci-pipeline-for-golang",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In the [previous post](/setting-up-an-appveyor-pipeline-for-golang) we looked at how we can set up a pipeline in AppVeyor for building and releasing a Golang application. Recently I made some changes to the project I'm working on, which [prevents the it to be cross-compiled on a Windows build agent](https://github.com/rjeczalik/notify/issues/108). After the change I was able to properly do the build only on an OSX machine.\\n\\nThis caused a problem for me, since AppVeyor only supports using a Windows machine as the build agent, so I wasn't able to properly cross compile the application any more.\\n\\nAnother really popular CI system for open source projects is Travis-CI. It works very similarly to AppVeyor, and its basic concepts are the same, but it's more tailored for building applications on a Linux-based environments. Luckily, it also supports using an OSX build agent, which is appropriate for my purposes.\\n\\nSince I've written a summary about setting up a Golang pipeline for AppVeyor, it just seemed logical to do the same for Travis.\\n\\n## Setting up the build\\n\\nSimilarly to AppVeyor, the normal way to use Travis to build a project is to include a yaml file at the root of the repository with the name `.travis.yml`, in which we can specify how we would like to compile our project, and we can also set up a configuration for releasing it.\\n\\n### Environment\\n\\nBy default Travis is using an Ubuntu virtual machine to run the build. If Linux is an appropriate environment for your build, you can probably use the default settings. (I would only recommend using an OSX agent if there is a real need for it, since the Linux environment is faster and boots up much quicker.) You can find all the detailed information about the different environments you can use in [the documentation](https://docs.travis-ci.com/user/ci-environment/).\\n\\nSince I wanted to use an OSX agent, I had to add the following line to the top of my `.travis.yml`.\\n\\n```yaml\\nos: osx\\n```\\n\\nThe next thing to set up is the programming language which is needed to build our application. In the case of Go, the following settings are needed. \\n\\n```yaml\\nlanguage: go\\n\\ngo:\\n- tip # The latest version of Go.\\n```\\n\\nWe can either set the version of the tools to a fix version, like `1.3`, or we can use `tip` to always get the latest version. We can find more options in [the docs](https://docs.travis-ci.com/user/languages/go).\\n\\n### Build\\n\\nThe next step is to configure the actual build. Similarly to my example for AppVeyor, I'm not setting up the actual `go build` commands here, but I have a separate build script file in my repository, and I'm just calling that script from Travis.\\n\\n```yaml\\nscript:\\n- \\\"./build.sh\\\"\\n```\\n\\nThe script builds the source code and puts the output binary in the `bin` folder, it looks something like this (This is a simplified example.)\\n\\n```bash\\n#!/bin/bash\\n\\ngo build -o bin/myawesomeapp -i .\\n``` \\n\\n### Deployment\\n\\nThe last thing to do is to configure how to deploy the application. This depends on what kind of project we're working on. The application I'm developing is a client side CLI tool. Of course the deployment would be very different for a  let's say  web application.\\n\\nSimilarly to AppVeyor, Travis has built-in deployment support for a [wide range](https://docs.travis-ci.com/user/deployment) of different systems. It includes support for GitHub Releases too, which I was already using.\\n\\nTo set up a GitHub release, we have to add the following section to our yaml.\\n\\n```yaml\\ndeploy:\\n  provider: releases\\n  skip_cleanup: true # Important, otherwise the build output would be purged.\\n  api_key:\\n    secure: lFGBaF...SJ1lDPDICY=\\n  file: bin/myawesomeapp\\n  on:\\n    repo: account/myawesomeproject\\n    tags: true # The deployment happens only if the commit has a tag.\\n``` \\n\\n#### GitHub authentication\\n\\nThere are [multiple ways](https://docs.travis-ci.com/user/deployment/releases/) to configure the `api_key` that will be used to authenticate AppVeyor to do the release, and we can also use a username/password combination (which is of course not a good idea if our yaml file will be public).\\n\\nThe simplest option (which is also recommended by the docs) is to [install the Travis CLI](https://github.com/travis-ci/travis.rb#installation), then step into the directory of our repository, and issue the `setup releases` command.\\n\\n```bash\\ntravis setup releases\\n```\\n\\nThis will prompt us to log into GitHub, then it generates an ApiKey, encrypts it and automatically adds it to our `.travis.yml` file.\\n\\nThe `file` property specifies the binary we want to upload to the GitHub release. We can also upload multiple files.\\n\\n```yaml\\n  file:\\n    - file1\\n    - file2\\n    - file3\\n```\\n\\nIn the `repo` field we have to enter our GitHub account name, and the name of our project (basically the route in the URL if we open our repository on GitHub).\\n\\nAnd if we set `tags` to true, then the deployment only happens for commits which have tags, and the name of the release will be the name of the tag. This is nice if we would like to explicitly control when we're releasing and what version number we want to use, which is a good opportunity to enforce conscious semantic versioning.\\n\\n## Full example\\n\\nThis is the full `.travis.yml` file we have to put into our repo to get a build and release working.\\n\\n```yaml\\nos: osx # Only use OSX if it's really needed!\\n\\nlanguage: go\\n\\ngo:\\n- tip # The latest version of Go.\\n\\nscript:\\n- \\\"./build.sh\\\"\\n\\ndeploy:\\n  provider: releases\\n  skip_cleanup: true # Important, otherwise the build output would be purged.\\n  api_key:\\n    secure: lFGBaF...SJ1lDPDICY=\\n  file: bin/myawesomeapp\\n  on:\\n    repo: account/myawesomeproject\\n    tags: true # The deployment happens only if the commit has a tag.\\n```\\n\\nWith this configuration the build itself runs for every commit and PR (which is nice, because we see if everything compiles fine), but the deployment only happens when we actually push a tag as well containing the version number of the new release.\\n\\nSetting up Travis turned out to be a very similar experience to using AppVeyor, so the decision whether to use one or the other boils down to whether a Windows or a Linux (or OSX) build environment is more appropriate for our scenario.\"}]],\"sections\":[[10,0]]}",
        "html": "<p>In the <a href=\"/setting-up-an-appveyor-pipeline-for-golang\">previous post</a> we looked at how we can set up a pipeline in AppVeyor for building and releasing a Golang application. Recently I made some changes to the project I'm working on, which <a href=\"https://github.com/rjeczalik/notify/issues/108\">prevents the it to be cross-compiled on a Windows build agent</a>. After the change I was able to properly do the build only on an OSX machine.</p>\n<p>This caused a problem for me, since AppVeyor only supports using a Windows machine as the build agent, so I wasn't able to properly cross compile the application any more.</p>\n<p>Another really popular CI system for open source projects is Travis-CI. It works very similarly to AppVeyor, and its basic concepts are the same, but it's more tailored for building applications on a Linux-based environments. Luckily, it also supports using an OSX build agent, which is appropriate for my purposes.</p>\n<p>Since I've written a summary about setting up a Golang pipeline for AppVeyor, it just seemed logical to do the same for Travis.</p>\n<h2 id=\"settingupthebuild\">Setting up the build</h2>\n<p>Similarly to AppVeyor, the normal way to use Travis to build a project is to include a yaml file at the root of the repository with the name <code>.travis.yml</code>, in which we can specify how we would like to compile our project, and we can also set up a configuration for releasing it.</p>\n<h3 id=\"environment\">Environment</h3>\n<p>By default Travis is using an Ubuntu virtual machine to run the build. If Linux is an appropriate environment for your build, you can probably use the default settings. (I would only recommend using an OSX agent if there is a real need for it, since the Linux environment is faster and boots up much quicker.) You can find all the detailed information about the different environments you can use in <a href=\"https://docs.travis-ci.com/user/ci-environment/\">the documentation</a>.</p>\n<p>Since I wanted to use an OSX agent, I had to add the following line to the top of my <code>.travis.yml</code>.</p>\n<pre><code class=\"language-yaml\">os: osx\n</code></pre>\n<p>The next thing to set up is the programming language which is needed to build our application. In the case of Go, the following settings are needed.</p>\n<pre><code class=\"language-yaml\">language: go\n\ngo:\n- tip # The latest version of Go.\n</code></pre>\n<p>We can either set the version of the tools to a fix version, like <code>1.3</code>, or we can use <code>tip</code> to always get the latest version. We can find more options in <a href=\"https://docs.travis-ci.com/user/languages/go\">the docs</a>.</p>\n<h3 id=\"build\">Build</h3>\n<p>The next step is to configure the actual build. Similarly to my example for AppVeyor, I'm not setting up the actual <code>go build</code> commands here, but I have a separate build script file in my repository, and I'm just calling that script from Travis.</p>\n<pre><code class=\"language-yaml\">script:\n- &quot;./build.sh&quot;\n</code></pre>\n<p>The script builds the source code and puts the output binary in the <code>bin</code> folder, it looks something like this (This is a simplified example.)</p>\n<pre><code class=\"language-bash\">#!/bin/bash\n\ngo build -o bin/myawesomeapp -i .\n</code></pre>\n<h3 id=\"deployment\">Deployment</h3>\n<p>The last thing to do is to configure how to deploy the application. This depends on what kind of project we're working on. The application I'm developing is a client side CLI tool. Of course the deployment would be very different for a  let's say  web application.</p>\n<p>Similarly to AppVeyor, Travis has built-in deployment support for a <a href=\"https://docs.travis-ci.com/user/deployment\">wide range</a> of different systems. It includes support for GitHub Releases too, which I was already using.</p>\n<p>To set up a GitHub release, we have to add the following section to our yaml.</p>\n<pre><code class=\"language-yaml\">deploy:\n  provider: releases\n  skip_cleanup: true # Important, otherwise the build output would be purged.\n  api_key:\n    secure: lFGBaF...SJ1lDPDICY=\n  file: bin/myawesomeapp\n  on:\n    repo: account/myawesomeproject\n    tags: true # The deployment happens only if the commit has a tag.\n</code></pre>\n<h4 id=\"githubauthentication\">GitHub authentication</h4>\n<p>There are <a href=\"https://docs.travis-ci.com/user/deployment/releases/\">multiple ways</a> to configure the <code>api_key</code> that will be used to authenticate AppVeyor to do the release, and we can also use a username/password combination (which is of course not a good idea if our yaml file will be public).</p>\n<p>The simplest option (which is also recommended by the docs) is to <a href=\"https://github.com/travis-ci/travis.rb#installation\">install the Travis CLI</a>, then step into the directory of our repository, and issue the <code>setup releases</code> command.</p>\n<pre><code class=\"language-bash\">travis setup releases\n</code></pre>\n<p>This will prompt us to log into GitHub, then it generates an ApiKey, encrypts it and automatically adds it to our <code>.travis.yml</code> file.</p>\n<p>The <code>file</code> property specifies the binary we want to upload to the GitHub release. We can also upload multiple files.</p>\n<pre><code class=\"language-yaml\">  file:\n    - file1\n    - file2\n    - file3\n</code></pre>\n<p>In the <code>repo</code> field we have to enter our GitHub account name, and the name of our project (basically the route in the URL if we open our repository on GitHub).</p>\n<p>And if we set <code>tags</code> to true, then the deployment only happens for commits which have tags, and the name of the release will be the name of the tag. This is nice if we would like to explicitly control when we're releasing and what version number we want to use, which is a good opportunity to enforce conscious semantic versioning.</p>\n<h2 id=\"fullexample\">Full example</h2>\n<p>This is the full <code>.travis.yml</code> file we have to put into our repo to get a build and release working.</p>\n<pre><code class=\"language-yaml\">os: osx # Only use OSX if it's really needed!\n\nlanguage: go\n\ngo:\n- tip # The latest version of Go.\n\nscript:\n- &quot;./build.sh&quot;\n\ndeploy:\n  provider: releases\n  skip_cleanup: true # Important, otherwise the build output would be purged.\n  api_key:\n    secure: lFGBaF...SJ1lDPDICY=\n  file: bin/myawesomeapp\n  on:\n    repo: account/myawesomeproject\n    tags: true # The deployment happens only if the commit has a tag.\n</code></pre>\n<p>With this configuration the build itself runs for every commit and PR (which is nice, because we see if everything compiles fine), but the deployment only happens when we actually push a tag as well containing the version number of the new release.</p>\n<p>Setting up Travis turned out to be a very similar experience to using AppVeyor, so the decision whether to use one or the other boils down to whether a Windows or a Linux (or OSX) build environment is more appropriate for our scenario.</p>\n",
        "comment_id": "25",
        "plaintext": "In the previous post [/setting-up-an-appveyor-pipeline-for-golang]  we looked at\nhow we can set up a pipeline in AppVeyor for building and releasing a Golang\napplication. Recently I made some changes to the project I'm working on, which \nprevents the it to be cross-compiled on a Windows build agent\n[https://github.com/rjeczalik/notify/issues/108]. After the change I was able to\nproperly do the build only on an OSX machine.\n\nThis caused a problem for me, since AppVeyor only supports using a Windows\nmachine as the build agent, so I wasn't able to properly cross compile the\napplication any more.\n\nAnother really popular CI system for open source projects is Travis-CI. It works\nvery similarly to AppVeyor, and its basic concepts are the same, but it's more\ntailored for building applications on a Linux-based environments. Luckily, it\nalso supports using an OSX build agent, which is appropriate for my purposes.\n\nSince I've written a summary about setting up a Golang pipeline for AppVeyor, it\njust seemed logical to do the same for Travis.\n\nSetting up the build\nSimilarly to AppVeyor, the normal way to use Travis to build a project is to\ninclude a yaml file at the root of the repository with the name .travis.yml, in\nwhich we can specify how we would like to compile our project, and we can also\nset up a configuration for releasing it.\n\nEnvironment\nBy default Travis is using an Ubuntu virtual machine to run the build. If Linux\nis an appropriate environment for your build, you can probably use the default\nsettings. (I would only recommend using an OSX agent if there is a real need for\nit, since the Linux environment is faster and boots up much quicker.) You can\nfind all the detailed information about the different environments you can use\nin the documentation [https://docs.travis-ci.com/user/ci-environment/].\n\nSince I wanted to use an OSX agent, I had to add the following line to the top\nof my .travis.yml.\n\nos: osx\n\n\nThe next thing to set up is the programming language which is needed to build\nour application. In the case of Go, the following settings are needed.\n\nlanguage: go\n\ngo:\n- tip # The latest version of Go.\n\n\nWe can either set the version of the tools to a fix version, like 1.3, or we can\nuse tip  to always get the latest version. We can find more options in the docs\n[https://docs.travis-ci.com/user/languages/go].\n\nBuild\nThe next step is to configure the actual build. Similarly to my example for\nAppVeyor, I'm not setting up the actual go build  commands here, but I have a\nseparate build script file in my repository, and I'm just calling that script\nfrom Travis.\n\nscript:\n- \"./build.sh\"\n\n\nThe script builds the source code and puts the output binary in the bin  folder,\nit looks something like this (This is a simplified example.)\n\n#!/bin/bash\n\ngo build -o bin/myawesomeapp -i .\n\n\nDeployment\nThe last thing to do is to configure how to deploy the application. This depends\non what kind of project we're working on. The application I'm developing is a\nclient side CLI tool. Of course the deployment would be very different for a \nlet's say  web application.\n\nSimilarly to AppVeyor, Travis has built-in deployment support for a wide range\n[https://docs.travis-ci.com/user/deployment]  of different systems. It includes\nsupport for GitHub Releases too, which I was already using.\n\nTo set up a GitHub release, we have to add the following section to our yaml.\n\ndeploy:\n  provider: releases\n  skip_cleanup: true # Important, otherwise the build output would be purged.\n  api_key:\n    secure: lFGBaF...SJ1lDPDICY=\n  file: bin/myawesomeapp\n  on:\n    repo: account/myawesomeproject\n    tags: true # The deployment happens only if the commit has a tag.\n\n\nGitHub authentication\nThere are multiple ways [https://docs.travis-ci.com/user/deployment/releases/] \nto configure the api_key  that will be used to authenticate AppVeyor to do the\nrelease, and we can also use a username/password combination (which is of course\nnot a good idea if our yaml file will be public).\n\nThe simplest option (which is also recommended by the docs) is to install the\nTravis CLI, then step into the directory of our repository, and issue the setup\nreleases  command.\n\ntravis setup releases\n\n\nThis will prompt us to log into GitHub, then it generates an ApiKey, encrypts it\nand automatically adds it to our .travis.yml  file.\n\nThe file  property specifies the binary we want to upload to the GitHub release.\nWe can also upload multiple files.\n\n  file:\n    - file1\n    - file2\n    - file3\n\n\nIn the repo  field we have to enter our GitHub account name, and the name of our\nproject (basically the route in the URL if we open our repository on GitHub).\n\nAnd if we set tags  to true, then the deployment only happens for commits which\nhave tags, and the name of the release will be the name of the tag. This is nice\nif we would like to explicitly control when we're releasing and what version\nnumber we want to use, which is a good opportunity to enforce conscious semantic\nversioning.\n\nFull example\nThis is the full .travis.yml  file we have to put into our repo to get a build\nand release working.\n\nos: osx # Only use OSX if it's really needed!\n\nlanguage: go\n\ngo:\n- tip # The latest version of Go.\n\nscript:\n- \"./build.sh\"\n\ndeploy:\n  provider: releases\n  skip_cleanup: true # Important, otherwise the build output would be purged.\n  api_key:\n    secure: lFGBaF...SJ1lDPDICY=\n  file: bin/myawesomeapp\n  on:\n    repo: account/myawesomeproject\n    tags: true # The deployment happens only if the commit has a tag.\n\n\nWith this configuration the build itself runs for every commit and PR (which is\nnice, because we see if everything compiles fine), but the deployment only\nhappens when we actually push a tag as well containing the version number of the\nnew release.\n\nSetting up Travis turned out to be a very similar experience to using AppVeyor,\nso the decision whether to use one or the other boils down to whether a Windows\nor a Linux (or OSX) build environment is more appropriate for our scenario.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "This post gives and Introduction to setting up a continuous delivery pipeline for a Golang-based project in Travis-CI.",
        "author_id": "1",
        "created_at": "2016-10-15 15:30:53",
        "created_by": "1",
        "updated_at": "2016-10-30 14:55:55",
        "updated_by": "1",
        "published_at": "2016-10-15 15:36:14",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2970",
        "uuid": "3751d0fe-7c81-443a-ac09-90dbb1e4a118",
        "title": "Programmatically refreshing a browser tab from a Golang application",
        "slug": "programmatically-refreshing-a-browser-tab-from-a-golang-application",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"# Introduction\\n\\nAt work I've been working on a client-side Golang application (a command-line tool), which is used as part of the development toolchain we're using at the company.\\n\\nThis application is used from the command line to upload packages to our development web server, which is then opened in the browser.\\n\\nInstead of opening our development site in a new tab every time, I wanted to programmatically refresh the browser tab if one has already been opened.\\n\\nI initially expected this to be a pretty easy task, but after some googling I had to realize it's not trivial at all.\\nApparently, there is no way to programmatically connect to the browser, query the list of tabs open, find a particular tab, and refresh it. (Especially not in a cross-browser and cross-platform way.)\\n\\nAfter searching and asking around, I was pointed to a technology called livereload, which is both a [development tool](http://livereload.com/) and an [npm package](https://www.npmjs.com/package/livereload) for automatically refreshing the browser when editing some HTML content, or when new HTML content is being generated during the developing a website.\\n\\nThe way livereload works is that a component is hosting a small WebSocket service to which the browser can connect. It is also running a file watcher watching all the content (HTML, CSS, JavaScript, etc.) which should trigger a browser refresh when changed.\\nThen a small piece of JavaScript code in the browser connects to the WebSocket server, and refreshes the tab every time it receives a message.\\n\\nThe feature I wanted to implement is a bit different: I don't want to watch a particular folder containing some files and refresh the browser on changes. What I want to do is be able to programmatically trigger a refresh from code.\\n\\nIt turned out there is no simpler way to achieve this than to utilize the same approach which is used by `livereload`: host a small WebSocket endpoint in my Golang app, connect to it from the browser, and send a message every time I want to refresh the page.\\n\\nImplementing this in Golang ended up being not too difficult, although there are a couple of gotchas you have to watch out for if your site is served over HTTPS.\\n\\n# Implementing the reload server\\n\\nThe Websocket service we want to implement is very simple: the client (the browser) never initiates communication, it's only the server (the Golang app) that sends a message when the page has to be refreshed. We need only a single message type (with no arguments), since the only action we want to implement is the reload.\\n\\nThe library [`websocket`](https://github.com/gorilla/websocket) from the [Gorilla web toolkit](http://www.gorillatoolkit.org/) can be used to implement the endpoint. I based my implementation on the [Chat example](https://github.com/gorilla/websocket/tree/master/examples/chat) provided by the library (basically I tried to trim it down as much as possible, so it only contains the parts necessary for my purposes).\\n\\nTo host a WebSocket endpoint, we need to have some boilerplate to manage the client connections and to send messages. I took the following two files from the [Chat example](https://github.com/gorilla/websocket/tree/master/examples/chat) without much modification.\\n\\nThe first helper file is `wsClient.go`, which is responsible for the low level WebSocket communication.\\n\\n```go\\npackage main\\n\\nimport (\\n    \\\"log\\\"\\n    \\\"net/http\\\"\\n    \\\"time\\\"\\n\\n    \\\"github.com/gorilla/websocket\\\"\\n)\\n\\nconst (\\n    // Time allowed to write a message to the peer.\\n    writeWait = 10 * time.Second\\n\\n    // Time allowed to read the next pong message from the peer.\\n    pongWait = 60 * time.Second\\n\\n    // Send pings to peer with this period. Must be less than pongWait.\\n    pingPeriod = (pongWait * 9) / 10\\n)\\n\\nvar (\\n    newline = []byte{'\\\\n'}\\n    space   = []byte{' '}\\n)\\n\\nvar upgrader = websocket.Upgrader{\\n    ReadBufferSize:  1024,\\n    WriteBufferSize: 1024,\\n    CheckOrigin: func(r *http.Request) bool {\\n        return true\\n    },\\n}\\n\\n// Client is an middleman between the websocket connection and the hub.\\ntype Client struct {\\n    hub *Hub\\n\\n    // The websocket connection.\\n    conn *websocket.Conn\\n\\n    // Buffered channel of outbound messages.\\n    send chan []byte\\n}\\n\\n// readPump pumps messages from the websocket connection to the hub.\\nfunc (c *Client) readPump() {\\n    defer func() {\\n        c.hub.unregister <- c\\n        c.conn.Close()\\n    }()\\n    for {\\n        _, _, err := c.conn.ReadMessage()\\n        if err != nil {\\n            if websocket.IsUnexpectedCloseError(err, websocket.CloseGoingAway) {\\n                log.Printf(\\\"An error happened when reading from the Websocket client: %v\\\", err)\\n            }\\n            break\\n        }\\n    }\\n}\\n\\n// write writes a message with the given message type and payload.\\nfunc (c *Client) write(mt int, payload []byte) error {\\n    c.conn.SetWriteDeadline(time.Now().Add(writeWait))\\n    return c.conn.WriteMessage(mt, payload)\\n}\\n\\n// writePump pumps messages from the hub to the websocket connection.\\nfunc (c *Client) writePump() {\\n    ticker := time.NewTicker(pingPeriod)\\n    defer func() {\\n        ticker.Stop()\\n        c.conn.Close()\\n    }()\\n    for {\\n        select {\\n        case message, ok := <-c.send:\\n            if !ok {\\n                // The hub closed the channel.\\n                c.write(websocket.CloseMessage, []byte{})\\n                return\\n            }\\n\\n            c.conn.SetWriteDeadline(time.Now().Add(writeWait))\\n            w, err := c.conn.NextWriter(websocket.TextMessage)\\n            if err != nil {\\n                return\\n            }\\n            w.Write(message)\\n\\n            n := len(c.send)\\n            for i := 0; i < n; i++ {\\n                w.Write(newline)\\n                w.Write(<-c.send)\\n            }\\n\\n            if err := w.Close(); err != nil {\\n                return\\n            }\\n        case <-ticker.C:\\n            if err := c.write(websocket.PingMessage, []byte{}); err != nil {\\n                return\\n            }\\n        }\\n    }\\n}\\n\\n// serveWs handles websocket requests from the peer.\\nfunc serveWs(hub *Hub, w http.ResponseWriter, r *http.Request) {\\n    conn, err := upgrader.Upgrade(w, r, nil)\\n    if err != nil {\\n        log.Println(err)\\n        return\\n    }\\n    client := &Client{hub: hub, conn: conn, send: make(chan []byte, 256)}\\n    client.hub.register <- client\\n    go client.writePump()\\n    client.readPump()\\n}\\n```\\n\\nSince this was originally implemented to support the two-way communication in a Chat application, probably it could be trimmed down even more.\\nThe second file, `wsHub.go` takes care of managing the list of client connections.\\n\\n```go\\npackage main\\n\\n// Hub maintains the set of active clients and broadcasts messages to the clients.\\ntype Hub struct {\\n    // Registered clients.\\n    clients map[*Client]bool\\n\\n    // Inbound messages from the clients.\\n    broadcast chan []byte\\n\\n    // Register requests from the clients.\\n    register chan *Client\\n\\n    // Unregister requests from clients.\\n    unregister chan *Client\\n}\\n\\nfunc newHub() *Hub {\\n    return &Hub{\\n        broadcast:  make(chan []byte),\\n        register:   make(chan *Client),\\n        unregister: make(chan *Client),\\n        clients:    make(map[*Client]bool),\\n    }\\n}\\n\\nfunc (h *Hub) run() {\\n    for {\\n        select {\\n        case client := <-h.register:\\n            h.clients[client] = true\\n        case client := <-h.unregister:\\n            if _, ok := h.clients[client]; ok {\\n                delete(h.clients, client)\\n                close(client.send)\\n            }\\n        case message := <-h.broadcast:\\n            for client := range h.clients {\\n                select {\\n                case client.send <- message:\\n                default:\\n                    close(client.send)\\n                    delete(h.clients, client)\\n                }\\n            }\\n        }\\n    }\\n}\\n```\\n\\nWith these helpers in place we can start up our actual WS endpoint.\\n\\n```go\\npackage main\\n\\nimport (\\n    \\\"bytes\\\"\\n    \\\"io/ioutil\\\"\\n    \\\"log\\\"\\n    \\\"net/http\\\"\\n)\\n\\nvar (\\n    hub *Hub\\n    // The port on which we are hosting the reload server has to be hardcoded on the client-side too.\\n    reloadAddress    = \\\":12450\\\"\\n)\\n\\nfunc startReloadServer() {\\n    hub = newHub()\\n    go hub.run()\\n    http.HandleFunc(\\\"/reload\\\", func(w http.ResponseWriter, r *http.Request) {\\n        serveWs(hub, w, r)\\n    })\\n\\n    go startServer()\\n    log.Println(\\\"Reload server listening at\\\", reloadAddress)\\n}\\n\\nfunc startServer() {\\n    err := http.ListenAndServe(reloadAddress, nil)\\n\\n    if err != nil {\\n        log.Println(\\\"Failed to start up the Reload server: \\\", err)\\n        return\\n    }\\n}\\n```\\n\\nIn this example I'm hosting the service on the port `12450`, which I randomly picked from the unassigned ports in the [registry maintained by IANA](http://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?&page=1). You can pick another port for your application, which can then be hardcoded into both the service and the client.\\n\\nCalling the function `startReloadServer` at the beginning of our application will start hosting the WebSocket endpoint, and it'll keep running until our app terminates.\\n\\nThen we can implement the function that will send the reload message to the browser. This is the function we have to call when we want to reload the browser.\\n\\n```go\\nfunc sendReload() {\\n    message := bytes.TrimSpace([]byte(\\\"reload\\\"))\\n    hub.broadcast <- message\\n}\\n```\\n\\nIn the example I'm sending the string `\\\"reload\\\"`, which doesn't have any role, the client won't interpret it at all, since the only function we have is reloading, which doesn't need any parameters. If we needed anything more complicated, here we could send an arbitrary message to the browser, which we can then process in JavaScript.\\n\\nWith all these building blocks in place the only thing we have to do is call `startReloadServer` to start hosting the service, and then call `sendReload` every time we want to refresh the browser.\\n\\n```go\\nstartReloadServer()\\n\\n...\\n\\nsendReload()\\n```\\n\\n# Connecting to the Reload service\\n\\nConnecting the website to the endpoint is pretty simple, but we have to keep in mind that when we open the site in the browser, our app hosting the endpoint might not run yet (or it might be stopped and restarted later). So if our site cannot connect initially, we need to periodically retry.\\n\\nThis can be done with the following code.\\n\\n```js\\nfunction tryConnectToReload(address) {\\n  var conn;\\n  // This is a statically defined port on which the app is hosting the reload service.\\n  conn = new WebSocket(\\\"ws://localhost:12450/reload\\\");\\n\\n  conn.onclose = function(evt) {\\n    // The reload endpoint hasn't been started yet, we are retrying in 2 seconds.\\n    setTimeout(() => tryConnectToReload(), 2000);\\n  };\\n\\n  conn.onmessage = function(evt) {\\n    console.log(\\\"Refresh received!\\\");\\n\\n    // If we uncomment this line, then the page will refresh every time a message is received.\\n    //location.reload()\\n  };\\n}\\n\\ntry {\\n  if (window[\\\"WebSocket\\\"]) {\\n    tryConnectToReload();\\n  } else {\\n    console.log(\\\"Your browser does not support WebSocket, cannot connect to the reload service.\\\");\\n  }\\n} catch (ex) {\\n  console.log('Exception during connecting to reload:', ex);\\n}\\n```\\n\\nSo if we call `location.reload()` in the `onmessage` handler, then the browser will be refreshed every time we receive a message.\\n\\n# Problems with TLS\\n\\nThe above solution works perfectly as long as our website is served over plain HTTP.\\nThis is typically the case if it's a site under development hosted on `localhost`.\\n\\nOn the other hand, if we access the site through HTTPS, things are a bit more tricky.\\n\\nIf we try to connect from a website served over HTTPS to a WebSocket endpoint hosted without TLS (over `ws://`), then  depending on the browser and the operating system  we might get the following error.\\n\\n```\\nstartReload.js:24 Mixed Content: The page at 'https://my-dev-application.com/' was loaded over HTTPS, but attempted to connect to the insecure WebSocket endpoint 'ws://localhost:12450/reload'. This request has been blocked; this endpoint must be available over WSS.\\n```\\n\\nI didn't find any overview about exactly which browsers and systems produce this error. Based on my tests, this problem occurs for Chrome on Linux, but not on Windows nor OSX, and it also happens for Firefox, on every operating system I tried.\\n\\nI couldn't find a perfect solution to the problem, but there is a workaround that can at least mitigate the issue. What we can do is host the WebSocket endpoint on both `ws` and `wss`, and try to connect to both from the client (first to `ws`, and if that fails, then to `wss`).\\n\\nThis solves the problem for both Chrome and Firefox, but there is one more thing we have to do. Since we are hosting the service on localhost, there is no way to get a valid SSL certificate, and Chrome rejects the connection by default. In order to make it ignore certificate errors when connecting to `localhost`, we have to go to the advanced settings page in Chrome by navigating to `chrome://flags`, and we have to enable the following setting.\\n\\n![](/content/images/2016/10/chrome-ignore-localhost-cert.png)\\n\\nWith Firefox this didn't cause a problem, the connection worked properly after I started hosting the endpoint on `wss`. I'll update the post if I encounter any problem with it.\\n\\n# Source\\n\\nI uploaded the full working example to [GitHub](https://github.com/markvincze/golang-reload-browser), which also contain the implementation of hosting the reload endpoint on both WS and WSS, and also the code for the client side to establish the connection.\"}]],\"sections\":[[10,0]]}",
        "html": "<h1 id=\"introduction\">Introduction</h1>\n<p>At work I've been working on a client-side Golang application (a command-line tool), which is used as part of the development toolchain we're using at the company.</p>\n<p>This application is used from the command line to upload packages to our development web server, which is then opened in the browser.</p>\n<p>Instead of opening our development site in a new tab every time, I wanted to programmatically refresh the browser tab if one has already been opened.</p>\n<p>I initially expected this to be a pretty easy task, but after some googling I had to realize it's not trivial at all.<br>\nApparently, there is no way to programmatically connect to the browser, query the list of tabs open, find a particular tab, and refresh it. (Especially not in a cross-browser and cross-platform way.)</p>\n<p>After searching and asking around, I was pointed to a technology called livereload, which is both a <a href=\"http://livereload.com/\">development tool</a> and an <a href=\"https://www.npmjs.com/package/livereload\">npm package</a> for automatically refreshing the browser when editing some HTML content, or when new HTML content is being generated during the developing a website.</p>\n<p>The way livereload works is that a component is hosting a small WebSocket service to which the browser can connect. It is also running a file watcher watching all the content (HTML, CSS, JavaScript, etc.) which should trigger a browser refresh when changed.<br>\nThen a small piece of JavaScript code in the browser connects to the WebSocket server, and refreshes the tab every time it receives a message.</p>\n<p>The feature I wanted to implement is a bit different: I don't want to watch a particular folder containing some files and refresh the browser on changes. What I want to do is be able to programmatically trigger a refresh from code.</p>\n<p>It turned out there is no simpler way to achieve this than to utilize the same approach which is used by <code>livereload</code>: host a small WebSocket endpoint in my Golang app, connect to it from the browser, and send a message every time I want to refresh the page.</p>\n<p>Implementing this in Golang ended up being not too difficult, although there are a couple of gotchas you have to watch out for if your site is served over HTTPS.</p>\n<h1 id=\"implementingthereloadserver\">Implementing the reload server</h1>\n<p>The Websocket service we want to implement is very simple: the client (the browser) never initiates communication, it's only the server (the Golang app) that sends a message when the page has to be refreshed. We need only a single message type (with no arguments), since the only action we want to implement is the reload.</p>\n<p>The library <a href=\"https://github.com/gorilla/websocket\"><code>websocket</code></a> from the <a href=\"http://www.gorillatoolkit.org/\">Gorilla web toolkit</a> can be used to implement the endpoint. I based my implementation on the <a href=\"https://github.com/gorilla/websocket/tree/master/examples/chat\">Chat example</a> provided by the library (basically I tried to trim it down as much as possible, so it only contains the parts necessary for my purposes).</p>\n<p>To host a WebSocket endpoint, we need to have some boilerplate to manage the client connections and to send messages. I took the following two files from the <a href=\"https://github.com/gorilla/websocket/tree/master/examples/chat\">Chat example</a> without much modification.</p>\n<p>The first helper file is <code>wsClient.go</code>, which is responsible for the low level WebSocket communication.</p>\n<pre><code class=\"language-go\">package main\n\nimport (\n    &quot;log&quot;\n    &quot;net/http&quot;\n    &quot;time&quot;\n\n    &quot;github.com/gorilla/websocket&quot;\n)\n\nconst (\n    // Time allowed to write a message to the peer.\n    writeWait = 10 * time.Second\n\n    // Time allowed to read the next pong message from the peer.\n    pongWait = 60 * time.Second\n\n    // Send pings to peer with this period. Must be less than pongWait.\n    pingPeriod = (pongWait * 9) / 10\n)\n\nvar (\n    newline = []byte{'\\n'}\n    space   = []byte{' '}\n)\n\nvar upgrader = websocket.Upgrader{\n    ReadBufferSize:  1024,\n    WriteBufferSize: 1024,\n    CheckOrigin: func(r *http.Request) bool {\n        return true\n    },\n}\n\n// Client is an middleman between the websocket connection and the hub.\ntype Client struct {\n    hub *Hub\n\n    // The websocket connection.\n    conn *websocket.Conn\n\n    // Buffered channel of outbound messages.\n    send chan []byte\n}\n\n// readPump pumps messages from the websocket connection to the hub.\nfunc (c *Client) readPump() {\n    defer func() {\n        c.hub.unregister &lt;- c\n        c.conn.Close()\n    }()\n    for {\n        _, _, err := c.conn.ReadMessage()\n        if err != nil {\n            if websocket.IsUnexpectedCloseError(err, websocket.CloseGoingAway) {\n                log.Printf(&quot;An error happened when reading from the Websocket client: %v&quot;, err)\n            }\n            break\n        }\n    }\n}\n\n// write writes a message with the given message type and payload.\nfunc (c *Client) write(mt int, payload []byte) error {\n    c.conn.SetWriteDeadline(time.Now().Add(writeWait))\n    return c.conn.WriteMessage(mt, payload)\n}\n\n// writePump pumps messages from the hub to the websocket connection.\nfunc (c *Client) writePump() {\n    ticker := time.NewTicker(pingPeriod)\n    defer func() {\n        ticker.Stop()\n        c.conn.Close()\n    }()\n    for {\n        select {\n        case message, ok := &lt;-c.send:\n            if !ok {\n                // The hub closed the channel.\n                c.write(websocket.CloseMessage, []byte{})\n                return\n            }\n\n            c.conn.SetWriteDeadline(time.Now().Add(writeWait))\n            w, err := c.conn.NextWriter(websocket.TextMessage)\n            if err != nil {\n                return\n            }\n            w.Write(message)\n\n            n := len(c.send)\n            for i := 0; i &lt; n; i++ {\n                w.Write(newline)\n                w.Write(&lt;-c.send)\n            }\n\n            if err := w.Close(); err != nil {\n                return\n            }\n        case &lt;-ticker.C:\n            if err := c.write(websocket.PingMessage, []byte{}); err != nil {\n                return\n            }\n        }\n    }\n}\n\n// serveWs handles websocket requests from the peer.\nfunc serveWs(hub *Hub, w http.ResponseWriter, r *http.Request) {\n    conn, err := upgrader.Upgrade(w, r, nil)\n    if err != nil {\n        log.Println(err)\n        return\n    }\n    client := &amp;Client{hub: hub, conn: conn, send: make(chan []byte, 256)}\n    client.hub.register &lt;- client\n    go client.writePump()\n    client.readPump()\n}\n</code></pre>\n<p>Since this was originally implemented to support the two-way communication in a Chat application, probably it could be trimmed down even more.<br>\nThe second file, <code>wsHub.go</code> takes care of managing the list of client connections.</p>\n<pre><code class=\"language-go\">package main\n\n// Hub maintains the set of active clients and broadcasts messages to the clients.\ntype Hub struct {\n    // Registered clients.\n    clients map[*Client]bool\n\n    // Inbound messages from the clients.\n    broadcast chan []byte\n\n    // Register requests from the clients.\n    register chan *Client\n\n    // Unregister requests from clients.\n    unregister chan *Client\n}\n\nfunc newHub() *Hub {\n    return &amp;Hub{\n        broadcast:  make(chan []byte),\n        register:   make(chan *Client),\n        unregister: make(chan *Client),\n        clients:    make(map[*Client]bool),\n    }\n}\n\nfunc (h *Hub) run() {\n    for {\n        select {\n        case client := &lt;-h.register:\n            h.clients[client] = true\n        case client := &lt;-h.unregister:\n            if _, ok := h.clients[client]; ok {\n                delete(h.clients, client)\n                close(client.send)\n            }\n        case message := &lt;-h.broadcast:\n            for client := range h.clients {\n                select {\n                case client.send &lt;- message:\n                default:\n                    close(client.send)\n                    delete(h.clients, client)\n                }\n            }\n        }\n    }\n}\n</code></pre>\n<p>With these helpers in place we can start up our actual WS endpoint.</p>\n<pre><code class=\"language-go\">package main\n\nimport (\n    &quot;bytes&quot;\n    &quot;io/ioutil&quot;\n    &quot;log&quot;\n    &quot;net/http&quot;\n)\n\nvar (\n    hub *Hub\n    // The port on which we are hosting the reload server has to be hardcoded on the client-side too.\n    reloadAddress    = &quot;:12450&quot;\n)\n\nfunc startReloadServer() {\n    hub = newHub()\n    go hub.run()\n    http.HandleFunc(&quot;/reload&quot;, func(w http.ResponseWriter, r *http.Request) {\n        serveWs(hub, w, r)\n    })\n\n    go startServer()\n    log.Println(&quot;Reload server listening at&quot;, reloadAddress)\n}\n\nfunc startServer() {\n    err := http.ListenAndServe(reloadAddress, nil)\n\n    if err != nil {\n        log.Println(&quot;Failed to start up the Reload server: &quot;, err)\n        return\n    }\n}\n</code></pre>\n<p>In this example I'm hosting the service on the port <code>12450</code>, which I randomly picked from the unassigned ports in the <a href=\"http://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?&amp;page=1\">registry maintained by IANA</a>. You can pick another port for your application, which can then be hardcoded into both the service and the client.</p>\n<p>Calling the function <code>startReloadServer</code> at the beginning of our application will start hosting the WebSocket endpoint, and it'll keep running until our app terminates.</p>\n<p>Then we can implement the function that will send the reload message to the browser. This is the function we have to call when we want to reload the browser.</p>\n<pre><code class=\"language-go\">func sendReload() {\n    message := bytes.TrimSpace([]byte(&quot;reload&quot;))\n    hub.broadcast &lt;- message\n}\n</code></pre>\n<p>In the example I'm sending the string <code>&quot;reload&quot;</code>, which doesn't have any role, the client won't interpret it at all, since the only function we have is reloading, which doesn't need any parameters. If we needed anything more complicated, here we could send an arbitrary message to the browser, which we can then process in JavaScript.</p>\n<p>With all these building blocks in place the only thing we have to do is call <code>startReloadServer</code> to start hosting the service, and then call <code>sendReload</code> every time we want to refresh the browser.</p>\n<pre><code class=\"language-go\">startReloadServer()\n\n...\n\nsendReload()\n</code></pre>\n<h1 id=\"connectingtothereloadservice\">Connecting to the Reload service</h1>\n<p>Connecting the website to the endpoint is pretty simple, but we have to keep in mind that when we open the site in the browser, our app hosting the endpoint might not run yet (or it might be stopped and restarted later). So if our site cannot connect initially, we need to periodically retry.</p>\n<p>This can be done with the following code.</p>\n<pre><code class=\"language-js\">function tryConnectToReload(address) {\n  var conn;\n  // This is a statically defined port on which the app is hosting the reload service.\n  conn = new WebSocket(&quot;ws://localhost:12450/reload&quot;);\n\n  conn.onclose = function(evt) {\n    // The reload endpoint hasn't been started yet, we are retrying in 2 seconds.\n    setTimeout(() =&gt; tryConnectToReload(), 2000);\n  };\n\n  conn.onmessage = function(evt) {\n    console.log(&quot;Refresh received!&quot;);\n\n    // If we uncomment this line, then the page will refresh every time a message is received.\n    //location.reload()\n  };\n}\n\ntry {\n  if (window[&quot;WebSocket&quot;]) {\n    tryConnectToReload();\n  } else {\n    console.log(&quot;Your browser does not support WebSocket, cannot connect to the reload service.&quot;);\n  }\n} catch (ex) {\n  console.log('Exception during connecting to reload:', ex);\n}\n</code></pre>\n<p>So if we call <code>location.reload()</code> in the <code>onmessage</code> handler, then the browser will be refreshed every time we receive a message.</p>\n<h1 id=\"problemswithtls\">Problems with TLS</h1>\n<p>The above solution works perfectly as long as our website is served over plain HTTP.<br>\nThis is typically the case if it's a site under development hosted on <code>localhost</code>.</p>\n<p>On the other hand, if we access the site through HTTPS, things are a bit more tricky.</p>\n<p>If we try to connect from a website served over HTTPS to a WebSocket endpoint hosted without TLS (over <code>ws://</code>), then  depending on the browser and the operating system  we might get the following error.</p>\n<pre><code>startReload.js:24 Mixed Content: The page at 'https://my-dev-application.com/' was loaded over HTTPS, but attempted to connect to the insecure WebSocket endpoint 'ws://localhost:12450/reload'. This request has been blocked; this endpoint must be available over WSS.\n</code></pre>\n<p>I didn't find any overview about exactly which browsers and systems produce this error. Based on my tests, this problem occurs for Chrome on Linux, but not on Windows nor OSX, and it also happens for Firefox, on every operating system I tried.</p>\n<p>I couldn't find a perfect solution to the problem, but there is a workaround that can at least mitigate the issue. What we can do is host the WebSocket endpoint on both <code>ws</code> and <code>wss</code>, and try to connect to both from the client (first to <code>ws</code>, and if that fails, then to <code>wss</code>).</p>\n<p>This solves the problem for both Chrome and Firefox, but there is one more thing we have to do. Since we are hosting the service on localhost, there is no way to get a valid SSL certificate, and Chrome rejects the connection by default. In order to make it ignore certificate errors when connecting to <code>localhost</code>, we have to go to the advanced settings page in Chrome by navigating to <code>chrome://flags</code>, and we have to enable the following setting.</p>\n<p><img src=\"/content/images/2016/10/chrome-ignore-localhost-cert.png\" alt=\"\"></p>\n<p>With Firefox this didn't cause a problem, the connection worked properly after I started hosting the endpoint on <code>wss</code>. I'll update the post if I encounter any problem with it.</p>\n<h1 id=\"source\">Source</h1>\n<p>I uploaded the full working example to <a href=\"https://github.com/markvincze/golang-reload-browser\">GitHub</a>, which also contain the implementation of hosting the reload endpoint on both WS and WSS, and also the code for the client side to establish the connection.</p>\n",
        "comment_id": "26",
        "plaintext": "Introduction\nAt work I've been working on a client-side Golang application (a command-line\ntool), which is used as part of the development toolchain we're using at the\ncompany.\n\nThis application is used from the command line to upload packages to our\ndevelopment web server, which is then opened in the browser.\n\nInstead of opening our development site in a new tab every time, I wanted to\nprogrammatically refresh the browser tab if one has already been opened.\n\nI initially expected this to be a pretty easy task, but after some googling I\nhad to realize it's not trivial at all.\nApparently, there is no way to programmatically connect to the browser, query\nthe list of tabs open, find a particular tab, and refresh it. (Especially not in\na cross-browser and cross-platform way.)\n\nAfter searching and asking around, I was pointed to a technology called\nlivereload, which is both a development tool [http://livereload.com/]  and an \nnpm package [https://www.npmjs.com/package/livereload]  for automatically\nrefreshing the browser when editing some HTML content, or when new HTML content\nis being generated during the developing a website.\n\nThe way livereload works is that a component is hosting a small WebSocket\nservice to which the browser can connect. It is also running a file watcher\nwatching all the content (HTML, CSS, JavaScript, etc.) which should trigger a\nbrowser refresh when changed.\nThen a small piece of JavaScript code in the browser connects to the WebSocket\nserver, and refreshes the tab every time it receives a message.\n\nThe feature I wanted to implement is a bit different: I don't want to watch a\nparticular folder containing some files and refresh the browser on changes. What\nI want to do is be able to programmatically trigger a refresh from code.\n\nIt turned out there is no simpler way to achieve this than to utilize the same\napproach which is used by livereload: host a small WebSocket endpoint in my\nGolang app, connect to it from the browser, and send a message every time I want\nto refresh the page.\n\nImplementing this in Golang ended up being not too difficult, although there are\na couple of gotchas you have to watch out for if your site is served over HTTPS.\n\nImplementing the reload server\nThe Websocket service we want to implement is very simple: the client (the\nbrowser) never initiates communication, it's only the server (the Golang app)\nthat sends a message when the page has to be refreshed. We need only a single\nmessage type (with no arguments), since the only action we want to implement is\nthe reload.\n\nThe library websocket [https://github.com/gorilla/websocket]  from the Gorilla\nweb toolkit [http://www.gorillatoolkit.org/]  can be used to implement the\nendpoint. I based my implementation on the Chat example\n[https://github.com/gorilla/websocket/tree/master/examples/chat]  provided by\nthe library (basically I tried to trim it down as much as possible, so it only\ncontains the parts necessary for my purposes).\n\nTo host a WebSocket endpoint, we need to have some boilerplate to manage the\nclient connections and to send messages. I took the following two files from the\n Chat example [https://github.com/gorilla/websocket/tree/master/examples/chat] \nwithout much modification.\n\nThe first helper file is wsClient.go, which is responsible for the low level\nWebSocket communication.\n\npackage main\n\nimport (\n    \"log\"\n    \"net/http\"\n    \"time\"\n\n    \"github.com/gorilla/websocket\"\n)\n\nconst (\n    // Time allowed to write a message to the peer.\n    writeWait = 10 * time.Second\n\n    // Time allowed to read the next pong message from the peer.\n    pongWait = 60 * time.Second\n\n    // Send pings to peer with this period. Must be less than pongWait.\n    pingPeriod = (pongWait * 9) / 10\n)\n\nvar (\n    newline = []byte{'\\n'}\n    space   = []byte{' '}\n)\n\nvar upgrader = websocket.Upgrader{\n    ReadBufferSize:  1024,\n    WriteBufferSize: 1024,\n    CheckOrigin: func(r *http.Request) bool {\n        return true\n    },\n}\n\n// Client is an middleman between the websocket connection and the hub.\ntype Client struct {\n    hub *Hub\n\n    // The websocket connection.\n    conn *websocket.Conn\n\n    // Buffered channel of outbound messages.\n    send chan []byte\n}\n\n// readPump pumps messages from the websocket connection to the hub.\nfunc (c *Client) readPump() {\n    defer func() {\n        c.hub.unregister <- c\n        c.conn.Close()\n    }()\n    for {\n        _, _, err := c.conn.ReadMessage()\n        if err != nil {\n            if websocket.IsUnexpectedCloseError(err, websocket.CloseGoingAway) {\n                log.Printf(\"An error happened when reading from the Websocket client: %v\", err)\n            }\n            break\n        }\n    }\n}\n\n// write writes a message with the given message type and payload.\nfunc (c *Client) write(mt int, payload []byte) error {\n    c.conn.SetWriteDeadline(time.Now().Add(writeWait))\n    return c.conn.WriteMessage(mt, payload)\n}\n\n// writePump pumps messages from the hub to the websocket connection.\nfunc (c *Client) writePump() {\n    ticker := time.NewTicker(pingPeriod)\n    defer func() {\n        ticker.Stop()\n        c.conn.Close()\n    }()\n    for {\n        select {\n        case message, ok := <-c.send:\n            if !ok {\n                // The hub closed the channel.\n                c.write(websocket.CloseMessage, []byte{})\n                return\n            }\n\n            c.conn.SetWriteDeadline(time.Now().Add(writeWait))\n            w, err := c.conn.NextWriter(websocket.TextMessage)\n            if err != nil {\n                return\n            }\n            w.Write(message)\n\n            n := len(c.send)\n            for i := 0; i < n; i++ {\n                w.Write(newline)\n                w.Write(<-c.send)\n            }\n\n            if err := w.Close(); err != nil {\n                return\n            }\n        case <-ticker.C:\n            if err := c.write(websocket.PingMessage, []byte{}); err != nil {\n                return\n            }\n        }\n    }\n}\n\n// serveWs handles websocket requests from the peer.\nfunc serveWs(hub *Hub, w http.ResponseWriter, r *http.Request) {\n    conn, err := upgrader.Upgrade(w, r, nil)\n    if err != nil {\n        log.Println(err)\n        return\n    }\n    client := &Client{hub: hub, conn: conn, send: make(chan []byte, 256)}\n    client.hub.register <- client\n    go client.writePump()\n    client.readPump()\n}\n\n\nSince this was originally implemented to support the two-way communication in a\nChat application, probably it could be trimmed down even more.\nThe second file, wsHub.go  takes care of managing the list of client\nconnections.\n\npackage main\n\n// Hub maintains the set of active clients and broadcasts messages to the clients.\ntype Hub struct {\n    // Registered clients.\n    clients map[*Client]bool\n\n    // Inbound messages from the clients.\n    broadcast chan []byte\n\n    // Register requests from the clients.\n    register chan *Client\n\n    // Unregister requests from clients.\n    unregister chan *Client\n}\n\nfunc newHub() *Hub {\n    return &Hub{\n        broadcast:  make(chan []byte),\n        register:   make(chan *Client),\n        unregister: make(chan *Client),\n        clients:    make(map[*Client]bool),\n    }\n}\n\nfunc (h *Hub) run() {\n    for {\n        select {\n        case client := <-h.register:\n            h.clients[client] = true\n        case client := <-h.unregister:\n            if _, ok := h.clients[client]; ok {\n                delete(h.clients, client)\n                close(client.send)\n            }\n        case message := <-h.broadcast:\n            for client := range h.clients {\n                select {\n                case client.send <- message:\n                default:\n                    close(client.send)\n                    delete(h.clients, client)\n                }\n            }\n        }\n    }\n}\n\n\nWith these helpers in place we can start up our actual WS endpoint.\n\npackage main\n\nimport (\n    \"bytes\"\n    \"io/ioutil\"\n    \"log\"\n    \"net/http\"\n)\n\nvar (\n    hub *Hub\n    // The port on which we are hosting the reload server has to be hardcoded on the client-side too.\n    reloadAddress    = \":12450\"\n)\n\nfunc startReloadServer() {\n    hub = newHub()\n    go hub.run()\n    http.HandleFunc(\"/reload\", func(w http.ResponseWriter, r *http.Request) {\n        serveWs(hub, w, r)\n    })\n\n    go startServer()\n    log.Println(\"Reload server listening at\", reloadAddress)\n}\n\nfunc startServer() {\n    err := http.ListenAndServe(reloadAddress, nil)\n\n    if err != nil {\n        log.Println(\"Failed to start up the Reload server: \", err)\n        return\n    }\n}\n\n\nIn this example I'm hosting the service on the port 12450, which I randomly\npicked from the unassigned ports in the registry maintained by IANA\n[http://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?&page=1]\n. You can pick another port for your application, which can then be hardcoded\ninto both the service and the client.\n\nCalling the function startReloadServer  at the beginning of our application will\nstart hosting the WebSocket endpoint, and it'll keep running until our app\nterminates.\n\nThen we can implement the function that will send the reload message to the\nbrowser. This is the function we have to call when we want to reload the\nbrowser.\n\nfunc sendReload() {\n    message := bytes.TrimSpace([]byte(\"reload\"))\n    hub.broadcast <- message\n}\n\n\nIn the example I'm sending the string \"reload\", which doesn't have any role, the\nclient won't interpret it at all, since the only function we have is reloading,\nwhich doesn't need any parameters. If we needed anything more complicated, here\nwe could send an arbitrary message to the browser, which we can then process in\nJavaScript.\n\nWith all these building blocks in place the only thing we have to do is call \nstartReloadServer  to start hosting the service, and then call sendReload  every\ntime we want to refresh the browser.\n\nstartReloadServer()\n\n...\n\nsendReload()\n\n\nConnecting to the Reload service\nConnecting the website to the endpoint is pretty simple, but we have to keep in\nmind that when we open the site in the browser, our app hosting the endpoint\nmight not run yet (or it might be stopped and restarted later). So if our site\ncannot connect initially, we need to periodically retry.\n\nThis can be done with the following code.\n\nfunction tryConnectToReload(address) {\n  var conn;\n  // This is a statically defined port on which the app is hosting the reload service.\n  conn = new WebSocket(\"ws://localhost:12450/reload\");\n\n  conn.onclose = function(evt) {\n    // The reload endpoint hasn't been started yet, we are retrying in 2 seconds.\n    setTimeout(() => tryConnectToReload(), 2000);\n  };\n\n  conn.onmessage = function(evt) {\n    console.log(\"Refresh received!\");\n\n    // If we uncomment this line, then the page will refresh every time a message is received.\n    //location.reload()\n  };\n}\n\ntry {\n  if (window[\"WebSocket\"]) {\n    tryConnectToReload();\n  } else {\n    console.log(\"Your browser does not support WebSocket, cannot connect to the reload service.\");\n  }\n} catch (ex) {\n  console.log('Exception during connecting to reload:', ex);\n}\n\n\nSo if we call location.reload()  in the onmessage  handler, then the browser\nwill be refreshed every time we receive a message.\n\nProblems with TLS\nThe above solution works perfectly as long as our website is served over plain\nHTTP.\nThis is typically the case if it's a site under development hosted on localhost.\n\nOn the other hand, if we access the site through HTTPS, things are a bit more\ntricky.\n\nIf we try to connect from a website served over HTTPS to a WebSocket endpoint\nhosted without TLS (over ws://), then  depending on the browser and the\noperating system  we might get the following error.\n\nstartReload.js:24 Mixed Content: The page at 'https://my-dev-application.com/' was loaded over HTTPS, but attempted to connect to the insecure WebSocket endpoint 'ws://localhost:12450/reload'. This request has been blocked; this endpoint must be available over WSS.\n\n\nI didn't find any overview about exactly which browsers and systems produce this\nerror. Based on my tests, this problem occurs for Chrome on Linux, but not on\nWindows nor OSX, and it also happens for Firefox, on every operating system I\ntried.\n\nI couldn't find a perfect solution to the problem, but there is a workaround\nthat can at least mitigate the issue. What we can do is host the WebSocket\nendpoint on both ws  and wss, and try to connect to both from the client (first\nto ws, and if that fails, then to wss).\n\nThis solves the problem for both Chrome and Firefox, but there is one more thing\nwe have to do. Since we are hosting the service on localhost, there is no way to\nget a valid SSL certificate, and Chrome rejects the connection by default. In\norder to make it ignore certificate errors when connecting to localhost, we have\nto go to the advanced settings page in Chrome by navigating to chrome://flags,\nand we have to enable the following setting.\n\n\n\nWith Firefox this didn't cause a problem, the connection worked properly after I\nstarted hosting the endpoint on wss. I'll update the post if I encounter any\nproblem with it.\n\nSource\nI uploaded the full working example to GitHub\n[https://github.com/markvincze/golang-reload-browser], which also contain the\nimplementation of hosting the reload endpoint on both WS and WSS, and also the\ncode for the client side to establish the connection.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "Programmatically refreshing a browser tab from an application can be done with a WebSocket connection. This post describes how to achieve this in Golang.",
        "author_id": "1",
        "created_at": "2016-10-30 14:03:44",
        "created_by": "1",
        "updated_at": "2016-10-30 16:03:11",
        "updated_by": "1",
        "published_at": "2016-10-30 14:39:59",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2971",
        "uuid": "f53de7b7-d447-4e51-9c61-947a01fd7e0d",
        "title": "Secure an ASP.NET Core api with Firebase",
        "slug": "secure-an-asp-net-core-api-with-firebase",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"# Introduction\\n\\n**Update**: Updated the code samples according to the changes introduced in .NET 2.0.\\n\\n[Firebase](https://firebase.google.com/) is an application development framework and infrastructure provided by Google. It offers a handful of services, like Storage, Analytics, Notifications and Hosting, mainly targeted to mobile application developers.\\n\\nAt first I was a bit confused by Firebase, since its functionality seems to overlap with Google's generic cloud environment, the Google Cloud Platform, which hasamong many other thingssimilar features.  \\nThere are questions about the relation between them in many forums, so I'm not the only one who was puzzled by this.\\n\\nIt seems that Google simply tries to differentiate between two separate offerings, where Google Cloud Platform (GCP) is the core infrastructure, providing many different lower level building blocks.  \\nOn the other hand, Firebase (which is mostly built on top of GCP under the hood) is a simpler framework offering fewer, higher level services.\\n\\nTo me it makes sense to build a higher level, easier to understand framework on top of GCP, since the number of different services offered in the Cloud Platform can be somewhat overwhelming, if all we want is a couple of simple api endpoints for our mobile application.\\n\\n![Offerings of the Google Cloud Platform](/content/images/2016/12/gcp-services.png)\\n\\nAnd of course it's not only the Google cloud which has this sort of \\\"feature creep\\\"Azure and Amazon AWS have the same problem. I find the the service overview page of AWS the scariest :).\\n\\n![Offerings of Amazon AWS](/content/images/2016/12/aws-services.png)\\n\\n## Authentication\\n\\nThe only Firebase service I'm going to focus on in this post is [Authentication](https://firebase.google.com/docs/auth/). It supports multiple authentication providers, you can use a custom email/password combination, you can hook in your own user store, or you can use third-party providers, like Google, Facebook or Twitter accounts. Then, in your application you can handle authentication with all these providers in a unified way.\\n\\nIn this post what I'm going to describe is not how to implement the sign-in part, that's well documented on the [Firebase site](https://firebase.google.com/docs/auth/), it has official SDK support for Android, iOS and web applications.\\n\\nThe scenario I'll write about is when you want to secure your ASP.NET Core api to be only accessed by already logged in users. A typical scenario is a single-page web application, or a mobile app, in which the sign-in is already implemented with Firebase, so the application already received a JWT access token (called the ID token). This token can be verified in our service by a standard JWT library, which is luckily already available in ASP.NET Core.\\n\\n# Configuring ASP.NET\\n\\nWe can use the package [Microsoft.AspNetCore.Authentication.JwtBearer](https://www.nuget.org/packages/Microsoft.AspNetCore.Authentication.JwtBearer/) to secure our api with Firebase. This package includes a middleware for automatically verifying JWT tokens coming in the `Authorization` header from the client on every request.\\n\\nI couldn't find any documentation about using this specifically with Firebase. Most of the blog posts about setting up this middleware were targeted for the scenario in which our api knows the symmetric key used by the authentication provider, and gets it from some form of secure configuration parameter.\\n\\nIn my case the scenario was different. I didn't want to create a new token, I just wanted to verify an existing token issued by Firebase.  \\nIn order just to verify an id token, we only have to know the public part of the asymmetric key being used to sign the token. And since the public key is not a secret, we won't need secure configuration, the key is published through a public endpoint of our Firebase service.\\n\\nSince the JwtBearer middleware supports a [wide range of options](https://github.com/AzureAD/azure-activedirectory-identitymodel-extensions-for-dotnet/blob/master/src/Microsoft.IdentityModel.Tokens/TokenValidationParameters.cs) to be set, it took some experimenting before I managed to find the set of options that work with the tokens issued by Firebase.\\n\\n## Set up the middleware\\n\\nIn order to verify the token in the `Authorization` header on every request, we need to add some code to our `Startup` class to wire up the authentication middleware.\\n\\nThe way to do this changed with the 2.0 version of ASP.NET Core, in the following sections we'll see an example of the setup for both versions.\\n\\n### .NET 2.0\\n\\nIn our `ConfigureServices` method we need to register the services necessary to handle authentication, and to specify the parameters of our Firebase project.\\n\\n```csharp\\nservices\\n    .AddAuthentication(JwtBearerDefaults.AuthenticationScheme)\\n    .AddJwtBearer(options =>\\n    {\\n        options.Authority = \\\"https://securetoken.google.com/my-firebase-project\\\";\\n        options.TokenValidationParameters = new TokenValidationParameters\\n        {\\n            ValidateIssuer = true,\\n            ValidIssuer = \\\"https://securetoken.google.com/my-firebase-project\\\",\\n            ValidateAudience = true,\\n            ValidAudience = \\\"my-firebase-project\\\",\\n            ValidateLifetime = true\\n        };\\n    });\\n```\\n\\nThen in our `Configure` method we have to do one simple call to register the actual middleware that will handle the authentication.\\n\\n```csharp\\napp.UseAuthentication();\\n```\\n\\n### .NET 1.X\\n\\nIn .NET 1.X the whole setup happens in the `Configure` method, we have to call the `UseJwtBearerAuthentication` extension method to wire up the middleware, and pass in the parameters of our Firebase project.\\n\\n```csharp\\napp.UseJwtBearerAuthentication(new JwtBearerOptions\\n{\\n    AutomaticAuthenticate = true,\\n    Authority = \\\"https://securetoken.google.com/my-firebase-project\\\",\\n    TokenValidationParameters = new TokenValidationParameters\\n    {\\n        ValidateIssuer = true,\\n        ValidIssuer = \\\"https://securetoken.google.com/my-firebase-project\\\",\\n        ValidateAudience = true,\\n        ValidAudience = \\\"my-firebase-project\\\",\\n        ValidateLifetime = true\\n    }\\n});\\n```\\n\\n### What does it do?\\n\\nIn the code samples, `my-firebase-project` is your project ID in Firebase.  \\nThe middleware is going to verify the `Authorization` header on every request, check if it is signed with the correct private key, and verify if it contains the specified issuer and audience, so that it was issued specifically by our Firebase project. (The [Firebase documentation](https://firebase.google.com/docs/auth/admin/verify-id-tokens) also describes what has to be done if we would want to implement this verification ourselves.)\\n\\nIf the verification is successful, it is going to store the user information together with all its claims in `HttpContext.User`, and `HttpContext.User.Identity.IsAuthenticated` will be true.\\n\\nHow to proceed after this is up to you: you can either use the built-in `Authorize` header on your controllers. If you put `[Authorize]` on top of a controller or an action, ASP.NET will return a 401 response if the request doesn't have a valid token in the `Authorization` header.  \\nOr you can implement your own code that does some custom verification based on `HttpContext.User`.\\n\\n### Gotcha\\n\\nAs Valeriy points out in the comments, you have to make sure to add the Jwt authentication middleware to the pipeline earlier than the MVC routing pipeline, so have the line `app.UseAuthentication` (or `app.UseJwtBearerAuthentication(...)` in case of .NET 1.X) before `app.UseMvc()`. Or if you use any kind of other middleware which does authorization based on `HttpContext.User`, that has to come in the pipeline after the authentication middleware.\\nThis is important because what the authentication middleware does is that it checks the value of the `Authorization` header, and based on that it populates `HttpContext.User`. And that's what the `Authorize` filter is using, so populating this data has to happen before the Mvc routing middleware executes, otherwise the authorization will always fail.\\n\\n### Update: How does token signing work?\\n\\nThere was some interesting discussion in the comments and in [this SO question](https://stackoverflow.com/questions/42336950/firebase-authentication-jwt-with-net-core/42410233), so I added this section to clarify how the token signing works.\\n\\nThe following diagram illustrates the (simplified) architecture and flow of using Firebase for authentication from an ASP.NET application.\\n\\n![Diagram illustrating the architecture and flow of using Firebase for authentication from an ASP.NET application.](/content/images/2017/02/firebase-auth-architecture-small-1.png)\\n\\nDuring signing in and accessing a secure endpoint, the following steps are involved.\\n\\n1. When our application starts up (and then later also periodically) the `JwtBearerMiddleware` calls `https://securetoken.google.com/my-firebase-project/.well-known/openid-configuration` (you can see more details in the [Source code](https://github.com/aspnet/Security/blob/22d2fe99c6fd9806b36025399a217a3a8b4e50f4/src/Microsoft.AspNetCore.Authentication.JwtBearer/JwtBearerMiddleware.cs#L81)). From there, it navigates to `https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com`, where the current public keys of Google are accessible.\\nIt's important that when we're using a public key asymmetric cryptosystem, the *public key is not kept as a secret*, but it is published in plain sight. That's what all the clients will be able to use to verify if something was signed by the private key (which is really a secret, and only the owner can have access to it).\\n2. A client signs in using their credential through Firebase. Firebase supports a variety of identity providers, it doesn't matter which one the user is using (Facebook, Google, Twitter, etc.).\\n3. If the signin was successful, then Firebase constructs a JWT token for the client. A crucial part of this token is that **it's signed using the private key of the key pair**. In contrast to the public key, the private key is never exposed publicly, it is kept as a secret inside Google's infrastructure. If a private key gets compromised, that means that any attacker having access to it is able to create valid JWT tokens.\\n4. The client receives the JWT token.\\n5. The client calls a secure endpoint on our Api, and puts the token in the `Authorization` header.\\nAt this point the `JwtBearerMiddleware` in the pipeline checks this token, and verifies if it's valid (if it was signed with Google's private key). The important thing to realize here is that in order to do the verification, our Api *does not need to have access to the private key*. Only the public key is necessary to do that. After verification, the middleware populates `HttpContext.User`, and `HttpContext.User.Identity.IsAuthenticated` accordingly.\\n\\nYou can find an even simpler description of this concept on the [RSA Wikipedia page](https://en.wikipedia.org/wiki/RSA_(cryptosystem)#Signing_messages).\"}]],\"sections\":[[10,0]]}",
        "html": "<h1 id=\"introduction\">Introduction</h1>\n<p><strong>Update</strong>: Updated the code samples according to the changes introduced in .NET 2.0.</p>\n<p><a href=\"https://firebase.google.com/\">Firebase</a> is an application development framework and infrastructure provided by Google. It offers a handful of services, like Storage, Analytics, Notifications and Hosting, mainly targeted to mobile application developers.</p>\n<p>At first I was a bit confused by Firebase, since its functionality seems to overlap with Google's generic cloud environment, the Google Cloud Platform, which hasamong many other thingssimilar features.<br>\nThere are questions about the relation between them in many forums, so I'm not the only one who was puzzled by this.</p>\n<p>It seems that Google simply tries to differentiate between two separate offerings, where Google Cloud Platform (GCP) is the core infrastructure, providing many different lower level building blocks.<br>\nOn the other hand, Firebase (which is mostly built on top of GCP under the hood) is a simpler framework offering fewer, higher level services.</p>\n<p>To me it makes sense to build a higher level, easier to understand framework on top of GCP, since the number of different services offered in the Cloud Platform can be somewhat overwhelming, if all we want is a couple of simple api endpoints for our mobile application.</p>\n<p><img src=\"/content/images/2016/12/gcp-services.png\" alt=\"Offerings of the Google Cloud Platform\"></p>\n<p>And of course it's not only the Google cloud which has this sort of &quot;feature creep&quot;Azure and Amazon AWS have the same problem. I find the the service overview page of AWS the scariest :).</p>\n<p><img src=\"/content/images/2016/12/aws-services.png\" alt=\"Offerings of Amazon AWS\"></p>\n<h2 id=\"authentication\">Authentication</h2>\n<p>The only Firebase service I'm going to focus on in this post is <a href=\"https://firebase.google.com/docs/auth/\">Authentication</a>. It supports multiple authentication providers, you can use a custom email/password combination, you can hook in your own user store, or you can use third-party providers, like Google, Facebook or Twitter accounts. Then, in your application you can handle authentication with all these providers in a unified way.</p>\n<p>In this post what I'm going to describe is not how to implement the sign-in part, that's well documented on the <a href=\"https://firebase.google.com/docs/auth/\">Firebase site</a>, it has official SDK support for Android, iOS and web applications.</p>\n<p>The scenario I'll write about is when you want to secure your ASP.NET Core api to be only accessed by already logged in users. A typical scenario is a single-page web application, or a mobile app, in which the sign-in is already implemented with Firebase, so the application already received a JWT access token (called the ID token). This token can be verified in our service by a standard JWT library, which is luckily already available in ASP.NET Core.</p>\n<h1 id=\"configuringaspnet\">Configuring ASP.NET</h1>\n<p>We can use the package <a href=\"https://www.nuget.org/packages/Microsoft.AspNetCore.Authentication.JwtBearer/\">Microsoft.AspNetCore.Authentication.JwtBearer</a> to secure our api with Firebase. This package includes a middleware for automatically verifying JWT tokens coming in the <code>Authorization</code> header from the client on every request.</p>\n<p>I couldn't find any documentation about using this specifically with Firebase. Most of the blog posts about setting up this middleware were targeted for the scenario in which our api knows the symmetric key used by the authentication provider, and gets it from some form of secure configuration parameter.</p>\n<p>In my case the scenario was different. I didn't want to create a new token, I just wanted to verify an existing token issued by Firebase.<br>\nIn order just to verify an id token, we only have to know the public part of the asymmetric key being used to sign the token. And since the public key is not a secret, we won't need secure configuration, the key is published through a public endpoint of our Firebase service.</p>\n<p>Since the JwtBearer middleware supports a <a href=\"https://github.com/AzureAD/azure-activedirectory-identitymodel-extensions-for-dotnet/blob/master/src/Microsoft.IdentityModel.Tokens/TokenValidationParameters.cs\">wide range of options</a> to be set, it took some experimenting before I managed to find the set of options that work with the tokens issued by Firebase.</p>\n<h2 id=\"setupthemiddleware\">Set up the middleware</h2>\n<p>In order to verify the token in the <code>Authorization</code> header on every request, we need to add some code to our <code>Startup</code> class to wire up the authentication middleware.</p>\n<p>The way to do this changed with the 2.0 version of ASP.NET Core, in the following sections we'll see an example of the setup for both versions.</p>\n<h3 id=\"net20\">.NET 2.0</h3>\n<p>In our <code>ConfigureServices</code> method we need to register the services necessary to handle authentication, and to specify the parameters of our Firebase project.</p>\n<pre><code class=\"language-csharp\">services\n    .AddAuthentication(JwtBearerDefaults.AuthenticationScheme)\n    .AddJwtBearer(options =&gt;\n    {\n        options.Authority = &quot;https://securetoken.google.com/my-firebase-project&quot;;\n        options.TokenValidationParameters = new TokenValidationParameters\n        {\n            ValidateIssuer = true,\n            ValidIssuer = &quot;https://securetoken.google.com/my-firebase-project&quot;,\n            ValidateAudience = true,\n            ValidAudience = &quot;my-firebase-project&quot;,\n            ValidateLifetime = true\n        };\n    });\n</code></pre>\n<p>Then in our <code>Configure</code> method we have to do one simple call to register the actual middleware that will handle the authentication.</p>\n<pre><code class=\"language-csharp\">app.UseAuthentication();\n</code></pre>\n<h3 id=\"net1x\">.NET 1.X</h3>\n<p>In .NET 1.X the whole setup happens in the <code>Configure</code> method, we have to call the <code>UseJwtBearerAuthentication</code> extension method to wire up the middleware, and pass in the parameters of our Firebase project.</p>\n<pre><code class=\"language-csharp\">app.UseJwtBearerAuthentication(new JwtBearerOptions\n{\n    AutomaticAuthenticate = true,\n    Authority = &quot;https://securetoken.google.com/my-firebase-project&quot;,\n    TokenValidationParameters = new TokenValidationParameters\n    {\n        ValidateIssuer = true,\n        ValidIssuer = &quot;https://securetoken.google.com/my-firebase-project&quot;,\n        ValidateAudience = true,\n        ValidAudience = &quot;my-firebase-project&quot;,\n        ValidateLifetime = true\n    }\n});\n</code></pre>\n<h3 id=\"whatdoesitdo\">What does it do?</h3>\n<p>In the code samples, <code>my-firebase-project</code> is your project ID in Firebase.<br>\nThe middleware is going to verify the <code>Authorization</code> header on every request, check if it is signed with the correct private key, and verify if it contains the specified issuer and audience, so that it was issued specifically by our Firebase project. (The <a href=\"https://firebase.google.com/docs/auth/admin/verify-id-tokens\">Firebase documentation</a> also describes what has to be done if we would want to implement this verification ourselves.)</p>\n<p>If the verification is successful, it is going to store the user information together with all its claims in <code>HttpContext.User</code>, and <code>HttpContext.User.Identity.IsAuthenticated</code> will be true.</p>\n<p>How to proceed after this is up to you: you can either use the built-in <code>Authorize</code> header on your controllers. If you put <code>[Authorize]</code> on top of a controller or an action, ASP.NET will return a 401 response if the request doesn't have a valid token in the <code>Authorization</code> header.<br>\nOr you can implement your own code that does some custom verification based on <code>HttpContext.User</code>.</p>\n<h3 id=\"gotcha\">Gotcha</h3>\n<p>As Valeriy points out in the comments, you have to make sure to add the Jwt authentication middleware to the pipeline earlier than the MVC routing pipeline, so have the line <code>app.UseAuthentication</code> (or <code>app.UseJwtBearerAuthentication(...)</code> in case of .NET 1.X) before <code>app.UseMvc()</code>. Or if you use any kind of other middleware which does authorization based on <code>HttpContext.User</code>, that has to come in the pipeline after the authentication middleware.<br>\nThis is important because what the authentication middleware does is that it checks the value of the <code>Authorization</code> header, and based on that it populates <code>HttpContext.User</code>. And that's what the <code>Authorize</code> filter is using, so populating this data has to happen before the Mvc routing middleware executes, otherwise the authorization will always fail.</p>\n<h3 id=\"updatehowdoestokensigningwork\">Update: How does token signing work?</h3>\n<p>There was some interesting discussion in the comments and in <a href=\"https://stackoverflow.com/questions/42336950/firebase-authentication-jwt-with-net-core/42410233\">this SO question</a>, so I added this section to clarify how the token signing works.</p>\n<p>The following diagram illustrates the (simplified) architecture and flow of using Firebase for authentication from an ASP.NET application.</p>\n<p><img src=\"/content/images/2017/02/firebase-auth-architecture-small-1.png\" alt=\"Diagram illustrating the architecture and flow of using Firebase for authentication from an ASP.NET application.\"></p>\n<p>During signing in and accessing a secure endpoint, the following steps are involved.</p>\n<ol>\n<li>When our application starts up (and then later also periodically) the <code>JwtBearerMiddleware</code> calls <code>https://securetoken.google.com/my-firebase-project/.well-known/openid-configuration</code> (you can see more details in the <a href=\"https://github.com/aspnet/Security/blob/22d2fe99c6fd9806b36025399a217a3a8b4e50f4/src/Microsoft.AspNetCore.Authentication.JwtBearer/JwtBearerMiddleware.cs#L81\">Source code</a>). From there, it navigates to <code>https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com</code>, where the current public keys of Google are accessible.<br>\nIt's important that when we're using a public key asymmetric cryptosystem, the <em>public key is not kept as a secret</em>, but it is published in plain sight. That's what all the clients will be able to use to verify if something was signed by the private key (which is really a secret, and only the owner can have access to it).</li>\n<li>A client signs in using their credential through Firebase. Firebase supports a variety of identity providers, it doesn't matter which one the user is using (Facebook, Google, Twitter, etc.).</li>\n<li>If the signin was successful, then Firebase constructs a JWT token for the client. A crucial part of this token is that <strong>it's signed using the private key of the key pair</strong>. In contrast to the public key, the private key is never exposed publicly, it is kept as a secret inside Google's infrastructure. If a private key gets compromised, that means that any attacker having access to it is able to create valid JWT tokens.</li>\n<li>The client receives the JWT token.</li>\n<li>The client calls a secure endpoint on our Api, and puts the token in the <code>Authorization</code> header.<br>\nAt this point the <code>JwtBearerMiddleware</code> in the pipeline checks this token, and verifies if it's valid (if it was signed with Google's private key). The important thing to realize here is that in order to do the verification, our Api <em>does not need to have access to the private key</em>. Only the public key is necessary to do that. After verification, the middleware populates <code>HttpContext.User</code>, and <code>HttpContext.User.Identity.IsAuthenticated</code> accordingly.</li>\n</ol>\n<p>You can find an even simpler description of this concept on the <a href=\"https://en.wikipedia.org/wiki/RSA_(cryptosystem)#Signing_messages\">RSA Wikipedia page</a>.</p>\n",
        "comment_id": "28",
        "plaintext": "Introduction\nUpdate: Updated the code samples according to the changes introduced in .NET\n2.0.\n\nFirebase [https://firebase.google.com/]  is an application development framework\nand infrastructure provided by Google. It offers a handful of services, like\nStorage, Analytics, Notifications and Hosting, mainly targeted to mobile\napplication developers.\n\nAt first I was a bit confused by Firebase, since its functionality seems to\noverlap with Google's generic cloud environment, the Google Cloud Platform,\nwhich hasamong many other thingssimilar features.\nThere are questions about the relation between them in many forums, so I'm not\nthe only one who was puzzled by this.\n\nIt seems that Google simply tries to differentiate between two separate\nofferings, where Google Cloud Platform (GCP) is the core infrastructure,\nproviding many different lower level building blocks.\nOn the other hand, Firebase (which is mostly built on top of GCP under the hood)\nis a simpler framework offering fewer, higher level services.\n\nTo me it makes sense to build a higher level, easier to understand framework on\ntop of GCP, since the number of different services offered in the Cloud Platform\ncan be somewhat overwhelming, if all we want is a couple of simple api endpoints\nfor our mobile application.\n\n\n\nAnd of course it's not only the Google cloud which has this sort of \"feature\ncreep\"Azure and Amazon AWS have the same problem. I find the the service\noverview page of AWS the scariest :).\n\n\n\nAuthentication\nThe only Firebase service I'm going to focus on in this post is Authentication\n[https://firebase.google.com/docs/auth/]. It supports multiple authentication\nproviders, you can use a custom email/password combination, you can hook in your\nown user store, or you can use third-party providers, like Google, Facebook or\nTwitter accounts. Then, in your application you can handle authentication with\nall these providers in a unified way.\n\nIn this post what I'm going to describe is not how to implement the sign-in\npart, that's well documented on the Firebase site\n[https://firebase.google.com/docs/auth/], it has official SDK support for\nAndroid, iOS and web applications.\n\nThe scenario I'll write about is when you want to secure your ASP.NET Core api\nto be only accessed by already logged in users. A typical scenario is a\nsingle-page web application, or a mobile app, in which the sign-in is already\nimplemented with Firebase, so the application already received a JWT access\ntoken (called the ID token). This token can be verified in our service by a\nstandard JWT library, which is luckily already available in ASP.NET Core.\n\nConfiguring ASP.NET\nWe can use the package Microsoft.AspNetCore.Authentication.JwtBearer\n[https://www.nuget.org/packages/Microsoft.AspNetCore.Authentication.JwtBearer/] \nto secure our api with Firebase. This package includes a middleware for\nautomatically verifying JWT tokens coming in the Authorization  header from the\nclient on every request.\n\nI couldn't find any documentation about using this specifically with Firebase.\nMost of the blog posts about setting up this middleware were targeted for the\nscenario in which our api knows the symmetric key used by the authentication\nprovider, and gets it from some form of secure configuration parameter.\n\nIn my case the scenario was different. I didn't want to create a new token, I\njust wanted to verify an existing token issued by Firebase.\nIn order just to verify an id token, we only have to know the public part of the\nasymmetric key being used to sign the token. And since the public key is not a\nsecret, we won't need secure configuration, the key is published through a\npublic endpoint of our Firebase service.\n\nSince the JwtBearer middleware supports a wide range of options\n[https://github.com/AzureAD/azure-activedirectory-identitymodel-extensions-for-dotnet/blob/master/src/Microsoft.IdentityModel.Tokens/TokenValidationParameters.cs] \n to be set, it took some experimenting before I managed to find the set of\noptions that work with the tokens issued by Firebase.\n\nSet up the middleware\nIn order to verify the token in the Authorization  header on every request, we\nneed to add some code to our Startup  class to wire up the authentication\nmiddleware.\n\nThe way to do this changed with the 2.0 version of ASP.NET Core, in the\nfollowing sections we'll see an example of the setup for both versions.\n\n.NET 2.0\nIn our ConfigureServices  method we need to register the services necessary to\nhandle authentication, and to specify the parameters of our Firebase project.\n\nservices\n    .AddAuthentication(JwtBearerDefaults.AuthenticationScheme)\n    .AddJwtBearer(options =>\n    {\n        options.Authority = \"https://securetoken.google.com/my-firebase-project\";\n        options.TokenValidationParameters = new TokenValidationParameters\n        {\n            ValidateIssuer = true,\n            ValidIssuer = \"https://securetoken.google.com/my-firebase-project\",\n            ValidateAudience = true,\n            ValidAudience = \"my-firebase-project\",\n            ValidateLifetime = true\n        };\n    });\n\n\nThen in our Configure  method we have to do one simple call to register the\nactual middleware that will handle the authentication.\n\napp.UseAuthentication();\n\n\n.NET 1.X\nIn .NET 1.X the whole setup happens in the Configure  method, we have to call\nthe UseJwtBearerAuthentication  extension method to wire up the middleware, and\npass in the parameters of our Firebase project.\n\napp.UseJwtBearerAuthentication(new JwtBearerOptions\n{\n    AutomaticAuthenticate = true,\n    Authority = \"https://securetoken.google.com/my-firebase-project\",\n    TokenValidationParameters = new TokenValidationParameters\n    {\n        ValidateIssuer = true,\n        ValidIssuer = \"https://securetoken.google.com/my-firebase-project\",\n        ValidateAudience = true,\n        ValidAudience = \"my-firebase-project\",\n        ValidateLifetime = true\n    }\n});\n\n\nWhat does it do?\nIn the code samples, my-firebase-project  is your project ID in Firebase.\nThe middleware is going to verify the Authorization  header on every request,\ncheck if it is signed with the correct private key, and verify if it contains\nthe specified issuer and audience, so that it was issued specifically by our\nFirebase project. (The Firebase documentation\n[https://firebase.google.com/docs/auth/admin/verify-id-tokens]  also describes\nwhat has to be done if we would want to implement this verification ourselves.)\n\nIf the verification is successful, it is going to store the user information\ntogether with all its claims in HttpContext.User, and \nHttpContext.User.Identity.IsAuthenticated  will be true.\n\nHow to proceed after this is up to you: you can either use the built-in \nAuthorize  header on your controllers. If you put [Authorize]  on top of a\ncontroller or an action, ASP.NET will return a 401 response if the request\ndoesn't have a valid token in the Authorization  header.\nOr you can implement your own code that does some custom verification based on \nHttpContext.User.\n\nGotcha\nAs Valeriy points out in the comments, you have to make sure to add the Jwt\nauthentication middleware to the pipeline earlier than the MVC routing pipeline,\nso have the line app.UseAuthentication  (or app.UseJwtBearerAuthentication(...) \nin case of .NET 1.X) before app.UseMvc(). Or if you use any kind of other\nmiddleware which does authorization based on HttpContext.User, that has to come\nin the pipeline after the authentication middleware.\nThis is important because what the authentication middleware does is that it\nchecks the value of the Authorization  header, and based on that it populates \nHttpContext.User. And that's what the Authorize  filter is using, so populating\nthis data has to happen before the Mvc routing middleware executes, otherwise\nthe authorization will always fail.\n\nUpdate: How does token signing work?\nThere was some interesting discussion in the comments and in this SO question\n[https://stackoverflow.com/questions/42336950/firebase-authentication-jwt-with-net-core/42410233]\n, so I added this section to clarify how the token signing works.\n\nThe following diagram illustrates the (simplified) architecture and flow of\nusing Firebase for authentication from an ASP.NET application.\n\n\n\nDuring signing in and accessing a secure endpoint, the following steps are\ninvolved.\n\n 1. When our application starts up (and then later also periodically) the \n    JwtBearerMiddleware  calls \n    https://securetoken.google.com/my-firebase-project/.well-known/openid-configuration \n     (you can see more details in the Source code). From there, it navigates to \n    https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com\n    , where the current public keys of Google are accessible.\n    It's important that when we're using a public key asymmetric cryptosystem,\n    the public key is not kept as a secret, but it is published in plain sight.\n    That's what all the clients will be able to use to verify if something was\n    signed by the private key (which is really a secret, and only the owner can\n    have access to it).\n 2. A client signs in using their credential through Firebase. Firebase supports\n    a variety of identity providers, it doesn't matter which one the user is\n    using (Facebook, Google, Twitter, etc.).\n 3. If the signin was successful, then Firebase constructs a JWT token for the\n    client. A crucial part of this token is that it's signed using the private\n    key of the key pair. In contrast to the public key, the private key is never\n    exposed publicly, it is kept as a secret inside Google's infrastructure. If\n    a private key gets compromised, that means that any attacker having access\n    to it is able to create valid JWT tokens.\n 4. The client receives the JWT token.\n 5. The client calls a secure endpoint on our Api, and puts the token in the \n    Authorization  header.\n    At this point the JwtBearerMiddleware  in the pipeline checks this token,\n    and verifies if it's valid (if it was signed with Google's private key). The\n    important thing to realize here is that in order to do the verification, our\n    Api does not need to have access to the private key. Only the public key is\n    necessary to do that. After verification, the middleware populates \n    HttpContext.User, and HttpContext.User.Identity.IsAuthenticated \n    accordingly.\n\nYou can find an even simpler description of this concept on the RSA Wikipedia\npage.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "How to secure an ASP.NET Core application with Firebase Authentication by verifying the access tokens in the Authorization header.",
        "author_id": "1",
        "created_at": "2016-12-04 18:34:42",
        "created_by": "1",
        "updated_at": "2017-08-22 19:06:38",
        "updated_by": "1",
        "published_at": "2017-08-22 19:00:00",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2972",
        "uuid": "a1872706-5332-49cb-9818-827eaf20cbb4",
        "title": "Setting up Coveralls with OpenCover for a .NET Core project",
        "slug": "setting-up-coveralls-for-a-net-core-project",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"**Update**: The original version of this post was written when the `dotnet` toolchain was built on top of the `project.json` based project structure. Since then the `project.json` was deprecated, and .NET Core moved to a `csproj`-based approach.  \\nBecause the `project.json` was completely deprecated, I decided not to write a new post, but simply adjust this one to work with the new toolchain (as of writing, that is `dotnet 1.1`).\\n\\nUnit test coverage is an interesting metric. It's not necessarily the most important one, and can be a bit misleading, but it gives at least an indication about how well our test suite covers our code base.\\n\\n100% unit tests coverage is often a stretch, and can require a huge amount of extra work. Over 90% we often hit the point of diminishing returns, and have to jump over lots of extra hoops to get those last couple of percents covered.\\n\\nNevertheless, it's nice to keep the coverage reasonably high, I usually try to aim at keeping it over 90%. This ensures that most of the code are exercised during a test run, and that the majority of our components are loosely coupled, and are able to be isolated from their dependencies.\\n\\nI find it particularly handy to keep an eye on the change to the coverage caused by pull requests, and the gamification aspect of it is extra motivation for the devs to keep it high.\\n\\nAs far as I can tell, the test coverage situation in .NET Core is not really established at this point. The .NET coverage tools don't have support for Core projects yet.  \\nThe only one I could find which works today is [OpenCover](https://github.com/OpenCover/opencover). In this post I'll cover setting up coverage generation with it and uploading it to Coveralls.\\n\\nSince OpenCover doesn't officially support .NET Core either, take everything in this post with a grain of salt, it definitely has the Works on My Machine badge, there is no guarantee the approach documented here won't break with an update to either .NET or OpenCover.\\n\\n# Generating coverage\\n\\nOne significant limitation of OpenCover at this point is that it can only be used on Windows. This is problematic if our ecosystem (the developer machines and build agents) are completely Linux- or Mac-based. In this case we'll have to wait until OpenCover (or any other coverage tool) starts supporting Linux.\\n\\nOn the other hand, if we are on Windows, coverage reporting already works nicely, although there are some gotchas we have to watch out for.\\n\\nTo give credits where it's due: I'm not the first one getting this to work. I based my solution on the [build script](https://github.com/GoogleCloudPlatform/google-cloud-dotnet/blob/master/build.sh) used by the .NET SDK of the Google Cloud Platform.\\n\\nTo generate the coverage for a particular test project we have to issue a single command with the OpenCover CLI (Assuming all the project dependencies have already been restored).\\n\\n```bash\\n$OPENCOVER \\\\\\n  -target:\\\"c:\\\\Program Files\\\\dotnet\\\\dotnet.exe\\\" \\\\\\n  -targetargs:\\\"test -f netcoreapp1.0 -c Release test/MyProject.UnitTests/MyProject.UnitTests.csproj\\\" \\\\\\n  -mergeoutput \\\\\\n  -hideskipped:File \\\\\\n  -output:coverage/coverage.xml \\\\\\n  -oldStyle \\\\\\n  -filter:\\\"+[MyProject*]* -[MyProject.UnitTests*]*\\\" \\\\\\n  -searchdirs:test/MyProject.UnitTests/bin/Release/netcoreapp1.0 \\\\\\n  -register:user\\n```\\n\\n - `-target` and `-targetargs`: These flags specify the operation which is executed, during which OpenCover collects the coverage statistics. In this case we specify executing the tests in our unit test project. (With a quick search I didn't find any recommendation about whether to use a Debug or a Release build, so I'm using Release, as it is done in [google-cloud-dotnet](https://github.com/GoogleCloudPlatform/google-cloud-dotnet).)\\n It's important to explicitly specify the framework we're using, so we can reliably set the binary output folder in the `-searchdirs` argument. We don't necessarily have to use `netcoreapp1.0`, `net451` works as well. (I haven't tried other flavors.)\\n - `-output`: The path to the generated output file.\\n - `-searchdirs`: We have to exactly specify the binary output folder of our test project. Keep in mind that the exact path is affected by the build configuration and the framework we're using. For example, in case of `net451`, instead of `/netcoreapp1.0/` we would have something like `/net451/win7-x64/` in the path.\\n - `-filter`: This argument is important, it specifies which projects we want to include in the report, and which ones we exclude. The [documentation](https://github.com/opencover/opencover/wiki/usage) contains the details, but the pattern `+[MyProject*]* -[MyProject.UnitTests*]*` is enough to get started. It makes everything under MyProject included, and excludes the code under the test project.\\n - `-mergeoutput, -hideskipped, -oldStyle, -register`: I didn't investigate too much in detail about these flags, I just copied them over from [google-cloud-dotnet](https://github.com/GoogleCloudPlatform/google-cloud-dotnet). Although I tried to remove them to make my script a bit simpler, but removing any of them made the coverage generation fail with various errors.\\n\\nOpenCover saves the coverage into the specified XML file, which can the be used to generate an HTML report, and to upload the data to Coveralls.\\n\\n**Note**: There is one gotcha I was fighting with for a while. If we don't have the `DebugType` element in our csproj defined (as it is not in the template that `dotnet new` generates), then the coverage generation will fail with an error saying that the Pdb file is missing.\\n\\nWe can fix this by adding this element and setting its value to `Full`.\\n\\n```xml\\n    <DebugType>full</DebugType>\\n```\\n\\n## Creating an HTML report\\n\\nThere is a simple tool for generating an HTML report from the OpenCover called [ReportGenerator](https://www.nuget.org/packages/ReportGenerator/). If we execute measuring the coverage in our CI pipeline, we can upload this report somewhere so we can freely analyze it.\\n\\nIf we upload the results to Coveralls, then this HTML report becomes less important, since we'll be browsing the report mostly on the Coveralls site, but it can still come handy if we want to generate the report locally, which is particularly useful if we are on a branch that we haven't pushed yet.\\n\\nAssuming the coverage result was saved to `coverage/coverage.xml`, the following command will generate the HTML report.\\n\\n```\\nReportGenerator.exe \\\\\\n  -reports:coverage/coverage.xml \\\\\\n  -targetdir:coverage \\\\\\n  -verbosity:Error\\n```\\n\\nIt'll output a bunch of HTML content, and we can browse the coverage by opening `index.htm`.\\n\\n# Uploading to Coveralls\\n\\nCoveralls is a service for collecting and tracking code coverage history and making it available through a web interface.\\n\\nLuckily it's free of charge for any open source project, and it can process the XML output format of OpenCover.\\n\\nWe can upload our previously generated report with the following command.\\n\\n```\\ncsmacnz.Coveralls.exe --opencover -i coverage/coverage.xml --useRelativePaths\\n```\\n\\nIn order for the command to work, the `COVERALLS_REPO_TOKEN` environment variable has to be set to contain the access token, which we can grab from the Coveralls website.\\n\\n# The full script\\n\\nYou can look at the [full script](https://github.com/markvincze/Stubbery/blob/master/coverage.sh) for coverage generation in the repository of [Stubbery](https://markvincze.github.io/Stubbery/), a library I've been working on for stubbing api dependencies. The script installs the necessary CLI tools at the beginning, so we can execute it both on the developer machines and on the agents without pre-installing anything.\\n\\nAnd there is a [separate script](https://github.com/markvincze/Stubbery/blob/master/coveralls.sh) for doing the coveralls report, but that's literally the single command I've shown above.\\n\\nCurrently I set up the [AppVeyor config file](https://github.com/markvincze/Stubbery/blob/master/appveyor.yml) so that the build and the coverage generation is executed only on commits to the master branch. It's also possible to set up the agents in a way that the coverage report runs on every branch. This is particularly nice, because Coveralls can leave a comment on the PR telling how the coverage changed.\\n\\n![Coveralls telling the coverage change on a PR.](/content/images/2016/11/coveralls-pr.png)\\n\\nWhen I'll have some more time, I want to look into how this can be set up with AppVeyor, and I'll update this post.\\n\\nThe last thing to do is to grab the badge from our Coveralls page, and add it to our README.\\n\\n[![Coverage badge on Stubbery](/content/images/2017/06/stubbery-badge-new.png)](https://github.com/markvincze/Stubbery)\\n\\nWith that we can either proudly demonstrate how well we're doing, or motivate us to increase the coverage even further. Happy testing! :)\"}]],\"sections\":[[10,0]]}",
        "html": "<p><strong>Update</strong>: The original version of this post was written when the <code>dotnet</code> toolchain was built on top of the <code>project.json</code> based project structure. Since then the <code>project.json</code> was deprecated, and .NET Core moved to a <code>csproj</code>-based approach.<br>\nBecause the <code>project.json</code> was completely deprecated, I decided not to write a new post, but simply adjust this one to work with the new toolchain (as of writing, that is <code>dotnet 1.1</code>).</p>\n<p>Unit test coverage is an interesting metric. It's not necessarily the most important one, and can be a bit misleading, but it gives at least an indication about how well our test suite covers our code base.</p>\n<p>100% unit tests coverage is often a stretch, and can require a huge amount of extra work. Over 90% we often hit the point of diminishing returns, and have to jump over lots of extra hoops to get those last couple of percents covered.</p>\n<p>Nevertheless, it's nice to keep the coverage reasonably high, I usually try to aim at keeping it over 90%. This ensures that most of the code are exercised during a test run, and that the majority of our components are loosely coupled, and are able to be isolated from their dependencies.</p>\n<p>I find it particularly handy to keep an eye on the change to the coverage caused by pull requests, and the gamification aspect of it is extra motivation for the devs to keep it high.</p>\n<p>As far as I can tell, the test coverage situation in .NET Core is not really established at this point. The .NET coverage tools don't have support for Core projects yet.<br>\nThe only one I could find which works today is <a href=\"https://github.com/OpenCover/opencover\">OpenCover</a>. In this post I'll cover setting up coverage generation with it and uploading it to Coveralls.</p>\n<p>Since OpenCover doesn't officially support .NET Core either, take everything in this post with a grain of salt, it definitely has the Works on My Machine badge, there is no guarantee the approach documented here won't break with an update to either .NET or OpenCover.</p>\n<h1 id=\"generatingcoverage\">Generating coverage</h1>\n<p>One significant limitation of OpenCover at this point is that it can only be used on Windows. This is problematic if our ecosystem (the developer machines and build agents) are completely Linux- or Mac-based. In this case we'll have to wait until OpenCover (or any other coverage tool) starts supporting Linux.</p>\n<p>On the other hand, if we are on Windows, coverage reporting already works nicely, although there are some gotchas we have to watch out for.</p>\n<p>To give credits where it's due: I'm not the first one getting this to work. I based my solution on the <a href=\"https://github.com/GoogleCloudPlatform/google-cloud-dotnet/blob/master/build.sh\">build script</a> used by the .NET SDK of the Google Cloud Platform.</p>\n<p>To generate the coverage for a particular test project we have to issue a single command with the OpenCover CLI (Assuming all the project dependencies have already been restored).</p>\n<pre><code class=\"language-bash\">$OPENCOVER \\\n  -target:&quot;c:\\Program Files\\dotnet\\dotnet.exe&quot; \\\n  -targetargs:&quot;test -f netcoreapp1.0 -c Release test/MyProject.UnitTests/MyProject.UnitTests.csproj&quot; \\\n  -mergeoutput \\\n  -hideskipped:File \\\n  -output:coverage/coverage.xml \\\n  -oldStyle \\\n  -filter:&quot;+[MyProject*]* -[MyProject.UnitTests*]*&quot; \\\n  -searchdirs:test/MyProject.UnitTests/bin/Release/netcoreapp1.0 \\\n  -register:user\n</code></pre>\n<ul>\n<li><code>-target</code> and <code>-targetargs</code>: These flags specify the operation which is executed, during which OpenCover collects the coverage statistics. In this case we specify executing the tests in our unit test project. (With a quick search I didn't find any recommendation about whether to use a Debug or a Release build, so I'm using Release, as it is done in <a href=\"https://github.com/GoogleCloudPlatform/google-cloud-dotnet\">google-cloud-dotnet</a>.)<br>\nIt's important to explicitly specify the framework we're using, so we can reliably set the binary output folder in the <code>-searchdirs</code> argument. We don't necessarily have to use <code>netcoreapp1.0</code>, <code>net451</code> works as well. (I haven't tried other flavors.)</li>\n<li><code>-output</code>: The path to the generated output file.</li>\n<li><code>-searchdirs</code>: We have to exactly specify the binary output folder of our test project. Keep in mind that the exact path is affected by the build configuration and the framework we're using. For example, in case of <code>net451</code>, instead of <code>/netcoreapp1.0/</code> we would have something like <code>/net451/win7-x64/</code> in the path.</li>\n<li><code>-filter</code>: This argument is important, it specifies which projects we want to include in the report, and which ones we exclude. The <a href=\"https://github.com/opencover/opencover/wiki/usage\">documentation</a> contains the details, but the pattern <code>+[MyProject*]* -[MyProject.UnitTests*]*</code> is enough to get started. It makes everything under MyProject included, and excludes the code under the test project.</li>\n<li><code>-mergeoutput, -hideskipped, -oldStyle, -register</code>: I didn't investigate too much in detail about these flags, I just copied them over from <a href=\"https://github.com/GoogleCloudPlatform/google-cloud-dotnet\">google-cloud-dotnet</a>. Although I tried to remove them to make my script a bit simpler, but removing any of them made the coverage generation fail with various errors.</li>\n</ul>\n<p>OpenCover saves the coverage into the specified XML file, which can the be used to generate an HTML report, and to upload the data to Coveralls.</p>\n<p><strong>Note</strong>: There is one gotcha I was fighting with for a while. If we don't have the <code>DebugType</code> element in our csproj defined (as it is not in the template that <code>dotnet new</code> generates), then the coverage generation will fail with an error saying that the Pdb file is missing.</p>\n<p>We can fix this by adding this element and setting its value to <code>Full</code>.</p>\n<pre><code class=\"language-xml\">    &lt;DebugType&gt;full&lt;/DebugType&gt;\n</code></pre>\n<h2 id=\"creatinganhtmlreport\">Creating an HTML report</h2>\n<p>There is a simple tool for generating an HTML report from the OpenCover called <a href=\"https://www.nuget.org/packages/ReportGenerator/\">ReportGenerator</a>. If we execute measuring the coverage in our CI pipeline, we can upload this report somewhere so we can freely analyze it.</p>\n<p>If we upload the results to Coveralls, then this HTML report becomes less important, since we'll be browsing the report mostly on the Coveralls site, but it can still come handy if we want to generate the report locally, which is particularly useful if we are on a branch that we haven't pushed yet.</p>\n<p>Assuming the coverage result was saved to <code>coverage/coverage.xml</code>, the following command will generate the HTML report.</p>\n<pre><code>ReportGenerator.exe \\\n  -reports:coverage/coverage.xml \\\n  -targetdir:coverage \\\n  -verbosity:Error\n</code></pre>\n<p>It'll output a bunch of HTML content, and we can browse the coverage by opening <code>index.htm</code>.</p>\n<h1 id=\"uploadingtocoveralls\">Uploading to Coveralls</h1>\n<p>Coveralls is a service for collecting and tracking code coverage history and making it available through a web interface.</p>\n<p>Luckily it's free of charge for any open source project, and it can process the XML output format of OpenCover.</p>\n<p>We can upload our previously generated report with the following command.</p>\n<pre><code>csmacnz.Coveralls.exe --opencover -i coverage/coverage.xml --useRelativePaths\n</code></pre>\n<p>In order for the command to work, the <code>COVERALLS_REPO_TOKEN</code> environment variable has to be set to contain the access token, which we can grab from the Coveralls website.</p>\n<h1 id=\"thefullscript\">The full script</h1>\n<p>You can look at the <a href=\"https://github.com/markvincze/Stubbery/blob/master/coverage.sh\">full script</a> for coverage generation in the repository of <a href=\"https://markvincze.github.io/Stubbery/\">Stubbery</a>, a library I've been working on for stubbing api dependencies. The script installs the necessary CLI tools at the beginning, so we can execute it both on the developer machines and on the agents without pre-installing anything.</p>\n<p>And there is a <a href=\"https://github.com/markvincze/Stubbery/blob/master/coveralls.sh\">separate script</a> for doing the coveralls report, but that's literally the single command I've shown above.</p>\n<p>Currently I set up the <a href=\"https://github.com/markvincze/Stubbery/blob/master/appveyor.yml\">AppVeyor config file</a> so that the build and the coverage generation is executed only on commits to the master branch. It's also possible to set up the agents in a way that the coverage report runs on every branch. This is particularly nice, because Coveralls can leave a comment on the PR telling how the coverage changed.</p>\n<p><img src=\"/content/images/2016/11/coveralls-pr.png\" alt=\"Coveralls telling the coverage change on a PR.\"></p>\n<p>When I'll have some more time, I want to look into how this can be set up with AppVeyor, and I'll update this post.</p>\n<p>The last thing to do is to grab the badge from our Coveralls page, and add it to our README.</p>\n<p><a href=\"https://github.com/markvincze/Stubbery\"><img src=\"/content/images/2017/06/stubbery-badge-new.png\" alt=\"Coverage badge on Stubbery\"></a></p>\n<p>With that we can either proudly demonstrate how well we're doing, or motivate us to increase the coverage even further. Happy testing! :)</p>\n",
        "comment_id": "27",
        "plaintext": "Update: The original version of this post was written when the dotnet  toolchain\nwas built on top of the project.json  based project structure. Since then the \nproject.json  was deprecated, and .NET Core moved to a csproj-based approach.\nBecause the project.json  was completely deprecated, I decided not to write a\nnew post, but simply adjust this one to work with the new toolchain (as of\nwriting, that is dotnet 1.1).\n\nUnit test coverage is an interesting metric. It's not necessarily the most\nimportant one, and can be a bit misleading, but it gives at least an indication\nabout how well our test suite covers our code base.\n\n100% unit tests coverage is often a stretch, and can require a huge amount of\nextra work. Over 90% we often hit the point of diminishing returns, and have to\njump over lots of extra hoops to get those last couple of percents covered.\n\nNevertheless, it's nice to keep the coverage reasonably high, I usually try to\naim at keeping it over 90%. This ensures that most of the code are exercised\nduring a test run, and that the majority of our components are loosely coupled,\nand are able to be isolated from their dependencies.\n\nI find it particularly handy to keep an eye on the change to the coverage caused\nby pull requests, and the gamification aspect of it is extra motivation for the\ndevs to keep it high.\n\nAs far as I can tell, the test coverage situation in .NET Core is not really\nestablished at this point. The .NET coverage tools don't have support for Core\nprojects yet.\nThe only one I could find which works today is OpenCover\n[https://github.com/OpenCover/opencover]. In this post I'll cover setting up\ncoverage generation with it and uploading it to Coveralls.\n\nSince OpenCover doesn't officially support .NET Core either, take everything in\nthis post with a grain of salt, it definitely has the Works on My Machine\nbadge, there is no guarantee the approach documented here won't break with an\nupdate to either .NET or OpenCover.\n\nGenerating coverage\nOne significant limitation of OpenCover at this point is that it can only be\nused on Windows. This is problematic if our ecosystem (the developer machines\nand build agents) are completely Linux- or Mac-based. In this case we'll have to\nwait until OpenCover (or any other coverage tool) starts supporting Linux.\n\nOn the other hand, if we are on Windows, coverage reporting already works\nnicely, although there are some gotchas we have to watch out for.\n\nTo give credits where it's due: I'm not the first one getting this to work. I\nbased my solution on the build script\n[https://github.com/GoogleCloudPlatform/google-cloud-dotnet/blob/master/build.sh] \n used by the .NET SDK of the Google Cloud Platform.\n\nTo generate the coverage for a particular test project we have to issue a single\ncommand with the OpenCover CLI (Assuming all the project dependencies have\nalready been restored).\n\n$OPENCOVER \\\n  -target:\"c:\\Program Files\\dotnet\\dotnet.exe\" \\\n  -targetargs:\"test -f netcoreapp1.0 -c Release test/MyProject.UnitTests/MyProject.UnitTests.csproj\" \\\n  -mergeoutput \\\n  -hideskipped:File \\\n  -output:coverage/coverage.xml \\\n  -oldStyle \\\n  -filter:\"+[MyProject*]* -[MyProject.UnitTests*]*\" \\\n  -searchdirs:test/MyProject.UnitTests/bin/Release/netcoreapp1.0 \\\n  -register:user\n\n\n * -target  and -targetargs: These flags specify the operation which is\n   executed, during which OpenCover collects the coverage statistics. In this\n   case we specify executing the tests in our unit test project. (With a quick\n   search I didn't find any recommendation about whether to use a Debug or a\n   Release build, so I'm using Release, as it is done in google-cloud-dotnet\n   [https://github.com/GoogleCloudPlatform/google-cloud-dotnet].)\n   It's important to explicitly specify the framework we're using, so we can\n   reliably set the binary output folder in the -searchdirs  argument. We don't\n   necessarily have to use netcoreapp1.0, net451  works as well. (I haven't\n   tried other flavors.)\n * -output: The path to the generated output file.\n * -searchdirs: We have to exactly specify the binary output folder of our test\n   project. Keep in mind that the exact path is affected by the build\n   configuration and the framework we're using. For example, in case of net451,\n   instead of /netcoreapp1.0/  we would have something like /net451/win7-x64/ \n   in the path.\n * -filter: This argument is important, it specifies which projects we want to\n   include in the report, and which ones we exclude. The documentation\n   [https://github.com/opencover/opencover/wiki/usage]  contains the details,\n   but the pattern +[MyProject*]* -[MyProject.UnitTests*]*  is enough to get\n   started. It makes everything under MyProject included, and excludes the code\n   under the test project.\n * -mergeoutput, -hideskipped, -oldStyle, -register: I didn't investigate too\n   much in detail about these flags, I just copied them over from \n   google-cloud-dotnet\n   [https://github.com/GoogleCloudPlatform/google-cloud-dotnet]. Although I\n   tried to remove them to make my script a bit simpler, but removing any of\n   them made the coverage generation fail with various errors.\n\nOpenCover saves the coverage into the specified XML file, which can the be used\nto generate an HTML report, and to upload the data to Coveralls.\n\nNote: There is one gotcha I was fighting with for a while. If we don't have the \nDebugType  element in our csproj defined (as it is not in the template that \ndotnet new  generates), then the coverage generation will fail with an error\nsaying that the Pdb file is missing.\n\nWe can fix this by adding this element and setting its value to Full.\n\n    <DebugType>full</DebugType>\n\n\nCreating an HTML report\nThere is a simple tool for generating an HTML report from the OpenCover called \nReportGenerator [https://www.nuget.org/packages/ReportGenerator/]. If we execute\nmeasuring the coverage in our CI pipeline, we can upload this report somewhere\nso we can freely analyze it.\n\nIf we upload the results to Coveralls, then this HTML report becomes less\nimportant, since we'll be browsing the report mostly on the Coveralls site, but\nit can still come handy if we want to generate the report locally, which is\nparticularly useful if we are on a branch that we haven't pushed yet.\n\nAssuming the coverage result was saved to coverage/coverage.xml, the following\ncommand will generate the HTML report.\n\nReportGenerator.exe \\\n  -reports:coverage/coverage.xml \\\n  -targetdir:coverage \\\n  -verbosity:Error\n\n\nIt'll output a bunch of HTML content, and we can browse the coverage by opening \nindex.htm.\n\nUploading to Coveralls\nCoveralls is a service for collecting and tracking code coverage history and\nmaking it available through a web interface.\n\nLuckily it's free of charge for any open source project, and it can process the\nXML output format of OpenCover.\n\nWe can upload our previously generated report with the following command.\n\ncsmacnz.Coveralls.exe --opencover -i coverage/coverage.xml --useRelativePaths\n\n\nIn order for the command to work, the COVERALLS_REPO_TOKEN  environment variable\nhas to be set to contain the access token, which we can grab from the Coveralls\nwebsite.\n\nThe full script\nYou can look at the full script\n[https://github.com/markvincze/Stubbery/blob/master/coverage.sh]  for coverage\ngeneration in the repository of Stubbery\n[https://markvincze.github.io/Stubbery/], a library I've been working on for\nstubbing api dependencies. The script installs the necessary CLI tools at the\nbeginning, so we can execute it both on the developer machines and on the agents\nwithout pre-installing anything.\n\nAnd there is a separate script\n[https://github.com/markvincze/Stubbery/blob/master/coveralls.sh]  for doing the\ncoveralls report, but that's literally the single command I've shown above.\n\nCurrently I set up the AppVeyor config file\n[https://github.com/markvincze/Stubbery/blob/master/appveyor.yml]  so that the\nbuild and the coverage generation is executed only on commits to the master\nbranch. It's also possible to set up the agents in a way that the coverage\nreport runs on every branch. This is particularly nice, because Coveralls can\nleave a comment on the PR telling how the coverage changed.\n\n\n\nWhen I'll have some more time, I want to look into how this can be set up with\nAppVeyor, and I'll update this post.\n\nThe last thing to do is to grab the badge from our Coveralls page, and add it to\nour README.\n\n  [https://github.com/markvincze/Stubbery]\n\nWith that we can either proudly demonstrate how well we're doing, or motivate us\nto increase the coverage even further. Happy testing! :)",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": "Test coverage using Coveralls with OpenCover for a .NET Core project",
        "meta_description": "This post describes setting up coverage generation with OpenCover for a .NET Core project, and shows how the report can be uploaded to Coveralls.",
        "author_id": "1",
        "created_at": "2016-11-06 00:39:02",
        "created_by": "1",
        "updated_at": "2017-06-14 19:58:45",
        "updated_by": "1",
        "published_at": "2017-06-14 19:46:00",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2973",
        "uuid": "5069d976-86d7-4370-9e41-172607054372",
        "title": "Jumpstart F# web development: F# with ASP.NET Core",
        "slug": "jumpstart-f-web-development-f-with-asp-net-core",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In this third part of the [series](/series-jumpstart-f-web-development) we'll look at how we can get started with developing an ASP.NET Core application using F#. This scenario is pretty straightforward, there are no extra hoops to jump over. In this post I'll describe the steps necessary to create a new ASP.NET application.\\n\\nIn ASP.NET Core we typically use the Kestrel web server to host our application, which is technically started up from a Console application. This is important for .NET Core, since the platform has to enable developers working on Linux or Mac, who don't have Visual Studio. So the developer experience have to be streamlined from the terminal as well.\\n\\nThis is good news, since we won't have any more problems with project types or Visual Studio (as we had [when we did F# and ASP.NET on the full .NET Framework]()).\\n\\n# Create the project\\n\\nWe can simply create a new empty F# project using the `dotnet` CLI by issuing the following command.\\n\\n```\\ndotnet new --lang fsharp\\n```\\n\\nThis command scaffolds an empty Console application for us. In order to use ASP.NET, add the following package dependencies to our `projet.json`.\\n\\n```json\\n      \\\"dependencies\\\": {\\n        ...\\n        \\\"Microsoft.AspNetCore.Mvc\\\": \\\"1.1.0\\\",\\n        \\\"Microsoft.AspNetCore.Server.Kestrel\\\": \\\"1.1.0\\\",\\n        \\\"Microsoft.AspNetCore.Diagnostics\\\": \\\"1.1.0\\\",\\n        \\\"Microsoft.AspNetCore.Hosting\\\": \\\"1.1.0\\\",\\n        \\\"Microsoft.Extensions.Logging.Console\\\": \\\"1.1.0\\\"\\n      }\\n```\\n\\n# F# source files in .NET Core\\n\\nIt's a bit inconvenient that with the current tooling we manually have to manage the list of our source files in the `project.json` file. Every source file in the project has to be listed in the `buildOptions/compile/includeFiles` property. It's important that the files have to be listed in the proper order. If a source file depends on another one, then it has to come later in the list than the one it depends on.\\n\\nFor at the end of this post, when we have the `Program.fs`, `Startup.fs` and the `HelloController.fs` files, the configuration should look like this.\\n\\n```json\\n{\\n  \\\"buildOptions\\\": {\\n    ...\\n    \\\"compile\\\": {\\n      \\\"includeFiles\\\": [\\n        \\\"Controllers/HelloController.fs\\\",\\n        \\\"Startup.fs\\\",\\n        \\\"Program.fs\\\"\\n      ]\\n    }\\n  },\\n  ...\\n}\\n```\\n\\n# Bootstrap the application\\n\\nIn our `Program.fs` file we have to start up the hosting of our application with Kestrel. In order to do that, replace the `main` function with the following code.\\n\\n```fsharp\\nopen JumpstartAspnetCoreFsharp\\nopen System\\nopen System.IO\\nopen Microsoft.AspNetCore.Hosting\\n\\n[<EntryPoint>]\\nlet main argv =\\n  let host =\\n    WebHostBuilder()\\n      .UseKestrel()\\n      .UseUrls(\\\"http://*:5000/\\\")\\n      .UseContentRoot(Directory.GetCurrentDirectory())\\n      .UseStartup<Startup>()\\n      .Build()\\n\\n  host.Run()\\n  0 // return an integer exit code\\n```\\n\\n# Set up our application\\n\\nThe type `Startup` should describe what our application is going to do. Let's add a new source file, `Startup.fs` to the project, create the `Startup` type, and add the calls necessary to use MVC.\\n\\n```fsharp\\nnamespace JumpstartAspnetCoreFsharp\\n\\nopen Microsoft.AspNetCore.Builder\\nopen Microsoft.AspNetCore.Hosting\\nopen Microsoft.Extensions.DependencyInjection\\nopen Microsoft.Extensions.Logging\\nopen System\\nopen System.IO\\n\\ntype Startup() =\\n    member __.ConfigureServices(services: IServiceCollection) =\\n        services.AddMvc() |> ignore\\n    member __.Configure (app : IApplicationBuilder)\\n                        (env : IHostingEnvironment)\\n                        (loggerFactory : ILoggerFactory) =\\n        loggerFactory.AddConsole() |> ignore\\n        app.UseMvc() |> ignore\\n```\\n\\n# Implement our controller\\n\\nThe last step is to implement our controller. The usual pattern is to put our controllers into a folder called `Controllers`. We can add a file to the project called `HelloController.fs`. The implementaion will be very similar to the one used in full .NET.\\n\\n```fsharp\\nnamespace JumpstartAspnetCoreFsharp.Controllers\\n\\nopen System\\nopen Microsoft.AspNetCore.Mvc\\n\\n[<Route(\\\"[controller]\\\")>]\\ntype HelloController() =\\n    inherit Controller()\\n    member this.Get() =\\n        this.Ok \\\"Hello from F# and ASP.NET Core!\\\"\\n```\\n\\n# Start the application\\n\\nThere is nothing else to do, we can go ahead restore the Nuget packages and start our application by issuing the following commands.\\n\\n```bash\\ndotnet restore\\ndotnet run\\n```\\n\\nThen we can navigate to http://localhost:5000/Hello to open the endpoint we implemented.\\n\\n# Tooling\\n\\nThe F# tooling for .NET Core seems to have some rough edges at this point. I tried using Visual Studio on Windows, and also VSCode with the [ionide-fsharp](http://ionide.io/) extensions. They both tend to be a bit flaky, occasionally the IntelliSense and the autocomplete stops working. In this case we should rebuild the project, and restart the editor.\\n\\nOnce the tooling of .NET Core projects gets finalized with the release of VS2017 and the new version of the `dotnet` CLI, I expect the F# tooling to catch up and fix these issues quickly.\\n\\n**Source**: I uploaded a complete working example to [this repository](https://github.com/markvincze/jumpstart-aspnetcore-fsharp).\\n\\nWe've seen that F# + ASP.NET Core is a good match, and works nicely out of the box.\\nIn the last part we will take a look at how to create a new project on .NET Core using Suave.IO.\"}]],\"sections\":[[10,0]]}",
        "html": "<p>In this third part of the <a href=\"/series-jumpstart-f-web-development\">series</a> we'll look at how we can get started with developing an ASP.NET Core application using F#. This scenario is pretty straightforward, there are no extra hoops to jump over. In this post I'll describe the steps necessary to create a new ASP.NET application.</p>\n<p>In ASP.NET Core we typically use the Kestrel web server to host our application, which is technically started up from a Console application. This is important for .NET Core, since the platform has to enable developers working on Linux or Mac, who don't have Visual Studio. So the developer experience have to be streamlined from the terminal as well.</p>\n<p>This is good news, since we won't have any more problems with project types or Visual Studio (as we had <a href=\"\">when we did F# and ASP.NET on the full .NET Framework</a>).</p>\n<h1 id=\"createtheproject\">Create the project</h1>\n<p>We can simply create a new empty F# project using the <code>dotnet</code> CLI by issuing the following command.</p>\n<pre><code>dotnet new --lang fsharp\n</code></pre>\n<p>This command scaffolds an empty Console application for us. In order to use ASP.NET, add the following package dependencies to our <code>projet.json</code>.</p>\n<pre><code class=\"language-json\">      &quot;dependencies&quot;: {\n        ...\n        &quot;Microsoft.AspNetCore.Mvc&quot;: &quot;1.1.0&quot;,\n        &quot;Microsoft.AspNetCore.Server.Kestrel&quot;: &quot;1.1.0&quot;,\n        &quot;Microsoft.AspNetCore.Diagnostics&quot;: &quot;1.1.0&quot;,\n        &quot;Microsoft.AspNetCore.Hosting&quot;: &quot;1.1.0&quot;,\n        &quot;Microsoft.Extensions.Logging.Console&quot;: &quot;1.1.0&quot;\n      }\n</code></pre>\n<h1 id=\"fsourcefilesinnetcore\">F# source files in .NET Core</h1>\n<p>It's a bit inconvenient that with the current tooling we manually have to manage the list of our source files in the <code>project.json</code> file. Every source file in the project has to be listed in the <code>buildOptions/compile/includeFiles</code> property. It's important that the files have to be listed in the proper order. If a source file depends on another one, then it has to come later in the list than the one it depends on.</p>\n<p>For at the end of this post, when we have the <code>Program.fs</code>, <code>Startup.fs</code> and the <code>HelloController.fs</code> files, the configuration should look like this.</p>\n<pre><code class=\"language-json\">{\n  &quot;buildOptions&quot;: {\n    ...\n    &quot;compile&quot;: {\n      &quot;includeFiles&quot;: [\n        &quot;Controllers/HelloController.fs&quot;,\n        &quot;Startup.fs&quot;,\n        &quot;Program.fs&quot;\n      ]\n    }\n  },\n  ...\n}\n</code></pre>\n<h1 id=\"bootstraptheapplication\">Bootstrap the application</h1>\n<p>In our <code>Program.fs</code> file we have to start up the hosting of our application with Kestrel. In order to do that, replace the <code>main</code> function with the following code.</p>\n<pre><code class=\"language-fsharp\">open JumpstartAspnetCoreFsharp\nopen System\nopen System.IO\nopen Microsoft.AspNetCore.Hosting\n\n[&lt;EntryPoint&gt;]\nlet main argv =\n  let host =\n    WebHostBuilder()\n      .UseKestrel()\n      .UseUrls(&quot;http://*:5000/&quot;)\n      .UseContentRoot(Directory.GetCurrentDirectory())\n      .UseStartup&lt;Startup&gt;()\n      .Build()\n\n  host.Run()\n  0 // return an integer exit code\n</code></pre>\n<h1 id=\"setupourapplication\">Set up our application</h1>\n<p>The type <code>Startup</code> should describe what our application is going to do. Let's add a new source file, <code>Startup.fs</code> to the project, create the <code>Startup</code> type, and add the calls necessary to use MVC.</p>\n<pre><code class=\"language-fsharp\">namespace JumpstartAspnetCoreFsharp\n\nopen Microsoft.AspNetCore.Builder\nopen Microsoft.AspNetCore.Hosting\nopen Microsoft.Extensions.DependencyInjection\nopen Microsoft.Extensions.Logging\nopen System\nopen System.IO\n\ntype Startup() =\n    member __.ConfigureServices(services: IServiceCollection) =\n        services.AddMvc() |&gt; ignore\n    member __.Configure (app : IApplicationBuilder)\n                        (env : IHostingEnvironment)\n                        (loggerFactory : ILoggerFactory) =\n        loggerFactory.AddConsole() |&gt; ignore\n        app.UseMvc() |&gt; ignore\n</code></pre>\n<h1 id=\"implementourcontroller\">Implement our controller</h1>\n<p>The last step is to implement our controller. The usual pattern is to put our controllers into a folder called <code>Controllers</code>. We can add a file to the project called <code>HelloController.fs</code>. The implementaion will be very similar to the one used in full .NET.</p>\n<pre><code class=\"language-fsharp\">namespace JumpstartAspnetCoreFsharp.Controllers\n\nopen System\nopen Microsoft.AspNetCore.Mvc\n\n[&lt;Route(&quot;[controller]&quot;)&gt;]\ntype HelloController() =\n    inherit Controller()\n    member this.Get() =\n        this.Ok &quot;Hello from F# and ASP.NET Core!&quot;\n</code></pre>\n<h1 id=\"starttheapplication\">Start the application</h1>\n<p>There is nothing else to do, we can go ahead restore the Nuget packages and start our application by issuing the following commands.</p>\n<pre><code class=\"language-bash\">dotnet restore\ndotnet run\n</code></pre>\n<p>Then we can navigate to <a href=\"http://localhost:5000/Hello\">http://localhost:5000/Hello</a> to open the endpoint we implemented.</p>\n<h1 id=\"tooling\">Tooling</h1>\n<p>The F# tooling for .NET Core seems to have some rough edges at this point. I tried using Visual Studio on Windows, and also VSCode with the <a href=\"http://ionide.io/\">ionide-fsharp</a> extensions. They both tend to be a bit flaky, occasionally the IntelliSense and the autocomplete stops working. In this case we should rebuild the project, and restart the editor.</p>\n<p>Once the tooling of .NET Core projects gets finalized with the release of VS2017 and the new version of the <code>dotnet</code> CLI, I expect the F# tooling to catch up and fix these issues quickly.</p>\n<p><strong>Source</strong>: I uploaded a complete working example to <a href=\"https://github.com/markvincze/jumpstart-aspnetcore-fsharp\">this repository</a>.</p>\n<p>We've seen that F# + ASP.NET Core is a good match, and works nicely out of the box.<br>\nIn the last part we will take a look at how to create a new project on .NET Core using Suave.IO.</p>\n",
        "comment_id": "32",
        "plaintext": "In this third part of the series [/series-jumpstart-f-web-development]  we'll\nlook at how we can get started with developing an ASP.NET Core application using\nF#. This scenario is pretty straightforward, there are no extra hoops to jump\nover. In this post I'll describe the steps necessary to create a new ASP.NET\napplication.\n\nIn ASP.NET Core we typically use the Kestrel web server to host our application,\nwhich is technically started up from a Console application. This is important\nfor .NET Core, since the platform has to enable developers working on Linux or\nMac, who don't have Visual Studio. So the developer experience have to be\nstreamlined from the terminal as well.\n\nThis is good news, since we won't have any more problems with project types or\nVisual Studio (as we had when we did F# and ASP.NET on the full .NET Framework).\n\nCreate the project\nWe can simply create a new empty F# project using the dotnet  CLI by issuing the\nfollowing command.\n\ndotnet new --lang fsharp\n\n\nThis command scaffolds an empty Console application for us. In order to use\nASP.NET, add the following package dependencies to our projet.json.\n\n      \"dependencies\": {\n        ...\n        \"Microsoft.AspNetCore.Mvc\": \"1.1.0\",\n        \"Microsoft.AspNetCore.Server.Kestrel\": \"1.1.0\",\n        \"Microsoft.AspNetCore.Diagnostics\": \"1.1.0\",\n        \"Microsoft.AspNetCore.Hosting\": \"1.1.0\",\n        \"Microsoft.Extensions.Logging.Console\": \"1.1.0\"\n      }\n\n\nF# source files in .NET Core\nIt's a bit inconvenient that with the current tooling we manually have to manage\nthe list of our source files in the project.json  file. Every source file in the\nproject has to be listed in the buildOptions/compile/includeFiles  property.\nIt's important that the files have to be listed in the proper order. If a source\nfile depends on another one, then it has to come later in the list than the one\nit depends on.\n\nFor at the end of this post, when we have the Program.fs, Startup.fs  and the \nHelloController.fs  files, the configuration should look like this.\n\n{\n  \"buildOptions\": {\n    ...\n    \"compile\": {\n      \"includeFiles\": [\n        \"Controllers/HelloController.fs\",\n        \"Startup.fs\",\n        \"Program.fs\"\n      ]\n    }\n  },\n  ...\n}\n\n\nBootstrap the application\nIn our Program.fs  file we have to start up the hosting of our application with\nKestrel. In order to do that, replace the main  function with the following\ncode.\n\nopen JumpstartAspnetCoreFsharp\nopen System\nopen System.IO\nopen Microsoft.AspNetCore.Hosting\n\n[<EntryPoint>]\nlet main argv =\n  let host =\n    WebHostBuilder()\n      .UseKestrel()\n      .UseUrls(\"http://*:5000/\")\n      .UseContentRoot(Directory.GetCurrentDirectory())\n      .UseStartup<Startup>()\n      .Build()\n\n  host.Run()\n  0 // return an integer exit code\n\n\nSet up our application\nThe type Startup  should describe what our application is going to do. Let's add\na new source file, Startup.fs  to the project, create the Startup  type, and add\nthe calls necessary to use MVC.\n\nnamespace JumpstartAspnetCoreFsharp\n\nopen Microsoft.AspNetCore.Builder\nopen Microsoft.AspNetCore.Hosting\nopen Microsoft.Extensions.DependencyInjection\nopen Microsoft.Extensions.Logging\nopen System\nopen System.IO\n\ntype Startup() =\n    member __.ConfigureServices(services: IServiceCollection) =\n        services.AddMvc() |> ignore\n    member __.Configure (app : IApplicationBuilder)\n                        (env : IHostingEnvironment)\n                        (loggerFactory : ILoggerFactory) =\n        loggerFactory.AddConsole() |> ignore\n        app.UseMvc() |> ignore\n\n\nImplement our controller\nThe last step is to implement our controller. The usual pattern is to put our\ncontrollers into a folder called Controllers. We can add a file to the project\ncalled HelloController.fs. The implementaion will be very similar to the one\nused in full .NET.\n\nnamespace JumpstartAspnetCoreFsharp.Controllers\n\nopen System\nopen Microsoft.AspNetCore.Mvc\n\n[<Route(\"[controller]\")>]\ntype HelloController() =\n    inherit Controller()\n    member this.Get() =\n        this.Ok \"Hello from F# and ASP.NET Core!\"\n\n\nStart the application\nThere is nothing else to do, we can go ahead restore the Nuget packages and\nstart our application by issuing the following commands.\n\ndotnet restore\ndotnet run\n\n\nThen we can navigate to http://localhost:5000/Hello  to open the endpoint we\nimplemented.\n\nTooling\nThe F# tooling for .NET Core seems to have some rough edges at this point. I\ntried using Visual Studio on Windows, and also VSCode with the ionide-fsharp\n[http://ionide.io/]  extensions. They both tend to be a bit flaky, occasionally\nthe IntelliSense and the autocomplete stops working. In this case we should\nrebuild the project, and restart the editor.\n\nOnce the tooling of .NET Core projects gets finalized with the release of VS2017\nand the new version of the dotnet  CLI, I expect the F# tooling to catch up and\nfix these issues quickly.\n\nSource: I uploaded a complete working example to this repository\n[https://github.com/markvincze/jumpstart-aspnetcore-fsharp].\n\nWe've seen that F# + ASP.NET Core is a good match, and works nicely out of the\nbox.\nIn the last part we will take a look at how to create a new project on .NET Core\nusing Suave.IO.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "An introduction to get started with web development in F#, using ASP.NET Core.",
        "author_id": "1",
        "created_at": "2017-02-26 20:22:33",
        "created_by": "1",
        "updated_at": "2017-04-21 13:30:41",
        "updated_by": "1",
        "published_at": "2017-02-26 20:25:14",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2974",
        "uuid": "2e5dd26e-ff7e-4496-9b83-1b33117c9263",
        "title": "Jumpstart F# web development: F# with Suave.IO on classic .NET",
        "slug": "jumpstart-f-web-development-f-with-suave-io-on-classic-net",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In the second part of [the series](/series-jumpstart-f-web-development) we take a look at how we can develop a web application using Sauve on the classic .NET Framework.\\n\\n![Suave logo](/content/images/2017/02/suave-logo.png)\\n\\n[Suave](https://suave.io/) is a lightweight web framework and server, which was implemented in F# from the ground up with functional programming in mind, using constructs which fit the F# word nicely, so we won't have to create .NET classes or use inheritence and state mutation to configure our application (as we had to do with ASP.NET).\\n\\nAnd based on its logo, you can already tell it's more hipster and cool to use Suave than anything else ;).\\n\\n# Create a new project with Visual Studio\\n\\nI mentioned that Suave is not just a framework, but also a web *server* in one library. This means that we don't need another server, like IIS to start up our application, so we don't need any special Visual Studio integration either.\\n\\nThus we can go ahead and simply create a new F# Console application, which, by default contains a `Program.fs` file with the following content.\\n\\n```fsharp\\n[<EntryPoint>]\\nlet main argv =\\n    printfn \\\"%A\\\" argv\\n    0 // return an integer exit code\\n```\\n\\nWe have to add the `Suave` nuget package to our project (by typing `Install-Package Suave` into the Package Manager Console, or using the GUI).\\n\\nThen we can implement a basic web application by replacing the default `main` function with the following code.\\n\\n```fsharp\\nopen Suave\\n\\n[<EntryPoint>]\\nlet main argv = \\n    startWebServer defaultConfig (Successful.OK \\\"Hello from Suave!\\\")\\n    0\\n```\\n\\nOr we can implement something more complicated by handling some different http methods and routes.\\n\\n```fsharp\\nlet app =\\n    choose\\n        [ GET >=> choose\\n            [ path \\\"/status\\\" >=> OK \\\"Service is UP\\\"\\n              path \\\"/hello\\\" >=> OK \\\"Hello World!\\\" ]\\n          POST >=> OK \\\"This was a POST\\\" ]\\n\\n[<EntryPoint>]\\nlet main argv = \\n    startWebServer defaultConfig app\\n    0 // return an integer exit code\\n```\\n\\nThe cool thing about Suave that it's programming model fits the functional constructs snugly, so it feels much more like a first-class citizen in the F# world than ASP.NET.  \\nFunctions and the composability features of F# really shine when setting up our routes and the various endpoints of our web application, and a complicated app can be described with surprisingly little, terse code.\\n\\nYou can read much more about Suave's capabilities on the [official site](https://suave.io/).\\n\\n# Without VS: embrace the open tools\\n\\nSince we don't need intricate Visual Studio integration to start up our web server, it's also an option to develop completely without VS, which comes especially handy if we want to work on a Mac with Mono (which is also supported by F# and Suave).\\n\\nIn order to do this we can utilize two open source tools.\\n\\n - [Paket](https://fsprojects.github.io/Paket/): A dependency manager for .NET and Mono, which is easy to use from the command line, and supports using Nuget too as the source of our dependencies.\\n - [Fake](http://fsharp.github.io/FAKE/): \\\"Make for F\\\", a scriptable, CLI-friendly build automation tool for F#.\\n\\nWith these tools it's relatively easy to set up a project with a couple of scripts, which is completely usable from the terminal without Visual Studio.\\n\\nYou can find a minimal example of this setup in my [jumpstart-suave](https://github.com/markvincze/jumpstart-suave) repository. It contains the following elements.\\n\\n - build.cmd: The main startup script, it does the following steps.\\n    1. If it's not present yet, it installs paket.\\n    2. If we don't have a `paket.lock` file (we don't have the actual package versions fixed, and haven't discovered the transitive dependencies), installs all the packages.\\n    3. Restores the actual packages.\\n    4. Starts up our main script by calling `FAKE`.\\n - build.fsx: Defines the `\\\"run\\\"` default target, which simply starts up the Suave web application (the same way we did in our VS Console project).\\n - app.fsx: The implementation of our main `app` function, which in this example returns the same 200 OK for every request.\\n - paket.dependencies: Specifies the external packages we want to install with Paket.\\n - paket.lock: The fixed down versions of all our direct and transitive dependencies. (This file is normally also commited to source control.)\\n\\n(I based this solution on a [more complicated repo](https://github.com/tpetricek/suave-xplat-gettingstarted) set up by Tomas Petricek, which also includes automatic reloading in case of a file change, and deployment to Azure and Heroku.)\\n\\nThis is a completely self-contained example, simply executing `build.cmd` will download all the dependencies, build the project and start up our webserver, straight from the command line.\\n\\n# Conclusion\\n\\nWith these examples we can see that Suave is a cool, lightweight and fun to use web framework, which fits really nicely into the functional F# world. And because the web server is also part of the library, and we have some CLI tools to install our dependencies and build the project, we can even do the whole development flow without Visual Studio.\\n\\nWhether to use Suave or ASP.NET is not a trivial question though. If we start a greenfield development project, I'd probably give Suave a go, just because it is more suitable for functional programming, but if we are migrating an existing ASP.NET application to F#, or we depend on existing libraries which are ASP.NET-specific, we might be better off sticking to Microsoft's framework.\\n\\nIn the next post of [this series](/series-jumpstart-f-web-development) we'll move from the classic .NET Framework to .NET Core, and take a look at how we can use F# with ASP.NET there.\"}]],\"sections\":[[10,0]]}",
        "html": "<p>In the second part of <a href=\"/series-jumpstart-f-web-development\">the series</a> we take a look at how we can develop a web application using Sauve on the classic .NET Framework.</p>\n<p><img src=\"/content/images/2017/02/suave-logo.png\" alt=\"Suave logo\"></p>\n<p><a href=\"https://suave.io/\">Suave</a> is a lightweight web framework and server, which was implemented in F# from the ground up with functional programming in mind, using constructs which fit the F# word nicely, so we won't have to create .NET classes or use inheritence and state mutation to configure our application (as we had to do with ASP.NET).</p>\n<p>And based on its logo, you can already tell it's more hipster and cool to use Suave than anything else ;).</p>\n<h1 id=\"createanewprojectwithvisualstudio\">Create a new project with Visual Studio</h1>\n<p>I mentioned that Suave is not just a framework, but also a web <em>server</em> in one library. This means that we don't need another server, like IIS to start up our application, so we don't need any special Visual Studio integration either.</p>\n<p>Thus we can go ahead and simply create a new F# Console application, which, by default contains a <code>Program.fs</code> file with the following content.</p>\n<pre><code class=\"language-fsharp\">[&lt;EntryPoint&gt;]\nlet main argv =\n    printfn &quot;%A&quot; argv\n    0 // return an integer exit code\n</code></pre>\n<p>We have to add the <code>Suave</code> nuget package to our project (by typing <code>Install-Package Suave</code> into the Package Manager Console, or using the GUI).</p>\n<p>Then we can implement a basic web application by replacing the default <code>main</code> function with the following code.</p>\n<pre><code class=\"language-fsharp\">open Suave\n\n[&lt;EntryPoint&gt;]\nlet main argv = \n    startWebServer defaultConfig (Successful.OK &quot;Hello from Suave!&quot;)\n    0\n</code></pre>\n<p>Or we can implement something more complicated by handling some different http methods and routes.</p>\n<pre><code class=\"language-fsharp\">let app =\n    choose\n        [ GET &gt;=&gt; choose\n            [ path &quot;/status&quot; &gt;=&gt; OK &quot;Service is UP&quot;\n              path &quot;/hello&quot; &gt;=&gt; OK &quot;Hello World!&quot; ]\n          POST &gt;=&gt; OK &quot;This was a POST&quot; ]\n\n[&lt;EntryPoint&gt;]\nlet main argv = \n    startWebServer defaultConfig app\n    0 // return an integer exit code\n</code></pre>\n<p>The cool thing about Suave that it's programming model fits the functional constructs snugly, so it feels much more like a first-class citizen in the F# world than ASP.NET.<br>\nFunctions and the composability features of F# really shine when setting up our routes and the various endpoints of our web application, and a complicated app can be described with surprisingly little, terse code.</p>\n<p>You can read much more about Suave's capabilities on the <a href=\"https://suave.io/\">official site</a>.</p>\n<h1 id=\"withoutvsembracetheopentools\">Without VS: embrace the open tools</h1>\n<p>Since we don't need intricate Visual Studio integration to start up our web server, it's also an option to develop completely without VS, which comes especially handy if we want to work on a Mac with Mono (which is also supported by F# and Suave).</p>\n<p>In order to do this we can utilize two open source tools.</p>\n<ul>\n<li><a href=\"https://fsprojects.github.io/Paket/\">Paket</a>: A dependency manager for .NET and Mono, which is easy to use from the command line, and supports using Nuget too as the source of our dependencies.</li>\n<li><a href=\"http://fsharp.github.io/FAKE/\">Fake</a>: &quot;Make for F&quot;, a scriptable, CLI-friendly build automation tool for F#.</li>\n</ul>\n<p>With these tools it's relatively easy to set up a project with a couple of scripts, which is completely usable from the terminal without Visual Studio.</p>\n<p>You can find a minimal example of this setup in my <a href=\"https://github.com/markvincze/jumpstart-suave\">jumpstart-suave</a> repository. It contains the following elements.</p>\n<ul>\n<li>build.cmd: The main startup script, it does the following steps.\n<ol>\n<li>If it's not present yet, it installs paket.</li>\n<li>If we don't have a <code>paket.lock</code> file (we don't have the actual package versions fixed, and haven't discovered the transitive dependencies), installs all the packages.</li>\n<li>Restores the actual packages.</li>\n<li>Starts up our main script by calling <code>FAKE</code>.</li>\n</ol>\n</li>\n<li>build.fsx: Defines the <code>&quot;run&quot;</code> default target, which simply starts up the Suave web application (the same way we did in our VS Console project).</li>\n<li>app.fsx: The implementation of our main <code>app</code> function, which in this example returns the same 200 OK for every request.</li>\n<li>paket.dependencies: Specifies the external packages we want to install with Paket.</li>\n<li>paket.lock: The fixed down versions of all our direct and transitive dependencies. (This file is normally also commited to source control.)</li>\n</ul>\n<p>(I based this solution on a <a href=\"https://github.com/tpetricek/suave-xplat-gettingstarted\">more complicated repo</a> set up by Tomas Petricek, which also includes automatic reloading in case of a file change, and deployment to Azure and Heroku.)</p>\n<p>This is a completely self-contained example, simply executing <code>build.cmd</code> will download all the dependencies, build the project and start up our webserver, straight from the command line.</p>\n<h1 id=\"conclusion\">Conclusion</h1>\n<p>With these examples we can see that Suave is a cool, lightweight and fun to use web framework, which fits really nicely into the functional F# world. And because the web server is also part of the library, and we have some CLI tools to install our dependencies and build the project, we can even do the whole development flow without Visual Studio.</p>\n<p>Whether to use Suave or ASP.NET is not a trivial question though. If we start a greenfield development project, I'd probably give Suave a go, just because it is more suitable for functional programming, but if we are migrating an existing ASP.NET application to F#, or we depend on existing libraries which are ASP.NET-specific, we might be better off sticking to Microsoft's framework.</p>\n<p>In the next post of <a href=\"/series-jumpstart-f-web-development\">this series</a> we'll move from the classic .NET Framework to .NET Core, and take a look at how we can use F# with ASP.NET there.</p>\n",
        "comment_id": "31",
        "plaintext": "In the second part of the series [/series-jumpstart-f-web-development]  we take\na look at how we can develop a web application using Sauve on the classic .NET\nFramework.\n\n\n\nSuave [https://suave.io/]  is a lightweight web framework and server, which was\nimplemented in F# from the ground up with functional programming in mind, using\nconstructs which fit the F# word nicely, so we won't have to create .NET classes\nor use inheritence and state mutation to configure our application (as we had to\ndo with ASP.NET).\n\nAnd based on its logo, you can already tell it's more hipster and cool to use\nSuave than anything else ;).\n\nCreate a new project with Visual Studio\nI mentioned that Suave is not just a framework, but also a web server  in one\nlibrary. This means that we don't need another server, like IIS to start up our\napplication, so we don't need any special Visual Studio integration either.\n\nThus we can go ahead and simply create a new F# Console application, which, by\ndefault contains a Program.fs  file with the following content.\n\n[<EntryPoint>]\nlet main argv =\n    printfn \"%A\" argv\n    0 // return an integer exit code\n\n\nWe have to add the Suave  nuget package to our project (by typing \nInstall-Package Suave  into the Package Manager Console, or using the GUI).\n\nThen we can implement a basic web application by replacing the default main \nfunction with the following code.\n\nopen Suave\n\n[<EntryPoint>]\nlet main argv = \n    startWebServer defaultConfig (Successful.OK \"Hello from Suave!\")\n    0\n\n\nOr we can implement something more complicated by handling some different http\nmethods and routes.\n\nlet app =\n    choose\n        [ GET >=> choose\n            [ path \"/status\" >=> OK \"Service is UP\"\n              path \"/hello\" >=> OK \"Hello World!\" ]\n          POST >=> OK \"This was a POST\" ]\n\n[<EntryPoint>]\nlet main argv = \n    startWebServer defaultConfig app\n    0 // return an integer exit code\n\n\nThe cool thing about Suave that it's programming model fits the functional\nconstructs snugly, so it feels much more like a first-class citizen in the F#\nworld than ASP.NET.\nFunctions and the composability features of F# really shine when setting up our\nroutes and the various endpoints of our web application, and a complicated app\ncan be described with surprisingly little, terse code.\n\nYou can read much more about Suave's capabilities on the official site\n[https://suave.io/].\n\nWithout VS: embrace the open tools\nSince we don't need intricate Visual Studio integration to start up our web\nserver, it's also an option to develop completely without VS, which comes\nespecially handy if we want to work on a Mac with Mono (which is also supported\nby F# and Suave).\n\nIn order to do this we can utilize two open source tools.\n\n * Paket [https://fsprojects.github.io/Paket/]: A dependency manager for .NET\n   and Mono, which is easy to use from the command line, and supports using\n   Nuget too as the source of our dependencies.\n * Fake [http://fsharp.github.io/FAKE/]: \"Make for F\", a scriptable,\n   CLI-friendly build automation tool for F#.\n\nWith these tools it's relatively easy to set up a project with a couple of\nscripts, which is completely usable from the terminal without Visual Studio.\n\nYou can find a minimal example of this setup in my jumpstart-suave\n[https://github.com/markvincze/jumpstart-suave]  repository. It contains the\nfollowing elements.\n\n * build.cmd: The main startup script, it does the following steps. 1. If it's\n       not present yet, it installs paket.\n    2. If we\n       don't have a paket.lock  file (we don't have the actual package versions\n       fixed, and haven't discovered the transitive dependencies), installs all\n       the packages.\n    3. Restores\n       the actual packages.\n    4. Starts up\n       our main script by calling FAKE.\n   \n   \n * build.fsx: Defines the \"run\"  default target, which simply starts up the\n   Suave web application (the same way we did in our VS Console project).\n * app.fsx: The implementation of our main app  function, which in this example\n   returns the same 200 OK for every request.\n * paket.dependencies: Specifies the external packages we want to install with\n   Paket.\n * paket.lock: The fixed down versions of all our direct and transitive\n   dependencies. (This file is normally also commited to source control.)\n\n(I based this solution on a more complicated repo\n[https://github.com/tpetricek/suave-xplat-gettingstarted]  set up by Tomas\nPetricek, which also includes automatic reloading in case of a file change, and\ndeployment to Azure and Heroku.)\n\nThis is a completely self-contained example, simply executing build.cmd  will\ndownload all the dependencies, build the project and start up our webserver,\nstraight from the command line.\n\nConclusion\nWith these examples we can see that Suave is a cool, lightweight and fun to use\nweb framework, which fits really nicely into the functional F# world. And\nbecause the web server is also part of the library, and we have some CLI tools\nto install our dependencies and build the project, we can even do the whole\ndevelopment flow without Visual Studio.\n\nWhether to use Suave or ASP.NET is not a trivial question though. If we start a\ngreenfield development project, I'd probably give Suave a go, just because it is\nmore suitable for functional programming, but if we are migrating an existing\nASP.NET application to F#, or we depend on existing libraries which are\nASP.NET-specific, we might be better off sticking to Microsoft's framework.\n\nIn the next post of this series [/series-jumpstart-f-web-development]  we'll\nmove from the classic .NET Framework to .NET Core, and take a look at how we can\nuse F# with ASP.NET there.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "An introduction to get started with web development in F#, using Suave.IO on the classic .NET Framework.",
        "author_id": "1",
        "created_at": "2017-02-11 15:10:52",
        "created_by": "1",
        "updated_at": "2017-02-11 15:14:31",
        "updated_by": "1",
        "published_at": "2017-02-11 15:12:50",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2975",
        "uuid": "1a54a30c-cf48-4297-8c44-99a2b3ce5dbb",
        "title": "Jumpstart F# web development: F# with ASP.NET on classic .NET",
        "slug": "jumpstart-f-web-development-f-with-asp-net-on-classic-net",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"The first part of my [series](/series-jumpstart-f-web-development) about jumstarting F# web development takes a look at probably the most straightforward-looking approach: write F# code using ASP.NET on the classic (full) .NET Framework. (If you're not familiar with the distinction between the full .NET and .NET Core, you can [read about it here](https://docs.microsoft.com/en-us/dotnet/articles/standard/choosing-core-framework-server).)\\n\\nTraditionally, the ASP.NET development workflow has been a very streamlined and convenient experience. If we're using C# and develop web applications with ASP.NET, we're used to the fact that evereything is straightforward and nicely integrated into Visual Studio, and most of the features also have GUI support.\\n\\nIf we choose F# as our programming language, we'll have to jump over some extra hoops to get Visual Studio and ASP.NET obey our bidding, and have to be prepared that some IDE features might not work properly.\\n\\n# Project type\\n\\nThe first initial problem is that there is no built-in project type in Visual Studio for ASP.NET and F#.  \\nOn the other hand, we need Visual Studio, and we need it to know about ASP.NET to get the support we need for ASP.NET development, such as publishing and integration with IIS (Express).\\n\\nAlthough VS doesn't have a built-in template for this purpose, it is possible to massage a csproj and change some GUIDs around until Visual Studio will handle it as an ASP.NET web application.  \\nMark Seeman [has described](http://blog.ploeh.dk/2013/08/23/how-to-create-a-pure-f-aspnet-web-api-project/) this process in detail.\\n\\nLuckily we don't have to do this manually any more, because actual project templates have been added to the VS marketplace.\\n\\nWe have to install the following two templates.\\n\\n - [F# Web Item Templates](https://marketplace.visualstudio.com/items?itemName=DanielMohl.FWebItemTemplates)\\n - [F# MVC 5](https://marketplace.visualstudio.com/items?itemName=DanielMohl.FMVC5)\\n\\nNow we can go ahead and create our project, we can select the project type \\\"F# ASP.NET MVC 5 and Web API 2\\\".\\n\\n![F# and ASP.NET project type](/content/images/2017/02/fsharpaspnetproject-1.png)\\n\\nThen we can select the kind of ASP.NET project we'd like to have. If we want to develop an API, and not an HTML web site, we can select \\\"Web Api 2\\\".\\n\\n![ASP.NET application types](/content/images/2017/02/fsharpaspnettemplatesubtypes-1.png)\\n\\nThe template adds all the necessary files to our project.\\n\\n![Generated project structure](/content/images/2017/02/templateproject-1.png)\\n\\nIt also adds an `Index.html` welcome page, and a `Content`, `Script` and `Fonts` folder.\\nIf you don't need this, feel free to delete them, the Api itself will still work.\\n\\n# Basic ASP.NET elements\\n\\nIn order to get a classic ASP.NET api up and running, at minimum we'll need the `Global` type and a controller.\\n\\nThis is what a minimal `Global.asax.fs` looks like.\\n\\n```fsharp\\nnamespace FSharpWeb\\n \\nopen System\\nopen System.Web.Http\\n\\ntype HttpRouteDefaults = { Controller : string; Id : obj }\\n \\ntype Global() =\\n    inherit System.Web.HttpApplication()\\n    member this.Application_Start (sender : obj) (e : EventArgs) =\\n        GlobalConfiguration.Configuration.Routes.MapHttpRoute(\\n            \\\"DefaultAPI\\\",\\n            \\\"{controller}/{id}\\\",\\n            { Controller = \\\"Home\\\"; Id = RouteParameter.Optional }) |> ignore\\n```\\n\\nAnd implementing a simple controller in F# can be done like this.\\n\\n```fsharp\\nnamespace FSharpWeb\\nopen System\\nopen System.Web.Http\\n\\ntype HomeController() =\\n    inherit ApiController()\\n    member this.Get() =\\n        this.Ok \\\"Hello from F#!\\\"\\n```\\n\\n# Fix serialization\\n\\nWe have two main ways to implement our data models on the Web Api boundary: we can either implement them as .NET classes, or we can use F# record types. I think the latter is the generally recommended approach, since it's a more idiomatic construct in the F# world, and it requires less code.  \\nYou can find more details about it in [this blog post](http://blog.ploeh.dk/2013/10/15/easy-aspnet-web-api-dtos-with-f-climutable-records/).\\n\\nEither way, to get serialization working properly, it's important to have these two lines in our `Global.asax.fs`.\\n\\n```fsharp\\nGlobalConfiguration.Configuration.Formatters.XmlFormatter.UseXmlSerializer <- true\\nGlobalConfiguration.Configuration.Formatters.JsonFormatter.SerializerSettings.ContractResolver <- Newtonsoft.Json.Serialization.CamelCasePropertyNamesContractResolver()\\n```\\n\\nThe first line switches from the `DataContractSerializer` to `XmlSerializer` for the case when we want to return our responses in XML, and the second line configures a specific contract resolver for Json.NET.  \\nThe reason we need them is to ensure we have correct property names in our responses. Without these configurations, the names would be somewhat garbled, which is caused by the property names that the F# compiler generates for record types in the background.\\n\\n# Add new items to the project\\n\\nEven if we use these project templates, Visual Studio is still somewhat confused about having F# and ASP.NET together, and it doesn't allow you to add any items (for example source files) to the project. (The Add New Item dialog is empty.)\\n\\nThis can be fixed by editing an entry in the registry. (This is supposed to be automatically done by a NuGet package which is present in the template, however, that didn't work for me with Visual Studio 2015. I suspect that it only works for earlier versions of VS.)\\n\\nTo fix this, do the following steps.\\n\\n1. Open regedit\\n2. Go to `HKEY_CURRENT_USER\\\\SOFTWARE\\\\Microsoft\\\\VisualStudio\\\\14.0_Config\\\\Projects\\\\`\\n3. Find `{349C5851-65DF-11DA-9384-00065B846F21}`, the one which has a child called `LanguageTemplates`.\\n4. Add a new String Value to `LanguageTemplates` with the key and value `{F2A71F9B-5D33-465A-A702-920D77279786}`. (It's important to have the curly braces around the Guid!)\\n\\n(Source: [http://bloggemdano.blogspot.nl/2013/11/adding-new-items-to-pure-f-aspnet.html](http://bloggemdano.blogspot.nl/2013/11/adding-new-items-to-pure-f-aspnet.html))\\n\\nAfter restarting Visual Studio, we'll be able to add items to our F# web projects as normal.\\n\\nWith these steps we have a working ASP.NET application written in F#.\\n\\nIn the [next part](/jumpstart-f-web-development-f-with-suave-io-on-classic-net/) of [the series](/series-jumpstart-f-web-development) we'll take a look at how to get started with Suave.IO on classic .NET.\"}]],\"sections\":[[10,0]]}",
        "html": "<p>The first part of my <a href=\"/series-jumpstart-f-web-development\">series</a> about jumstarting F# web development takes a look at probably the most straightforward-looking approach: write F# code using ASP.NET on the classic (full) .NET Framework. (If you're not familiar with the distinction between the full .NET and .NET Core, you can <a href=\"https://docs.microsoft.com/en-us/dotnet/articles/standard/choosing-core-framework-server\">read about it here</a>.)</p>\n<p>Traditionally, the ASP.NET development workflow has been a very streamlined and convenient experience. If we're using C# and develop web applications with ASP.NET, we're used to the fact that evereything is straightforward and nicely integrated into Visual Studio, and most of the features also have GUI support.</p>\n<p>If we choose F# as our programming language, we'll have to jump over some extra hoops to get Visual Studio and ASP.NET obey our bidding, and have to be prepared that some IDE features might not work properly.</p>\n<h1 id=\"projecttype\">Project type</h1>\n<p>The first initial problem is that there is no built-in project type in Visual Studio for ASP.NET and F#.<br>\nOn the other hand, we need Visual Studio, and we need it to know about ASP.NET to get the support we need for ASP.NET development, such as publishing and integration with IIS (Express).</p>\n<p>Although VS doesn't have a built-in template for this purpose, it is possible to massage a csproj and change some GUIDs around until Visual Studio will handle it as an ASP.NET web application.<br>\nMark Seeman <a href=\"http://blog.ploeh.dk/2013/08/23/how-to-create-a-pure-f-aspnet-web-api-project/\">has described</a> this process in detail.</p>\n<p>Luckily we don't have to do this manually any more, because actual project templates have been added to the VS marketplace.</p>\n<p>We have to install the following two templates.</p>\n<ul>\n<li><a href=\"https://marketplace.visualstudio.com/items?itemName=DanielMohl.FWebItemTemplates\">F# Web Item Templates</a></li>\n<li><a href=\"https://marketplace.visualstudio.com/items?itemName=DanielMohl.FMVC5\">F# MVC 5</a></li>\n</ul>\n<p>Now we can go ahead and create our project, we can select the project type &quot;F# ASP.NET MVC 5 and Web API 2&quot;.</p>\n<p><img src=\"/content/images/2017/02/fsharpaspnetproject-1.png\" alt=\"F# and ASP.NET project type\"></p>\n<p>Then we can select the kind of ASP.NET project we'd like to have. If we want to develop an API, and not an HTML web site, we can select &quot;Web Api 2&quot;.</p>\n<p><img src=\"/content/images/2017/02/fsharpaspnettemplatesubtypes-1.png\" alt=\"ASP.NET application types\"></p>\n<p>The template adds all the necessary files to our project.</p>\n<p><img src=\"/content/images/2017/02/templateproject-1.png\" alt=\"Generated project structure\"></p>\n<p>It also adds an <code>Index.html</code> welcome page, and a <code>Content</code>, <code>Script</code> and <code>Fonts</code> folder.<br>\nIf you don't need this, feel free to delete them, the Api itself will still work.</p>\n<h1 id=\"basicaspnetelements\">Basic ASP.NET elements</h1>\n<p>In order to get a classic ASP.NET api up and running, at minimum we'll need the <code>Global</code> type and a controller.</p>\n<p>This is what a minimal <code>Global.asax.fs</code> looks like.</p>\n<pre><code class=\"language-fsharp\">namespace FSharpWeb\n \nopen System\nopen System.Web.Http\n\ntype HttpRouteDefaults = { Controller : string; Id : obj }\n \ntype Global() =\n    inherit System.Web.HttpApplication()\n    member this.Application_Start (sender : obj) (e : EventArgs) =\n        GlobalConfiguration.Configuration.Routes.MapHttpRoute(\n            &quot;DefaultAPI&quot;,\n            &quot;{controller}/{id}&quot;,\n            { Controller = &quot;Home&quot;; Id = RouteParameter.Optional }) |&gt; ignore\n</code></pre>\n<p>And implementing a simple controller in F# can be done like this.</p>\n<pre><code class=\"language-fsharp\">namespace FSharpWeb\nopen System\nopen System.Web.Http\n\ntype HomeController() =\n    inherit ApiController()\n    member this.Get() =\n        this.Ok &quot;Hello from F#!&quot;\n</code></pre>\n<h1 id=\"fixserialization\">Fix serialization</h1>\n<p>We have two main ways to implement our data models on the Web Api boundary: we can either implement them as .NET classes, or we can use F# record types. I think the latter is the generally recommended approach, since it's a more idiomatic construct in the F# world, and it requires less code.<br>\nYou can find more details about it in <a href=\"http://blog.ploeh.dk/2013/10/15/easy-aspnet-web-api-dtos-with-f-climutable-records/\">this blog post</a>.</p>\n<p>Either way, to get serialization working properly, it's important to have these two lines in our <code>Global.asax.fs</code>.</p>\n<pre><code class=\"language-fsharp\">GlobalConfiguration.Configuration.Formatters.XmlFormatter.UseXmlSerializer &lt;- true\nGlobalConfiguration.Configuration.Formatters.JsonFormatter.SerializerSettings.ContractResolver &lt;- Newtonsoft.Json.Serialization.CamelCasePropertyNamesContractResolver()\n</code></pre>\n<p>The first line switches from the <code>DataContractSerializer</code> to <code>XmlSerializer</code> for the case when we want to return our responses in XML, and the second line configures a specific contract resolver for Json.NET.<br>\nThe reason we need them is to ensure we have correct property names in our responses. Without these configurations, the names would be somewhat garbled, which is caused by the property names that the F# compiler generates for record types in the background.</p>\n<h1 id=\"addnewitemstotheproject\">Add new items to the project</h1>\n<p>Even if we use these project templates, Visual Studio is still somewhat confused about having F# and ASP.NET together, and it doesn't allow you to add any items (for example source files) to the project. (The Add New Item dialog is empty.)</p>\n<p>This can be fixed by editing an entry in the registry. (This is supposed to be automatically done by a NuGet package which is present in the template, however, that didn't work for me with Visual Studio 2015. I suspect that it only works for earlier versions of VS.)</p>\n<p>To fix this, do the following steps.</p>\n<ol>\n<li>Open regedit</li>\n<li>Go to <code>HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\VisualStudio\\14.0_Config\\Projects\\</code></li>\n<li>Find <code>{349C5851-65DF-11DA-9384-00065B846F21}</code>, the one which has a child called <code>LanguageTemplates</code>.</li>\n<li>Add a new String Value to <code>LanguageTemplates</code> with the key and value <code>{F2A71F9B-5D33-465A-A702-920D77279786}</code>. (It's important to have the curly braces around the Guid!)</li>\n</ol>\n<p>(Source: <a href=\"http://bloggemdano.blogspot.nl/2013/11/adding-new-items-to-pure-f-aspnet.html\">http://bloggemdano.blogspot.nl/2013/11/adding-new-items-to-pure-f-aspnet.html</a>)</p>\n<p>After restarting Visual Studio, we'll be able to add items to our F# web projects as normal.</p>\n<p>With these steps we have a working ASP.NET application written in F#.</p>\n<p>In the <a href=\"/jumpstart-f-web-development-f-with-suave-io-on-classic-net/\">next part</a> of <a href=\"/series-jumpstart-f-web-development\">the series</a> we'll take a look at how to get started with Suave.IO on classic .NET.</p>\n",
        "comment_id": "30",
        "plaintext": "The first part of my series [/series-jumpstart-f-web-development]  about\njumstarting F# web development takes a look at probably the most\nstraightforward-looking approach: write F# code using ASP.NET on the classic\n(full) .NET Framework. (If you're not familiar with the distinction between the\nfull .NET and .NET Core, you can read about it here\n[https://docs.microsoft.com/en-us/dotnet/articles/standard/choosing-core-framework-server]\n.)\n\nTraditionally, the ASP.NET development workflow has been a very streamlined and\nconvenient experience. If we're using C# and develop web applications with\nASP.NET, we're used to the fact that evereything is straightforward and nicely\nintegrated into Visual Studio, and most of the features also have GUI support.\n\nIf we choose F# as our programming language, we'll have to jump over some extra\nhoops to get Visual Studio and ASP.NET obey our bidding, and have to be prepared\nthat some IDE features might not work properly.\n\nProject type\nThe first initial problem is that there is no built-in project type in Visual\nStudio for ASP.NET and F#.\nOn the other hand, we need Visual Studio, and we need it to know about ASP.NET\nto get the support we need for ASP.NET development, such as publishing and\nintegration with IIS (Express).\n\nAlthough VS doesn't have a built-in template for this purpose, it is possible to\nmassage a csproj and change some GUIDs around until Visual Studio will handle it\nas an ASP.NET web application.\nMark Seeman has described\n[http://blog.ploeh.dk/2013/08/23/how-to-create-a-pure-f-aspnet-web-api-project/] \n this process in detail.\n\nLuckily we don't have to do this manually any more, because actual project\ntemplates have been added to the VS marketplace.\n\nWe have to install the following two templates.\n\n * F# Web Item Templates\n   [https://marketplace.visualstudio.com/items?itemName=DanielMohl.FWebItemTemplates]\n * F# MVC 5\n   [https://marketplace.visualstudio.com/items?itemName=DanielMohl.FMVC5]\n\nNow we can go ahead and create our project, we can select the project type \"F#\nASP.NET MVC 5 and Web API 2\".\n\n\n\nThen we can select the kind of ASP.NET project we'd like to have. If we want to\ndevelop an API, and not an HTML web site, we can select \"Web Api 2\".\n\n\n\nThe template adds all the necessary files to our project.\n\n\n\nIt also adds an Index.html  welcome page, and a Content, Script  and Fonts \nfolder.\nIf you don't need this, feel free to delete them, the Api itself will still\nwork.\n\nBasic ASP.NET elements\nIn order to get a classic ASP.NET api up and running, at minimum we'll need the \nGlobal  type and a controller.\n\nThis is what a minimal Global.asax.fs  looks like.\n\nnamespace FSharpWeb\n \nopen System\nopen System.Web.Http\n\ntype HttpRouteDefaults = { Controller : string; Id : obj }\n \ntype Global() =\n    inherit System.Web.HttpApplication()\n    member this.Application_Start (sender : obj) (e : EventArgs) =\n        GlobalConfiguration.Configuration.Routes.MapHttpRoute(\n            \"DefaultAPI\",\n            \"{controller}/{id}\",\n            { Controller = \"Home\"; Id = RouteParameter.Optional }) |> ignore\n\n\nAnd implementing a simple controller in F# can be done like this.\n\nnamespace FSharpWeb\nopen System\nopen System.Web.Http\n\ntype HomeController() =\n    inherit ApiController()\n    member this.Get() =\n        this.Ok \"Hello from F#!\"\n\n\nFix serialization\nWe have two main ways to implement our data models on the Web Api boundary: we\ncan either implement them as .NET classes, or we can use F# record types. I\nthink the latter is the generally recommended approach, since it's a more\nidiomatic construct in the F# world, and it requires less code.\nYou can find more details about it in this blog post\n[http://blog.ploeh.dk/2013/10/15/easy-aspnet-web-api-dtos-with-f-climutable-records/]\n.\n\nEither way, to get serialization working properly, it's important to have these\ntwo lines in our Global.asax.fs.\n\nGlobalConfiguration.Configuration.Formatters.XmlFormatter.UseXmlSerializer <- true\nGlobalConfiguration.Configuration.Formatters.JsonFormatter.SerializerSettings.ContractResolver <- Newtonsoft.Json.Serialization.CamelCasePropertyNamesContractResolver()\n\n\nThe first line switches from the DataContractSerializer  to XmlSerializer  for\nthe case when we want to return our responses in XML, and the second line\nconfigures a specific contract resolver for Json.NET.\nThe reason we need them is to ensure we have correct property names in our\nresponses. Without these configurations, the names would be somewhat garbled,\nwhich is caused by the property names that the F# compiler generates for record\ntypes in the background.\n\nAdd new items to the project\nEven if we use these project templates, Visual Studio is still somewhat confused\nabout having F# and ASP.NET together, and it doesn't allow you to add any items\n(for example source files) to the project. (The Add New Item dialog is empty.)\n\nThis can be fixed by editing an entry in the registry. (This is supposed to be\nautomatically done by a NuGet package which is present in the template, however,\nthat didn't work for me with Visual Studio 2015. I suspect that it only works\nfor earlier versions of VS.)\n\nTo fix this, do the following steps.\n\n 1. Open regedit\n 2. Go to \n    HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\VisualStudio\\14.0_Config\\Projects\\\n 3. Find {349C5851-65DF-11DA-9384-00065B846F21}, the one which has a child\n    called LanguageTemplates.\n 4. Add a new String Value to LanguageTemplates  with the key and value \n    {F2A71F9B-5D33-465A-A702-920D77279786}. (It's important to have the curly\n    braces around the Guid!)\n\n(Source: \nhttp://bloggemdano.blogspot.nl/2013/11/adding-new-items-to-pure-f-aspnet.html)\n\nAfter restarting Visual Studio, we'll be able to add items to our F# web\nprojects as normal.\n\nWith these steps we have a working ASP.NET application written in F#.\n\nIn the next part [/jumpstart-f-web-development-f-with-suave-io-on-classic-net/] \nof the series [/series-jumpstart-f-web-development]  we'll take a look at how to\nget started with Suave.IO on classic .NET.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "An introduction to get started with web development in F#, using ASP.NET on the classic .NET Framework.",
        "author_id": "1",
        "created_at": "2017-02-05 17:25:59",
        "created_by": "1",
        "updated_at": "2017-02-11 15:13:56",
        "updated_by": "1",
        "published_at": "2017-02-05 17:38:02",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2976",
        "uuid": "019666eb-c466-4807-a896-515020dfb4a7",
        "title": "A data model exercise in two languages, part 2: F#",
        "slug": "a-data-model-exercise-in-two-languages-part-2-f",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"# Introduction\\n\\nIn [the previous post](/a-data-model-exercise-in-two-languages-part-1-c) I took a look at a data modelling exercise in C#, I designed a data model to represent a card in the [standard 52-card deck](https://en.wikipedia.org/wiki/Standard_52-card_deck).\\n\\nWe saw some of the problems we face when designing data models in an object oriented language, particularly the lack of ability to express that a certain object can have a value of multiple different types, but it can have a value of only one of those types at any one time (a card is *either* a value card, a face card, or a joker).  \\nWe can get around these limitations by implementing custom validation logic that forces us to create only valid instances of a given class, but it can leave us wondering why cannot the type system help us express our intention more clearly.\\n\\nIn this post we'll see how we can desing the same data model in F#, and how some constructs of functional programming can mitigate some of these limitations.\\n\\n# The task\\n\\nWe will solve the same assignment: design a data model to represent a card in the standard 52-card deck.  \\nYou can find a detailed description of the exercise in the [previous post]().\\n\\n# Data types in F&#35;\\n\\nSince I expect most readers to be less experienced in F# than C#, I'll give a brief introduction to the basic data types of F#, to have a baseline for any further discussion.\\n\\n## Record types\\n\\nRecords are the basic data types we can use to model a complex entity with multiple fields. We can think about them as immutable structs, which only have public fields, and don't have behavior. (Technically we can define member methods on records, but it's not an idiomatic thing to do in F#.)\\n\\n```fsharp\\ntype Customer = {\\n    Id : int\\n    Name : string\\n    Email : string\\n}\\n```\\n\\nWe can create an instance of a record with the following syntax.\\n\\n```fsharp\\nlet cust = {\\n    Id = 5,\\n    Name = \\\"Jane Smith\\\"\\n    Email = \\\"jsmith@acme.com\\\"\\n}\\n```\\n\\nTwo interesting things to note:\\n\\n - We don't have to specify the type. If it is unambiguous, the compiler will infer the proper type for our value.\\n - It is mandatory to specify every field of the record. If we skip one, we receive a build error. This is a great thing to ensure that we update every place depending on our type when we add a new field, and to keep a strong coherence among the fields of any record type.\\n \\nMore info in [the docs](https://docs.microsoft.com/en-us/dotnet/articles/fsharp/language-reference/records).\\n\\n# Tuple\\n\\nSimilarly to other functional programming languages, F# provides convenient syntax and support for using tuples. We can create a tuple of multiple values simply by listing them between parens, separating them with commas.\\n\\n```fsharp\\nlet point = (1.5, 4.3)\\nlet personWithAge = (\\\"Jane Smith\\\", 25)\\n```\\n\\nF# also supports pattern matching with tuples (which is again a typical feature of FP languages, and to some extent it also arrived in JavaScript in the form of [destructuring](https://developer.mozilla.org/nl/docs/Web/JavaScript/Reference/Operatoren/Destructuring_assignment)):\\n\\n```fsharp\\nlet x, y = point\\n```\\n\\nWe can also specify tuples in type declarations, where if we want to define a tuple of 3 elements, we can use the following syntax.\\n\\n```fsharp\\ntype MyTuple = string * int * Customer\\n```\\n\\n# Discriminated union\\n\\nThis is another basic F# data type which will be particularly interesting in our exercise. Its name is a bit scary at first, but the concept is very simple.\\n\\nAt first sight a discriminated union looks like an `enum` in C#, for example we can define a logging level with it.\\n\\n```fsharp\\ntype LoggingLevel =\\n| Debug\\n| Info\\n| Error\\n```\\n\\nHowever, a discriminated union is more than this. Every case we define (which is called a *named case*) can also have a value associated to it, and the different cases can have different values.  \\nAn idiomatic example from the official documentation.\\n\\n```fsharp\\ntype Shape =\\n| Rectangle of width : float * length : float\\n| Circle of radius : float\\n```\\n\\n(Note that we can also see here the syntax for specifying a tuple type.)  \\nThen we can construct a value of the different cases by specifying the case name.\\n\\n```fsharp\\nlet rect = Rectangle (length = 2.3, width = 10.0)\\nlet circ = Circle (2.0)\\n```\\n\\nAnd when we actually want to use a value with a discriminated union type, we can use pattern matching to handle the different cases.\\n\\n```fsharp\\nlet calculateArea shape =\\n    match shape with\\n    | Rectangle (w, l) -> w * l\\n    | Circle r -> r * r * 3.14\\n```\\n\\nWe can think of discriminated unions as a way to achieve static, compile-time polymorphism. (Static, because all the cases of the union are defined in one place, and there is no way to extend the union with other cases from the outside, as opposed to OO inheritance. We could try to model this behavior in OO by defining an abstract base class, and creating some sealed derived types. But even this could not prevent the users of our library to introduce additional derived types in the same hierarchy.)\\n\\nOne of the nice benefits of having *all* the cases of the union defined in one place is that the compiler can check if we always cover all the cases everywhere we are doing pattern matching on a certain union type, which is extremely handy when we introduce a new case, so the compiler can remind us to update all the places in our codebase which are using the union.  \\nFor example if we introduce a new `Square` case for our `Shape` union, we'll get this nice warning message.\\n\\n![The F# compiler displaying a warning when we don't handle every case of a discriminated union in pattern matching.](/content/images/2017/05/fsharp-union-match-warning.png)\\n\\n# Implementation\\n\\nIn order to implement the card data model in F#, we can start the same way we did in C#, by defining the types representing the various suits and faces. In C# we used an enum for this purpose, in F# we can use discriminated unions.\\n\\n```fsharp\\ntype Suit =\\n| Hearts\\n| Spades\\n| Clubs\\n| Diamonds\\n\\ntype Face =\\n| Jack\\n| Queen\\n| King\\n| Ace\\n```\\n\\nThe next step is to create the actual data type which will represent a card from the deck. Remember: what we want to express is that the card is *either* one of these options.\\n\\n - A value card with a suit and a number value.\\n - A face card with a suit and a face.\\n - A joker.\\n\\nThis is exactly the kind of concept we can clearly express using a discriminated union.\\n\\n```fsharp\\ntype Card =\\n| FaceCard of Suit * Face\\n| ValueCard of Suit * int\\n| Joker\\n```\\n\\nWe can create some actual values of this type with the following sytax.\\n\\n```fsharp\\nlet jackOfHearts = FaceCard (Hearts, Jack)\\nlet threeOfClubs = ValueCard (Clubs, 3)\\nlet joker = Joker\\n```\\n\\nSome things to notice:\\n\\n - There is no way to create a value which is \\\"both\\\" a face and a value card (or a joker). Every value falls exactly into one of the cases.\\n - The definition of the discriminated union forces us to provide the necessary input when we create a value, namely, the suit and face in case of a face card, the suit and the value in case of a value card, and nothing in case of a joker. (And there is no way to provide or set \\\"more\\\" data then what the specific case of the union needs.)\\n\\nSo with the 4 lines of code defining the `Card` union we achieved the same goal as what we did in C# with the following implementation.\\n\\n```csharp\\nclass Card\\n{\\n    public Suit Suit { get; }\\n\\n    public Face? Face { get; }\\n\\n    public int? Value { get; }\\n\\n    public bool IsJoker { get; }\\n\\n    private Card(Suit suit, Face? face, int? value, bool isJoker)\\n    {\\n        Suit = suit;\\n        Face = face;\\n        Value = value;\\n        IsJoker = isJoker;\\n    }\\n\\n    public static Card CreateFace(Suit suit, Face face)\\n    {\\n        return new Card(suit, face, null, false);\\n    }\\n\\n    public static Card CreateValue(Suit suit, int value)\\n    {\\n        return new Card(suit, null, value, false);\\n    }\\n\\n    public static Card CreateJoker()\\n    {\\n        return new Card(default(Suit), null, null, true);\\n    }\\n}\\n```\\n\\nThis illustrates how powerful and concise construct a disciminated union can be, and since I got familiar with it, I miss it every day when doing OO development in C#.\\n\\nFinally let's take a look at what it looks like if we want to actually process a value of this type, for example if we want to implement the [score calculation](https://en.wikipedia.org/wiki/Rummy#Scoring) of the card game Rummy. Here we can see the pattern matching syntax again.\\n\\n```fsharp\\nlet calculateValue card =\\n    match card with\\n    | Joker -> 0\\n    | FaceCard (Spades, Queen) -> 40\\n    | FaceCard (_, Ace) -> 15\\n    | FaceCard (_, _) -> 10\\n    | ValueCard (_, 10) -> 10\\n    | _ -> 5\\n```\\n\\nAnd this is how we can call this function with a value we created.\\n\\n```fsharp\\nlet jackOfHearts = FaceCard (Hearts, Jack)\\n\\n// The value of rummyScore will be 10.\\nlet rummyScore = calculateValue jackOfHearts\\n```\\n\\nWith this exercise I wanted to illustrate how the type system and the language features of F# can help us express some constructs which are inconvenient to model in object oriented languages. Particularly the discriminated union is a data type that I really recommend for every developer to get familiar with (of which the only downside is that we'll be constantly wishing we had this feature in every language :)).\\n\\nAt first sight this example might seem a bit specific, but these scenarios pop up in every day work much more than we'd expect. Let's look at a couple example.\\n\\nThe result of an operation that might not found the result, or return an error.\\n\\n```fsharp\\ntype OperationResult =\\n| Success of data : Data\\n| NotFound\\n| Error of errorMessage : string\\n```\\n\\nAn HTTP request, which is either a GET (having only a URL) or a POST (having a URL and a body).\\n\\n```fsharp\\ntype HttpRequest =\\n| Get of url : string\\n| Post of url : string * body : byte array\\n```\\n\\nA data type representing credentials, either with username and password, or a certificate.\\n\\n```fsharp\\ntype Credentials = \\n| UserPass of user : string * password : string\\n| Certificate of certFilePath : string\\n| None\\n```\\n\\nLearning about these functional programming features greatly changed how I think about data models and interface design in my everyday work (programmign in OO languages).  \\nWith this post I wanted to give a brief introduction to these constructs. I hope these examples will provide motivation to get familiar with F# and functional programming.\"}]],\"sections\":[[10,0]]}",
        "html": "<h1 id=\"introduction\">Introduction</h1>\n<p>In <a href=\"/a-data-model-exercise-in-two-languages-part-1-c\">the previous post</a> I took a look at a data modelling exercise in C#, I designed a data model to represent a card in the <a href=\"https://en.wikipedia.org/wiki/Standard_52-card_deck\">standard 52-card deck</a>.</p>\n<p>We saw some of the problems we face when designing data models in an object oriented language, particularly the lack of ability to express that a certain object can have a value of multiple different types, but it can have a value of only one of those types at any one time (a card is <em>either</em> a value card, a face card, or a joker).<br>\nWe can get around these limitations by implementing custom validation logic that forces us to create only valid instances of a given class, but it can leave us wondering why cannot the type system help us express our intention more clearly.</p>\n<p>In this post we'll see how we can desing the same data model in F#, and how some constructs of functional programming can mitigate some of these limitations.</p>\n<h1 id=\"thetask\">The task</h1>\n<p>We will solve the same assignment: design a data model to represent a card in the standard 52-card deck.<br>\nYou can find a detailed description of the exercise in the <a href=\"\">previous post</a>.</p>\n<h1 id=\"datatypesinf\">Data types in F#</h1>\n<p>Since I expect most readers to be less experienced in F# than C#, I'll give a brief introduction to the basic data types of F#, to have a baseline for any further discussion.</p>\n<h2 id=\"recordtypes\">Record types</h2>\n<p>Records are the basic data types we can use to model a complex entity with multiple fields. We can think about them as immutable structs, which only have public fields, and don't have behavior. (Technically we can define member methods on records, but it's not an idiomatic thing to do in F#.)</p>\n<pre><code class=\"language-fsharp\">type Customer = {\n    Id : int\n    Name : string\n    Email : string\n}\n</code></pre>\n<p>We can create an instance of a record with the following syntax.</p>\n<pre><code class=\"language-fsharp\">let cust = {\n    Id = 5,\n    Name = &quot;Jane Smith&quot;\n    Email = &quot;jsmith@acme.com&quot;\n}\n</code></pre>\n<p>Two interesting things to note:</p>\n<ul>\n<li>We don't have to specify the type. If it is unambiguous, the compiler will infer the proper type for our value.</li>\n<li>It is mandatory to specify every field of the record. If we skip one, we receive a build error. This is a great thing to ensure that we update every place depending on our type when we add a new field, and to keep a strong coherence among the fields of any record type.</li>\n</ul>\n<p>More info in <a href=\"https://docs.microsoft.com/en-us/dotnet/articles/fsharp/language-reference/records\">the docs</a>.</p>\n<h1 id=\"tuple\">Tuple</h1>\n<p>Similarly to other functional programming languages, F# provides convenient syntax and support for using tuples. We can create a tuple of multiple values simply by listing them between parens, separating them with commas.</p>\n<pre><code class=\"language-fsharp\">let point = (1.5, 4.3)\nlet personWithAge = (&quot;Jane Smith&quot;, 25)\n</code></pre>\n<p>F# also supports pattern matching with tuples (which is again a typical feature of FP languages, and to some extent it also arrived in JavaScript in the form of <a href=\"https://developer.mozilla.org/nl/docs/Web/JavaScript/Reference/Operatoren/Destructuring_assignment\">destructuring</a>):</p>\n<pre><code class=\"language-fsharp\">let x, y = point\n</code></pre>\n<p>We can also specify tuples in type declarations, where if we want to define a tuple of 3 elements, we can use the following syntax.</p>\n<pre><code class=\"language-fsharp\">type MyTuple = string * int * Customer\n</code></pre>\n<h1 id=\"discriminatedunion\">Discriminated union</h1>\n<p>This is another basic F# data type which will be particularly interesting in our exercise. Its name is a bit scary at first, but the concept is very simple.</p>\n<p>At first sight a discriminated union looks like an <code>enum</code> in C#, for example we can define a logging level with it.</p>\n<pre><code class=\"language-fsharp\">type LoggingLevel =\n| Debug\n| Info\n| Error\n</code></pre>\n<p>However, a discriminated union is more than this. Every case we define (which is called a <em>named case</em>) can also have a value associated to it, and the different cases can have different values.<br>\nAn idiomatic example from the official documentation.</p>\n<pre><code class=\"language-fsharp\">type Shape =\n| Rectangle of width : float * length : float\n| Circle of radius : float\n</code></pre>\n<p>(Note that we can also see here the syntax for specifying a tuple type.)<br>\nThen we can construct a value of the different cases by specifying the case name.</p>\n<pre><code class=\"language-fsharp\">let rect = Rectangle (length = 2.3, width = 10.0)\nlet circ = Circle (2.0)\n</code></pre>\n<p>And when we actually want to use a value with a discriminated union type, we can use pattern matching to handle the different cases.</p>\n<pre><code class=\"language-fsharp\">let calculateArea shape =\n    match shape with\n    | Rectangle (w, l) -&gt; w * l\n    | Circle r -&gt; r * r * 3.14\n</code></pre>\n<p>We can think of discriminated unions as a way to achieve static, compile-time polymorphism. (Static, because all the cases of the union are defined in one place, and there is no way to extend the union with other cases from the outside, as opposed to OO inheritance. We could try to model this behavior in OO by defining an abstract base class, and creating some sealed derived types. But even this could not prevent the users of our library to introduce additional derived types in the same hierarchy.)</p>\n<p>One of the nice benefits of having <em>all</em> the cases of the union defined in one place is that the compiler can check if we always cover all the cases everywhere we are doing pattern matching on a certain union type, which is extremely handy when we introduce a new case, so the compiler can remind us to update all the places in our codebase which are using the union.<br>\nFor example if we introduce a new <code>Square</code> case for our <code>Shape</code> union, we'll get this nice warning message.</p>\n<p><img src=\"/content/images/2017/05/fsharp-union-match-warning.png\" alt=\"The F# compiler displaying a warning when we don't handle every case of a discriminated union in pattern matching.\"></p>\n<h1 id=\"implementation\">Implementation</h1>\n<p>In order to implement the card data model in F#, we can start the same way we did in C#, by defining the types representing the various suits and faces. In C# we used an enum for this purpose, in F# we can use discriminated unions.</p>\n<pre><code class=\"language-fsharp\">type Suit =\n| Hearts\n| Spades\n| Clubs\n| Diamonds\n\ntype Face =\n| Jack\n| Queen\n| King\n| Ace\n</code></pre>\n<p>The next step is to create the actual data type which will represent a card from the deck. Remember: what we want to express is that the card is <em>either</em> one of these options.</p>\n<ul>\n<li>A value card with a suit and a number value.</li>\n<li>A face card with a suit and a face.</li>\n<li>A joker.</li>\n</ul>\n<p>This is exactly the kind of concept we can clearly express using a discriminated union.</p>\n<pre><code class=\"language-fsharp\">type Card =\n| FaceCard of Suit * Face\n| ValueCard of Suit * int\n| Joker\n</code></pre>\n<p>We can create some actual values of this type with the following sytax.</p>\n<pre><code class=\"language-fsharp\">let jackOfHearts = FaceCard (Hearts, Jack)\nlet threeOfClubs = ValueCard (Clubs, 3)\nlet joker = Joker\n</code></pre>\n<p>Some things to notice:</p>\n<ul>\n<li>There is no way to create a value which is &quot;both&quot; a face and a value card (or a joker). Every value falls exactly into one of the cases.</li>\n<li>The definition of the discriminated union forces us to provide the necessary input when we create a value, namely, the suit and face in case of a face card, the suit and the value in case of a value card, and nothing in case of a joker. (And there is no way to provide or set &quot;more&quot; data then what the specific case of the union needs.)</li>\n</ul>\n<p>So with the 4 lines of code defining the <code>Card</code> union we achieved the same goal as what we did in C# with the following implementation.</p>\n<pre><code class=\"language-csharp\">class Card\n{\n    public Suit Suit { get; }\n\n    public Face? Face { get; }\n\n    public int? Value { get; }\n\n    public bool IsJoker { get; }\n\n    private Card(Suit suit, Face? face, int? value, bool isJoker)\n    {\n        Suit = suit;\n        Face = face;\n        Value = value;\n        IsJoker = isJoker;\n    }\n\n    public static Card CreateFace(Suit suit, Face face)\n    {\n        return new Card(suit, face, null, false);\n    }\n\n    public static Card CreateValue(Suit suit, int value)\n    {\n        return new Card(suit, null, value, false);\n    }\n\n    public static Card CreateJoker()\n    {\n        return new Card(default(Suit), null, null, true);\n    }\n}\n</code></pre>\n<p>This illustrates how powerful and concise construct a disciminated union can be, and since I got familiar with it, I miss it every day when doing OO development in C#.</p>\n<p>Finally let's take a look at what it looks like if we want to actually process a value of this type, for example if we want to implement the <a href=\"https://en.wikipedia.org/wiki/Rummy#Scoring\">score calculation</a> of the card game Rummy. Here we can see the pattern matching syntax again.</p>\n<pre><code class=\"language-fsharp\">let calculateValue card =\n    match card with\n    | Joker -&gt; 0\n    | FaceCard (Spades, Queen) -&gt; 40\n    | FaceCard (_, Ace) -&gt; 15\n    | FaceCard (_, _) -&gt; 10\n    | ValueCard (_, 10) -&gt; 10\n    | _ -&gt; 5\n</code></pre>\n<p>And this is how we can call this function with a value we created.</p>\n<pre><code class=\"language-fsharp\">let jackOfHearts = FaceCard (Hearts, Jack)\n\n// The value of rummyScore will be 10.\nlet rummyScore = calculateValue jackOfHearts\n</code></pre>\n<p>With this exercise I wanted to illustrate how the type system and the language features of F# can help us express some constructs which are inconvenient to model in object oriented languages. Particularly the discriminated union is a data type that I really recommend for every developer to get familiar with (of which the only downside is that we'll be constantly wishing we had this feature in every language :)).</p>\n<p>At first sight this example might seem a bit specific, but these scenarios pop up in every day work much more than we'd expect. Let's look at a couple example.</p>\n<p>The result of an operation that might not found the result, or return an error.</p>\n<pre><code class=\"language-fsharp\">type OperationResult =\n| Success of data : Data\n| NotFound\n| Error of errorMessage : string\n</code></pre>\n<p>An HTTP request, which is either a GET (having only a URL) or a POST (having a URL and a body).</p>\n<pre><code class=\"language-fsharp\">type HttpRequest =\n| Get of url : string\n| Post of url : string * body : byte array\n</code></pre>\n<p>A data type representing credentials, either with username and password, or a certificate.</p>\n<pre><code class=\"language-fsharp\">type Credentials = \n| UserPass of user : string * password : string\n| Certificate of certFilePath : string\n| None\n</code></pre>\n<p>Learning about these functional programming features greatly changed how I think about data models and interface design in my everyday work (programmign in OO languages).<br>\nWith this post I wanted to give a brief introduction to these constructs. I hope these examples will provide motivation to get familiar with F# and functional programming.</p>\n",
        "comment_id": "38",
        "plaintext": "Introduction\nIn the previous post [/a-data-model-exercise-in-two-languages-part-1-c]  I took\na look at a data modelling exercise in C#, I designed a data model to represent\na card in the standard 52-card deck\n[https://en.wikipedia.org/wiki/Standard_52-card_deck].\n\nWe saw some of the problems we face when designing data models in an object\noriented language, particularly the lack of ability to express that a certain\nobject can have a value of multiple different types, but it can have a value of\nonly one of those types at any one time (a card is either  a value card, a face\ncard, or a joker).\nWe can get around these limitations by implementing custom validation logic that\nforces us to create only valid instances of a given class, but it can leave us\nwondering why cannot the type system help us express our intention more clearly.\n\nIn this post we'll see how we can desing the same data model in F#, and how some\nconstructs of functional programming can mitigate some of these limitations.\n\nThe task\nWe will solve the same assignment: design a data model to represent a card in\nthe standard 52-card deck.\nYou can find a detailed description of the exercise in the previous post.\n\nData types in F#\nSince I expect most readers to be less experienced in F# than C#, I'll give a\nbrief introduction to the basic data types of F#, to have a baseline for any\nfurther discussion.\n\nRecord types\nRecords are the basic data types we can use to model a complex entity with\nmultiple fields. We can think about them as immutable structs, which only have\npublic fields, and don't have behavior. (Technically we can define member\nmethods on records, but it's not an idiomatic thing to do in F#.)\n\ntype Customer = {\n    Id : int\n    Name : string\n    Email : string\n}\n\n\nWe can create an instance of a record with the following syntax.\n\nlet cust = {\n    Id = 5,\n    Name = \"Jane Smith\"\n    Email = \"jsmith@acme.com\"\n}\n\n\nTwo interesting things to note:\n\n * We don't have to specify the type. If it is unambiguous, the compiler will\n   infer the proper type for our value.\n * It is mandatory to specify every field of the record. If we skip one, we\n   receive a build error. This is a great thing to ensure that we update every\n   place depending on our type when we add a new field, and to keep a strong\n   coherence among the fields of any record type.\n\nMore info in the docs\n[https://docs.microsoft.com/en-us/dotnet/articles/fsharp/language-reference/records]\n.\n\nTuple\nSimilarly to other functional programming languages, F# provides convenient\nsyntax and support for using tuples. We can create a tuple of multiple values\nsimply by listing them between parens, separating them with commas.\n\nlet point = (1.5, 4.3)\nlet personWithAge = (\"Jane Smith\", 25)\n\n\nF# also supports pattern matching with tuples (which is again a typical feature\nof FP languages, and to some extent it also arrived in JavaScript in the form of\n destructuring\n[https://developer.mozilla.org/nl/docs/Web/JavaScript/Reference/Operatoren/Destructuring_assignment]\n):\n\nlet x, y = point\n\n\nWe can also specify tuples in type declarations, where if we want to define a\ntuple of 3 elements, we can use the following syntax.\n\ntype MyTuple = string * int * Customer\n\n\nDiscriminated union\nThis is another basic F# data type which will be particularly interesting in our\nexercise. Its name is a bit scary at first, but the concept is very simple.\n\nAt first sight a discriminated union looks like an enum  in C#, for example we\ncan define a logging level with it.\n\ntype LoggingLevel =\n| Debug\n| Info\n| Error\n\n\nHowever, a discriminated union is more than this. Every case we define (which is\ncalled a named case) can also have a value associated to it, and the different\ncases can have different values.\nAn idiomatic example from the official documentation.\n\ntype Shape =\n| Rectangle of width : float * length : float\n| Circle of radius : float\n\n\n(Note that we can also see here the syntax for specifying a tuple type.)\nThen we can construct a value of the different cases by specifying the case\nname.\n\nlet rect = Rectangle (length = 2.3, width = 10.0)\nlet circ = Circle (2.0)\n\n\nAnd when we actually want to use a value with a discriminated union type, we can\nuse pattern matching to handle the different cases.\n\nlet calculateArea shape =\n    match shape with\n    | Rectangle (w, l) -> w * l\n    | Circle r -> r * r * 3.14\n\n\nWe can think of discriminated unions as a way to achieve static, compile-time\npolymorphism. (Static, because all the cases of the union are defined in one\nplace, and there is no way to extend the union with other cases from the\noutside, as opposed to OO inheritance. We could try to model this behavior in OO\nby defining an abstract base class, and creating some sealed derived types. But\neven this could not prevent the users of our library to introduce additional\nderived types in the same hierarchy.)\n\nOne of the nice benefits of having all  the cases of the union defined in one\nplace is that the compiler can check if we always cover all the cases everywhere\nwe are doing pattern matching on a certain union type, which is extremely handy\nwhen we introduce a new case, so the compiler can remind us to update all the\nplaces in our codebase which are using the union.\nFor example if we introduce a new Square  case for our Shape  union, we'll get\nthis nice warning message.\n\n\n\nImplementation\nIn order to implement the card data model in F#, we can start the same way we\ndid in C#, by defining the types representing the various suits and faces. In C#\nwe used an enum for this purpose, in F# we can use discriminated unions.\n\ntype Suit =\n| Hearts\n| Spades\n| Clubs\n| Diamonds\n\ntype Face =\n| Jack\n| Queen\n| King\n| Ace\n\n\nThe next step is to create the actual data type which will represent a card from\nthe deck. Remember: what we want to express is that the card is either  one of\nthese options.\n\n * A value card with a suit and a number value.\n * A face card with a suit and a face.\n * A joker.\n\nThis is exactly the kind of concept we can clearly express using a discriminated\nunion.\n\ntype Card =\n| FaceCard of Suit * Face\n| ValueCard of Suit * int\n| Joker\n\n\nWe can create some actual values of this type with the following sytax.\n\nlet jackOfHearts = FaceCard (Hearts, Jack)\nlet threeOfClubs = ValueCard (Clubs, 3)\nlet joker = Joker\n\n\nSome things to notice:\n\n * There is no way to create a value which is \"both\" a face and a value card (or\n   a joker). Every value falls exactly into one of the cases.\n * The definition of the discriminated union forces us to provide the necessary\n   input when we create a value, namely, the suit and face in case of a face\n   card, the suit and the value in case of a value card, and nothing in case of\n   a joker. (And there is no way to provide or set \"more\" data then what the\n   specific case of the union needs.)\n\nSo with the 4 lines of code defining the Card  union we achieved the same goal\nas what we did in C# with the following implementation.\n\nclass Card\n{\n    public Suit Suit { get; }\n\n    public Face? Face { get; }\n\n    public int? Value { get; }\n\n    public bool IsJoker { get; }\n\n    private Card(Suit suit, Face? face, int? value, bool isJoker)\n    {\n        Suit = suit;\n        Face = face;\n        Value = value;\n        IsJoker = isJoker;\n    }\n\n    public static Card CreateFace(Suit suit, Face face)\n    {\n        return new Card(suit, face, null, false);\n    }\n\n    public static Card CreateValue(Suit suit, int value)\n    {\n        return new Card(suit, null, value, false);\n    }\n\n    public static Card CreateJoker()\n    {\n        return new Card(default(Suit), null, null, true);\n    }\n}\n\n\nThis illustrates how powerful and concise construct a disciminated union can be,\nand since I got familiar with it, I miss it every day when doing OO development\nin C#.\n\nFinally let's take a look at what it looks like if we want to actually process a\nvalue of this type, for example if we want to implement the score calculation \nof the card game Rummy. Here we can see the pattern matching syntax again.\n\nlet calculateValue card =\n    match card with\n    | Joker -> 0\n    | FaceCard (Spades, Queen) -> 40\n    | FaceCard (_, Ace) -> 15\n    | FaceCard (_, _) -> 10\n    | ValueCard (_, 10) -> 10\n    | _ -> 5\n\n\nAnd this is how we can call this function with a value we created.\n\nlet jackOfHearts = FaceCard (Hearts, Jack)\n\n// The value of rummyScore will be 10.\nlet rummyScore = calculateValue jackOfHearts\n\n\nWith this exercise I wanted to illustrate how the type system and the language\nfeatures of F# can help us express some constructs which are inconvenient to\nmodel in object oriented languages. Particularly the discriminated union is a\ndata type that I really recommend for every developer to get familiar with (of\nwhich the only downside is that we'll be constantly wishing we had this feature\nin every language :)).\n\nAt first sight this example might seem a bit specific, but these scenarios pop\nup in every day work much more than we'd expect. Let's look at a couple example.\n\nThe result of an operation that might not found the result, or return an error.\n\ntype OperationResult =\n| Success of data : Data\n| NotFound\n| Error of errorMessage : string\n\n\nAn HTTP request, which is either a GET (having only a URL) or a POST (having a\nURL and a body).\n\ntype HttpRequest =\n| Get of url : string\n| Post of url : string * body : byte array\n\n\nA data type representing credentials, either with username and password, or a\ncertificate.\n\ntype Credentials = \n| UserPass of user : string * password : string\n| Certificate of certFilePath : string\n| None\n\n\nLearning about these functional programming features greatly changed how I think\nabout data models and interface design in my everyday work (programmign in OO\nlanguages).\nWith this post I wanted to give a brief introduction to these constructs. I hope\nthese examples will provide motivation to get familiar with F# and functional\nprogramming.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "Second part of an exercise illustrating how F# can provide an elegant solution to some of the challenges we face when designing data models in OO languages.",
        "author_id": "1",
        "created_at": "2017-05-22 19:40:09",
        "created_by": "1",
        "updated_at": "2017-05-23 07:11:11",
        "updated_by": "1",
        "published_at": "2017-05-22 19:56:14",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2977",
        "uuid": "533d363b-ba2b-4391-bc55-fba121c1954c",
        "title": "Playing with the composition of the Kleisli category in C#",
        "slug": "playing-with-the-composition-of-the-kleisli-category-in-c",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"# Introduction\\n\\nRecently I learnt about an interesting concept in category theory called a *Kleisli category*.\\n\\nLet's look at a concrete example I took from [this blog post](https://bartoszmilewski.com/2014/12/23/kleisli-categories/) (in his series about category theory) by Bartosz Milewski.\\nWe would like to extend all of the functions in a library in a way that besides their normal result, they also return an extra string. We'll consider this extra string a sort of log message or \\\"comment\\\", which we'll collect as we call various methods.\\n\\nFor example we might have the following original method.\\n\\n```csharp\\nint Increment(int x)\\n{\\n    return x + 1;\\n}\\n```\\n\\nWe can extend it the following way to also return a message.\\n\\n```csharp\\n(int result, string log) Increment(int x)\\n{\\n    return (x + 1, \\\"Incremented\\\");\\n}\\n```\\n\\n(I wanted to be fancy, so I'm using C# 7 tuple literals, but I could also have used a `Tuple<int, string>` as the return type.)\\nFor the sake of the example I'll implement another method.\\n\\n```csharp\\n(int result, string log) Double(int x)\\n{\\n    return (x * 2, \\\"Doubled\\\");\\n}\\n```\\n\\nIf we extend the return type of a function with something extra like this, we call it an *embellished* function. We could imagine many other useful ways to embellish functions. For example we could return what the execution time of the function was, or return a second value containing any errors happened during the call.\\n\\nIt turns out that if we define a way to compose such embellished functions (for example compose `Increment` and `Double` so that they are applied after each other), and if we can compose the embellished return values associatively (for example the extra log messages can be concatenated), then our input and output data types, and our functions will form a specific *category* of category theory called the *[Kleisli category](https://en.wikipedia.org/wiki/Kleisli_category)*. If you are interested in the details, I recommend the [above mentioned post](https://bartoszmilewski.com/2014/12/23/kleisli-categories/).\\n\\n# Composition\\n\\nIn this post I'm going to focus on the **composition** part, more specifically how such composition can be implemented in C#.\\n\\nWe want to have a methodlet's call it `Compose`which takes two such embellished functions as input, and returns their composition, that is, a method that executes the two functions after each other, returns the second output, and concatenates the extra log messages.\\n*(Note: I'm using the C# terminology pretty loosely here. In C# functions are not first class citizens to the extent they are in some other languages, although we can have various constructs that we can use in a similar way, such as delegates, method groups, anonymous functions or lambda expressions.)*\\n\\nIt was pretty straightforward to implement a specific composition method for the above scenario. The only tricky part, that needed an extra mental step is that in this method we shouldn't actually *call* the functions passed as inputs, but we just have to return a function that will call them when executed.\\n\\n```csharp\\nFunc<int, (int, string)> Compose(Func<int, (int, string)> a, Func<int, (int, string)> b)\\n{\\n    return (int x) =>\\n    {\\n        var (aResult, aLog) = a(x);\\n        var (bResult, bLog) = b(aResult);\\n        return (bResult, aLog + bLog);\\n    };\\n}\\n```\\n\\nWe can try our composition method in a console application.\\n\\n```csharp\\nvar incrementAndDouble = Compose(Increment, Double);\\n\\nvar (result, log) = incrementAndDouble(2);\\n\\nConsole.WriteLine($\\\"Result: {result}, Log: {log}\\\");\\n```\\n\\nAnd in the terminal we'll see that it works:\\n\\n```\\nResult: 6, Log: IncrementedDoubled\\n```\\n\\n## Generic composition\\n\\nThe second thing I wanted to achieve is to improve the `Compose` method by making it generic, so that it can work for all input and output types, not just for integers. This didn't seem to be difficult to achieve, I replaced the concrete types with generic type parameters (except the `string`, with which the return type is embellished).\\n\\n```csharp\\nFunc<A, (C, string)> Compose<A, B, C>(Func<A, (B, string)> a, Func<B, (C, string)> b)\\n{\\n    return (A x) =>\\n    {\\n        var (aResult, aLog) = a(x);\\n        var (bResult, bLog) = b(aResult);\\n        return (bResult, aLog + bLog);\\n    };\\n}\\n```\\n\\nThe configuration of the type parameters nicely correspond to function composition in a mathematical sense, since if we have a function `f :: a -> b` and `g :: b -> c`, then the type of their composition will be `g  f :: a -> c`. Notice that although we use integers both as our input and output, in the generic `Compose` we still have different type arguments for the parameters and the return values, so it also supports functions that have different input and output types.\\n\\nI was really happy with this implementation, the building blocks of C# seemed to fall nicely in place. Unfortunately when I tried to use this generic version of `Compose` with the same call:\\n\\n```csharp\\nvar incrementAndDouble = Compose(Increment, Double);\\n```\\n\\nI received the following build error.\\n\\n> `Error CS0411: The type arguments for method 'Program.Compose<A, B, C>(Func<A, (B, string)>, Func<B, (C, string)>)' cannot be inferred from the usage. Try specifying the type arguments explicitly.`\\n\\nI was surprised by this error message, and couldn't immediately figure out its reason. Since in the two methods `Increment` and `Double` the parameter and return types are explicitly specified, I thought the compiler would be able to *infer* the proper values for the type arguments.\\n\\nI found the reason in the answer to [this SO question](http://stackoverflow.com/questions/7400550/c-sharp-infer-generic-type-based-on-passing-a-delegate): The C# spec states that type inference based on the output type only works if the types are explicitly specified, *or* if we pass in an anonymous function. The C# constructs we're trying to pass in to `Compose` are *method groups*, and the spec does not require the type inference to work for them. (This would not be impossible to do, but it would probably not worth the effort by the compiler, and it could cause more problems, as stated by Eric Lippert in [this comment](http://stackoverflow.com/questions/6229131/why-cant-c-sharp-infer-type-from-this-seemingly-simple-obvious-case#comment7312385_6231921). One of the reasons why this is problematic, is that a method group might have more than one corresponding method underneath because of method overloading, in which case the type inference wouldn't necessarily be unambiguous.)\\n\\nThe way to work around this is to help the compiler, and somehow tell it the actual types of our methods.\\nWe can either explicitly specify the type arguments:\\n\\n```csharp\\nvar incrementAndDouble = Compose<int, int, int>(Increment, Double);\\n```\\n\\nOras the spec suggestspass in the methods as anonymous functions instead of method groups:\\n\\n```csharp\\nvar incrementAndDouble = Compose((int x) => Increment(x), (int x) => Double(x));\\n```\\n\\nI'm not particularly happy about these workarounds, but so far I couldn't find an easier way to do this in C#.\\n\\nI'm not sure if I'll ever use this approach in a real project, but I found this example interesting enough to share, especially regarding the limitations of the C# type inference algorithm.\\n\\n# Composition in F&#35;\\n\\nJust to see what the same thing would look like in a language where functions are first-class citizens (and where we don't have function overloading), here is the implementation of the same two functions and the generic composition in F#.\\n\\n```fsharp\\nlet increment x = (x + 1, \\\"Incremented\\\")\\n\\nlet double x = (x * 2, \\\"Doubled\\\")\\n\\nlet compose a b =\\n    fun x ->\\n        let (ares, alog) = a x\\n        let (bres, blog) = b ares\\n        (bres, alog + blog)\\n\\nlet incrementAndDouble = compose increment double\\n\\nlet (result, log) = incrementAndDouble 2\\n\\nprintfn \\\"Result: %i, Log: %s\\\" result log\\n```\\n\\nI really like the clarity of the of this code, and especially that through the powerful type inference capabilities of the F# compiler we get both strict strong typing and genericity without explicitly specifying any type signature.\\n\\nThe automatically inferred type signature of the `compose` function is the following (where `a * b` means a tuple of `a` and `b`):\\n\\n```fsharp\\nval compose : a:('a -> 'b * string) -> b:('b -> 'c * string) -> x:'a -> 'c * string\\n```\\n\\nWhich nicely resonates with the generic type signature we had to specify in the C# implementation.\\n\\nI hope you found this post interesting, andespecially since I'm still in the very beginning of learning about category theoryany feedback and suggestion is welcome.\\nI'll try to write some more posts like this as I continue experimenting with concepts of category theory in C# and F#.\"}]],\"sections\":[[10,0]]}",
        "html": "<h1 id=\"introduction\">Introduction</h1>\n<p>Recently I learnt about an interesting concept in category theory called a <em>Kleisli category</em>.</p>\n<p>Let's look at a concrete example I took from <a href=\"https://bartoszmilewski.com/2014/12/23/kleisli-categories/\">this blog post</a> (in his series about category theory) by Bartosz Milewski.<br>\nWe would like to extend all of the functions in a library in a way that besides their normal result, they also return an extra string. We'll consider this extra string a sort of log message or &quot;comment&quot;, which we'll collect as we call various methods.</p>\n<p>For example we might have the following original method.</p>\n<pre><code class=\"language-csharp\">int Increment(int x)\n{\n    return x + 1;\n}\n</code></pre>\n<p>We can extend it the following way to also return a message.</p>\n<pre><code class=\"language-csharp\">(int result, string log) Increment(int x)\n{\n    return (x + 1, &quot;Incremented&quot;);\n}\n</code></pre>\n<p>(I wanted to be fancy, so I'm using C# 7 tuple literals, but I could also have used a <code>Tuple&lt;int, string&gt;</code> as the return type.)<br>\nFor the sake of the example I'll implement another method.</p>\n<pre><code class=\"language-csharp\">(int result, string log) Double(int x)\n{\n    return (x * 2, &quot;Doubled&quot;);\n}\n</code></pre>\n<p>If we extend the return type of a function with something extra like this, we call it an <em>embellished</em> function. We could imagine many other useful ways to embellish functions. For example we could return what the execution time of the function was, or return a second value containing any errors happened during the call.</p>\n<p>It turns out that if we define a way to compose such embellished functions (for example compose <code>Increment</code> and <code>Double</code> so that they are applied after each other), and if we can compose the embellished return values associatively (for example the extra log messages can be concatenated), then our input and output data types, and our functions will form a specific <em>category</em> of category theory called the <em><a href=\"https://en.wikipedia.org/wiki/Kleisli_category\">Kleisli category</a></em>. If you are interested in the details, I recommend the <a href=\"https://bartoszmilewski.com/2014/12/23/kleisli-categories/\">above mentioned post</a>.</p>\n<h1 id=\"composition\">Composition</h1>\n<p>In this post I'm going to focus on the <strong>composition</strong> part, more specifically how such composition can be implemented in C#.</p>\n<p>We want to have a methodlet's call it <code>Compose</code>which takes two such embellished functions as input, and returns their composition, that is, a method that executes the two functions after each other, returns the second output, and concatenates the extra log messages.<br>\n<em>(Note: I'm using the C# terminology pretty loosely here. In C# functions are not first class citizens to the extent they are in some other languages, although we can have various constructs that we can use in a similar way, such as delegates, method groups, anonymous functions or lambda expressions.)</em></p>\n<p>It was pretty straightforward to implement a specific composition method for the above scenario. The only tricky part, that needed an extra mental step is that in this method we shouldn't actually <em>call</em> the functions passed as inputs, but we just have to return a function that will call them when executed.</p>\n<pre><code class=\"language-csharp\">Func&lt;int, (int, string)&gt; Compose(Func&lt;int, (int, string)&gt; a, Func&lt;int, (int, string)&gt; b)\n{\n    return (int x) =&gt;\n    {\n        var (aResult, aLog) = a(x);\n        var (bResult, bLog) = b(aResult);\n        return (bResult, aLog + bLog);\n    };\n}\n</code></pre>\n<p>We can try our composition method in a console application.</p>\n<pre><code class=\"language-csharp\">var incrementAndDouble = Compose(Increment, Double);\n\nvar (result, log) = incrementAndDouble(2);\n\nConsole.WriteLine($&quot;Result: {result}, Log: {log}&quot;);\n</code></pre>\n<p>And in the terminal we'll see that it works:</p>\n<pre><code>Result: 6, Log: IncrementedDoubled\n</code></pre>\n<h2 id=\"genericcomposition\">Generic composition</h2>\n<p>The second thing I wanted to achieve is to improve the <code>Compose</code> method by making it generic, so that it can work for all input and output types, not just for integers. This didn't seem to be difficult to achieve, I replaced the concrete types with generic type parameters (except the <code>string</code>, with which the return type is embellished).</p>\n<pre><code class=\"language-csharp\">Func&lt;A, (C, string)&gt; Compose&lt;A, B, C&gt;(Func&lt;A, (B, string)&gt; a, Func&lt;B, (C, string)&gt; b)\n{\n    return (A x) =&gt;\n    {\n        var (aResult, aLog) = a(x);\n        var (bResult, bLog) = b(aResult);\n        return (bResult, aLog + bLog);\n    };\n}\n</code></pre>\n<p>The configuration of the type parameters nicely correspond to function composition in a mathematical sense, since if we have a function <code>f :: a -&gt; b</code> and <code>g :: b -&gt; c</code>, then the type of their composition will be <code>g  f :: a -&gt; c</code>. Notice that although we use integers both as our input and output, in the generic <code>Compose</code> we still have different type arguments for the parameters and the return values, so it also supports functions that have different input and output types.</p>\n<p>I was really happy with this implementation, the building blocks of C# seemed to fall nicely in place. Unfortunately when I tried to use this generic version of <code>Compose</code> with the same call:</p>\n<pre><code class=\"language-csharp\">var incrementAndDouble = Compose(Increment, Double);\n</code></pre>\n<p>I received the following build error.</p>\n<blockquote>\n<p><code>Error CS0411: The type arguments for method 'Program.Compose&lt;A, B, C&gt;(Func&lt;A, (B, string)&gt;, Func&lt;B, (C, string)&gt;)' cannot be inferred from the usage. Try specifying the type arguments explicitly.</code></p>\n</blockquote>\n<p>I was surprised by this error message, and couldn't immediately figure out its reason. Since in the two methods <code>Increment</code> and <code>Double</code> the parameter and return types are explicitly specified, I thought the compiler would be able to <em>infer</em> the proper values for the type arguments.</p>\n<p>I found the reason in the answer to <a href=\"http://stackoverflow.com/questions/7400550/c-sharp-infer-generic-type-based-on-passing-a-delegate\">this SO question</a>: The C# spec states that type inference based on the output type only works if the types are explicitly specified, <em>or</em> if we pass in an anonymous function. The C# constructs we're trying to pass in to <code>Compose</code> are <em>method groups</em>, and the spec does not require the type inference to work for them. (This would not be impossible to do, but it would probably not worth the effort by the compiler, and it could cause more problems, as stated by Eric Lippert in <a href=\"http://stackoverflow.com/questions/6229131/why-cant-c-sharp-infer-type-from-this-seemingly-simple-obvious-case#comment7312385_6231921\">this comment</a>. One of the reasons why this is problematic, is that a method group might have more than one corresponding method underneath because of method overloading, in which case the type inference wouldn't necessarily be unambiguous.)</p>\n<p>The way to work around this is to help the compiler, and somehow tell it the actual types of our methods.<br>\nWe can either explicitly specify the type arguments:</p>\n<pre><code class=\"language-csharp\">var incrementAndDouble = Compose&lt;int, int, int&gt;(Increment, Double);\n</code></pre>\n<p>Oras the spec suggestspass in the methods as anonymous functions instead of method groups:</p>\n<pre><code class=\"language-csharp\">var incrementAndDouble = Compose((int x) =&gt; Increment(x), (int x) =&gt; Double(x));\n</code></pre>\n<p>I'm not particularly happy about these workarounds, but so far I couldn't find an easier way to do this in C#.</p>\n<p>I'm not sure if I'll ever use this approach in a real project, but I found this example interesting enough to share, especially regarding the limitations of the C# type inference algorithm.</p>\n<h1 id=\"compositioninf\">Composition in F#</h1>\n<p>Just to see what the same thing would look like in a language where functions are first-class citizens (and where we don't have function overloading), here is the implementation of the same two functions and the generic composition in F#.</p>\n<pre><code class=\"language-fsharp\">let increment x = (x + 1, &quot;Incremented&quot;)\n\nlet double x = (x * 2, &quot;Doubled&quot;)\n\nlet compose a b =\n    fun x -&gt;\n        let (ares, alog) = a x\n        let (bres, blog) = b ares\n        (bres, alog + blog)\n\nlet incrementAndDouble = compose increment double\n\nlet (result, log) = incrementAndDouble 2\n\nprintfn &quot;Result: %i, Log: %s&quot; result log\n</code></pre>\n<p>I really like the clarity of the of this code, and especially that through the powerful type inference capabilities of the F# compiler we get both strict strong typing and genericity without explicitly specifying any type signature.</p>\n<p>The automatically inferred type signature of the <code>compose</code> function is the following (where <code>a * b</code> means a tuple of <code>a</code> and <code>b</code>):</p>\n<pre><code class=\"language-fsharp\">val compose : a:('a -&gt; 'b * string) -&gt; b:('b -&gt; 'c * string) -&gt; x:'a -&gt; 'c * string\n</code></pre>\n<p>Which nicely resonates with the generic type signature we had to specify in the C# implementation.</p>\n<p>I hope you found this post interesting, andespecially since I'm still in the very beginning of learning about category theoryany feedback and suggestion is welcome.<br>\nI'll try to write some more posts like this as I continue experimenting with concepts of category theory in C# and F#.</p>\n",
        "comment_id": "36",
        "plaintext": "Introduction\nRecently I learnt about an interesting concept in category theory called a \nKleisli category.\n\nLet's look at a concrete example I took from this blog post\n[https://bartoszmilewski.com/2014/12/23/kleisli-categories/]  (in his series\nabout category theory) by Bartosz Milewski.\nWe would like to extend all of the functions in a library in a way that besides\ntheir normal result, they also return an extra string. We'll consider this extra\nstring a sort of log message or \"comment\", which we'll collect as we call\nvarious methods.\n\nFor example we might have the following original method.\n\nint Increment(int x)\n{\n    return x + 1;\n}\n\n\nWe can extend it the following way to also return a message.\n\n(int result, string log) Increment(int x)\n{\n    return (x + 1, \"Incremented\");\n}\n\n\n(I wanted to be fancy, so I'm using C# 7 tuple literals, but I could also have\nused a Tuple<int, string>  as the return type.)\nFor the sake of the example I'll implement another method.\n\n(int result, string log) Double(int x)\n{\n    return (x * 2, \"Doubled\");\n}\n\n\nIf we extend the return type of a function with something extra like this, we\ncall it an embellished  function. We could imagine many other useful ways to\nembellish functions. For example we could return what the execution time of the\nfunction was, or return a second value containing any errors happened during the\ncall.\n\nIt turns out that if we define a way to compose such embellished functions (for\nexample compose Increment  and Double  so that they are applied after each\nother), and if we can compose the embellished return values associatively (for\nexample the extra log messages can be concatenated), then our input and output\ndata types, and our functions will form a specific category  of category theory\ncalled the Kleisli category [https://en.wikipedia.org/wiki/Kleisli_category]. If\nyou are interested in the details, I recommend the above mentioned post\n[https://bartoszmilewski.com/2014/12/23/kleisli-categories/].\n\nComposition\nIn this post I'm going to focus on the composition  part, more specifically how\nsuch composition can be implemented in C#.\n\nWe want to have a methodlet's call it Composewhich takes two such embellished\nfunctions as input, and returns their composition, that is, a method that\nexecutes the two functions after each other, returns the second output, and\nconcatenates the extra log messages.\n(Note: I'm using the C# terminology pretty loosely here. In C# functions are not\nfirst class citizens to the extent they are in some other languages, although we\ncan have various constructs that we can use in a similar way, such as delegates,\nmethod groups, anonymous functions or lambda expressions.)\n\nIt was pretty straightforward to implement a specific composition method for the\nabove scenario. The only tricky part, that needed an extra mental step is that\nin this method we shouldn't actually call  the functions passed as inputs, but\nwe just have to return a function that will call them when executed.\n\nFunc<int, (int, string)> Compose(Func<int, (int, string)> a, Func<int, (int, string)> b)\n{\n    return (int x) =>\n    {\n        var (aResult, aLog) = a(x);\n        var (bResult, bLog) = b(aResult);\n        return (bResult, aLog + bLog);\n    };\n}\n\n\nWe can try our composition method in a console application.\n\nvar incrementAndDouble = Compose(Increment, Double);\n\nvar (result, log) = incrementAndDouble(2);\n\nConsole.WriteLine($\"Result: {result}, Log: {log}\");\n\n\nAnd in the terminal we'll see that it works:\n\nResult: 6, Log: IncrementedDoubled\n\n\nGeneric composition\nThe second thing I wanted to achieve is to improve the Compose  method by making\nit generic, so that it can work for all input and output types, not just for\nintegers. This didn't seem to be difficult to achieve, I replaced the concrete\ntypes with generic type parameters (except the string, with which the return\ntype is embellished).\n\nFunc<A, (C, string)> Compose<A, B, C>(Func<A, (B, string)> a, Func<B, (C, string)> b)\n{\n    return (A x) =>\n    {\n        var (aResult, aLog) = a(x);\n        var (bResult, bLog) = b(aResult);\n        return (bResult, aLog + bLog);\n    };\n}\n\n\nThe configuration of the type parameters nicely correspond to function\ncomposition in a mathematical sense, since if we have a function f :: a -> b \nand g :: b -> c, then the type of their composition will be g  f :: a -> c.\nNotice that although we use integers both as our input and output, in the\ngeneric Compose  we still have different type arguments for the parameters and\nthe return values, so it also supports functions that have different input and\noutput types.\n\nI was really happy with this implementation, the building blocks of C# seemed to\nfall nicely in place. Unfortunately when I tried to use this generic version of \nCompose  with the same call:\n\nvar incrementAndDouble = Compose(Increment, Double);\n\n\nI received the following build error.\n\nError CS0411: The type arguments for method 'Program.Compose<A, B, C>(Func<A,\n(B, string)>, Func<B, (C, string)>)' cannot be inferred from the usage. Try\nspecifying the type arguments explicitly.\n\nI was surprised by this error message, and couldn't immediately figure out its\nreason. Since in the two methods Increment  and Double  the parameter and return\ntypes are explicitly specified, I thought the compiler would be able to infer \nthe proper values for the type arguments.\n\nI found the reason in the answer to this SO question\n[http://stackoverflow.com/questions/7400550/c-sharp-infer-generic-type-based-on-passing-a-delegate]\n: The C# spec states that type inference based on the output type only works if\nthe types are explicitly specified, or  if we pass in an anonymous function. The\nC# constructs we're trying to pass in to Compose  are method groups, and the\nspec does not require the type inference to work for them. (This would not be\nimpossible to do, but it would probably not worth the effort by the compiler,\nand it could cause more problems, as stated by Eric Lippert in this comment. One\nof the reasons why this is problematic, is that a method group might have more\nthan one corresponding method underneath because of method overloading, in which\ncase the type inference wouldn't necessarily be unambiguous.)\n\nThe way to work around this is to help the compiler, and somehow tell it the\nactual types of our methods.\nWe can either explicitly specify the type arguments:\n\nvar incrementAndDouble = Compose<int, int, int>(Increment, Double);\n\n\nOras the spec suggestspass in the methods as anonymous functions instead of\nmethod groups:\n\nvar incrementAndDouble = Compose((int x) => Increment(x), (int x) => Double(x));\n\n\nI'm not particularly happy about these workarounds, but so far I couldn't find\nan easier way to do this in C#.\n\nI'm not sure if I'll ever use this approach in a real project, but I found this\nexample interesting enough to share, especially regarding the limitations of the\nC# type inference algorithm.\n\nComposition in F#\nJust to see what the same thing would look like in a language where functions\nare first-class citizens (and where we don't have function overloading), here is\nthe implementation of the same two functions and the generic composition in F#.\n\nlet increment x = (x + 1, \"Incremented\")\n\nlet double x = (x * 2, \"Doubled\")\n\nlet compose a b =\n    fun x ->\n        let (ares, alog) = a x\n        let (bres, blog) = b ares\n        (bres, alog + blog)\n\nlet incrementAndDouble = compose increment double\n\nlet (result, log) = incrementAndDouble 2\n\nprintfn \"Result: %i, Log: %s\" result log\n\n\nI really like the clarity of the of this code, and especially that through the\npowerful type inference capabilities of the F# compiler we get both strict\nstrong typing and genericity without explicitly specifying any type signature.\n\nThe automatically inferred type signature of the compose  function is the\nfollowing (where a * b  means a tuple of a  and b):\n\nval compose : a:('a -> 'b * string) -> b:('b -> 'c * string) -> x:'a -> 'c * string\n\n\nWhich nicely resonates with the generic type signature we had to specify in the\nC# implementation.\n\nI hope you found this post interesting, andespecially since I'm still in the\nvery beginning of learning about category theoryany feedback and suggestion is\nwelcome.\nI'll try to write some more posts like this as I continue experimenting with\nconcepts of category theory in C# and F#.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "Taking a look at how the composition of the Kleisli category can be implemented in C#, and what are the limitations we have to face in type inference.",
        "author_id": "1",
        "created_at": "2017-04-19 21:23:06",
        "created_by": "1",
        "updated_at": "2017-04-21 13:29:40",
        "updated_by": "1",
        "published_at": "2017-04-19 21:27:16",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2978",
        "uuid": "386f3335-e854-4133-99da-b6645b3c4806",
        "title": "Jumpstart F# web development: F# with Suave.IO on .NET Core",
        "slug": "jumpstart-f-web-development-f-with-suave-io-on-net-core",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In a [previous post](/jumpstart-f-web-development-f-with-suave-io-on-classic-net/) we've seen how we can create a simple web application with Suave on the full .NET Framework.\\nIn the last post of the [series](/series-jumpstart-f-web-development) we'll take a look at how we can do the same thing on .NET Core.\\n\\nThis is gonna be a short post, since there are no real gotchas in this scenario, it's really easy to set everything up.\\n\\n**Note**: In this post I will use the new csproj-based .NET tooling. Since the project.json-based tooling is deprecated, I don't think anybody should invest in using it any more. I was using version **1.0.1** of the `dotnet` CLI when writing this post.\\n\\n# Create a Suave app on .NET Core\\n\\nLet's start by creating a new F# project. Since a Suave web app is just a Console app in which we fire up our web server, we can use the Console project template.\\n\\n```bash\\nmkdir mysuaveapp\\ncd mysuaveapp\\ndotnet new console -lang f#\\n```\\n\\nThen add the package Suave as a dependency.\\n\\n```bash\\ndotnet add package Suave\\n```\\n\\nNow we can implement a simple web application.\\nThis is what the `console` template scaffolded for us by default in the `Program.fs` file:\\n\\n```fsharp\\nopen System\\n\\n[<EntryPoint>]\\nlet main argv =\\n    printfn \\\"Hello World from F#!\\\"\\n    0\\n```\\n\\nModify this to have the following content, starting up a Suave web app with a couple of routes.\\n\\n```fsharp\\nopen Suave\\nopen Suave.Filters\\nopen Suave.Operators\\nopen Suave.Successful\\n\\nlet app =\\n    choose\\n        [ GET >=> choose\\n            [ path \\\"/\\\" >=> OK \\\"Index\\\"\\n              path \\\"/hello\\\" >=> OK \\\"Hello!\\\" ]\\n          POST >=> choose\\n            [ path \\\"/hello\\\" >=> OK \\\"Hello POST!\\\" ] ]\\n\\n[<EntryPoint>]\\nlet main argv =\\n    startWebServer defaultConfig app\\n    0\\n```\\n\\nThe last thing we have to do is restore the dependencies and start up our app.\\n\\n```bash\\ndotnet restore\\ndotnet run\\n```\\n\\nThat's it! Our app should be running, and we can open it by navigating to http://localhost:8080.\\nI also uploaded a working example to [this Github repository](https://github.com/markvincze/jumpstart-netcore-suave).\\n\\n# Different ways to run Suave\\n\\nIn the above example we are running Suave directly on top of .NET Core using the web server built-in the library. However, since .NET Core is modular, that's not the only option. We can also run Suave on top of the web server created for ASP.NET Core called Kestrel, or we can even run it on top of ASP.NET Core using a special middleware.\\n\\n![Different options to run Suave on .NET Core](/content/images/2017/03/suave-options.png)\\n\\nWe can do this with the following packages.\\n\\n - [Suave.Kestrel](https://github.com/Krzysztof-Cieslak/Suave.Kestrel): Run Suave on top of the Kestrel web server. (This is a early proof of concept implementation, and doesn't seem to be actively worked on any more.)\\n - [Suave.AspNetCore](https://github.com/SuaveIO/Suave.AspNetCore): Run Suave on top of ASP.NET Core, using a middleware to connect the two frameworks together. This library seems to be more actively developed, and with this we can even combine ASP.NET and Suave endpoints in the same application.\\n\\n# F# tooling issues\\n\\nAs described on [this Github page](https://github.com/dotnet/netcorecli-fsc/wiki/.NET-Core-SDK-1.0.1#ide-support), at the moment there are some issues with the current version of the FSharp SDK and the Ionide extension (which is used in VSCode for F# development). The extension does not work with the 1.0 version of the SDK.\\nThe workaround to get it to work is to change the version to `1.0.0-beta-060000`, and to add a reference to the tool `dotnet-compile-fsc`. I did this change in the repo on the [this branch](https://github.com/markvincze/jumpstart-netcore-suave/tree/vscode-inoide-workaround).\\n\\nAlso, if you are using VSCode, you have to do `dotnet build` from the terminal before starting the editor to make it work properly.\\n\\nWith the release of the 1.0 version of the new csproj-based dotnet tooling, I expect all the problems related to editor integration and tooling to be fixed soon.\"}]],\"sections\":[[10,0]]}",
        "html": "<p>In a <a href=\"/jumpstart-f-web-development-f-with-suave-io-on-classic-net/\">previous post</a> we've seen how we can create a simple web application with Suave on the full .NET Framework.<br>\nIn the last post of the <a href=\"/series-jumpstart-f-web-development\">series</a> we'll take a look at how we can do the same thing on .NET Core.</p>\n<p>This is gonna be a short post, since there are no real gotchas in this scenario, it's really easy to set everything up.</p>\n<p><strong>Note</strong>: In this post I will use the new csproj-based .NET tooling. Since the project.json-based tooling is deprecated, I don't think anybody should invest in using it any more. I was using version <strong>1.0.1</strong> of the <code>dotnet</code> CLI when writing this post.</p>\n<h1 id=\"createasuaveapponnetcore\">Create a Suave app on .NET Core</h1>\n<p>Let's start by creating a new F# project. Since a Suave web app is just a Console app in which we fire up our web server, we can use the Console project template.</p>\n<pre><code class=\"language-bash\">mkdir mysuaveapp\ncd mysuaveapp\ndotnet new console -lang f#\n</code></pre>\n<p>Then add the package Suave as a dependency.</p>\n<pre><code class=\"language-bash\">dotnet add package Suave\n</code></pre>\n<p>Now we can implement a simple web application.<br>\nThis is what the <code>console</code> template scaffolded for us by default in the <code>Program.fs</code> file:</p>\n<pre><code class=\"language-fsharp\">open System\n\n[&lt;EntryPoint&gt;]\nlet main argv =\n    printfn &quot;Hello World from F#!&quot;\n    0\n</code></pre>\n<p>Modify this to have the following content, starting up a Suave web app with a couple of routes.</p>\n<pre><code class=\"language-fsharp\">open Suave\nopen Suave.Filters\nopen Suave.Operators\nopen Suave.Successful\n\nlet app =\n    choose\n        [ GET &gt;=&gt; choose\n            [ path &quot;/&quot; &gt;=&gt; OK &quot;Index&quot;\n              path &quot;/hello&quot; &gt;=&gt; OK &quot;Hello!&quot; ]\n          POST &gt;=&gt; choose\n            [ path &quot;/hello&quot; &gt;=&gt; OK &quot;Hello POST!&quot; ] ]\n\n[&lt;EntryPoint&gt;]\nlet main argv =\n    startWebServer defaultConfig app\n    0\n</code></pre>\n<p>The last thing we have to do is restore the dependencies and start up our app.</p>\n<pre><code class=\"language-bash\">dotnet restore\ndotnet run\n</code></pre>\n<p>That's it! Our app should be running, and we can open it by navigating to <a href=\"http://localhost:8080\">http://localhost:8080</a>.<br>\nI also uploaded a working example to <a href=\"https://github.com/markvincze/jumpstart-netcore-suave\">this Github repository</a>.</p>\n<h1 id=\"differentwaystorunsuave\">Different ways to run Suave</h1>\n<p>In the above example we are running Suave directly on top of .NET Core using the web server built-in the library. However, since .NET Core is modular, that's not the only option. We can also run Suave on top of the web server created for ASP.NET Core called Kestrel, or we can even run it on top of ASP.NET Core using a special middleware.</p>\n<p><img src=\"/content/images/2017/03/suave-options.png\" alt=\"Different options to run Suave on .NET Core\"></p>\n<p>We can do this with the following packages.</p>\n<ul>\n<li><a href=\"https://github.com/Krzysztof-Cieslak/Suave.Kestrel\">Suave.Kestrel</a>: Run Suave on top of the Kestrel web server. (This is a early proof of concept implementation, and doesn't seem to be actively worked on any more.)</li>\n<li><a href=\"https://github.com/SuaveIO/Suave.AspNetCore\">Suave.AspNetCore</a>: Run Suave on top of ASP.NET Core, using a middleware to connect the two frameworks together. This library seems to be more actively developed, and with this we can even combine ASP.NET and Suave endpoints in the same application.</li>\n</ul>\n<h1 id=\"ftoolingissues\">F# tooling issues</h1>\n<p>As described on <a href=\"https://github.com/dotnet/netcorecli-fsc/wiki/.NET-Core-SDK-1.0.1#ide-support\">this Github page</a>, at the moment there are some issues with the current version of the FSharp SDK and the Ionide extension (which is used in VSCode for F# development). The extension does not work with the 1.0 version of the SDK.<br>\nThe workaround to get it to work is to change the version to <code>1.0.0-beta-060000</code>, and to add a reference to the tool <code>dotnet-compile-fsc</code>. I did this change in the repo on the <a href=\"https://github.com/markvincze/jumpstart-netcore-suave/tree/vscode-inoide-workaround\">this branch</a>.</p>\n<p>Also, if you are using VSCode, you have to do <code>dotnet build</code> from the terminal before starting the editor to make it work properly.</p>\n<p>With the release of the 1.0 version of the new csproj-based dotnet tooling, I expect all the problems related to editor integration and tooling to be fixed soon.</p>\n",
        "comment_id": "33",
        "plaintext": "In a previous post\n[/jumpstart-f-web-development-f-with-suave-io-on-classic-net/]  we've seen how\nwe can create a simple web application with Suave on the full .NET Framework.\nIn the last post of the series [/series-jumpstart-f-web-development]  we'll take\na look at how we can do the same thing on .NET Core.\n\nThis is gonna be a short post, since there are no real gotchas in this scenario,\nit's really easy to set everything up.\n\nNote: In this post I will use the new csproj-based .NET tooling. Since the\nproject.json-based tooling is deprecated, I don't think anybody should invest in\nusing it any more. I was using version 1.0.1  of the dotnet  CLI when writing\nthis post.\n\nCreate a Suave app on .NET Core\nLet's start by creating a new F# project. Since a Suave web app is just a\nConsole app in which we fire up our web server, we can use the Console project\ntemplate.\n\nmkdir mysuaveapp\ncd mysuaveapp\ndotnet new console -lang f#\n\n\nThen add the package Suave as a dependency.\n\ndotnet add package Suave\n\n\nNow we can implement a simple web application.\nThis is what the console  template scaffolded for us by default in the \nProgram.fs  file:\n\nopen System\n\n[<EntryPoint>]\nlet main argv =\n    printfn \"Hello World from F#!\"\n    0\n\n\nModify this to have the following content, starting up a Suave web app with a\ncouple of routes.\n\nopen Suave\nopen Suave.Filters\nopen Suave.Operators\nopen Suave.Successful\n\nlet app =\n    choose\n        [ GET >=> choose\n            [ path \"/\" >=> OK \"Index\"\n              path \"/hello\" >=> OK \"Hello!\" ]\n          POST >=> choose\n            [ path \"/hello\" >=> OK \"Hello POST!\" ] ]\n\n[<EntryPoint>]\nlet main argv =\n    startWebServer defaultConfig app\n    0\n\n\nThe last thing we have to do is restore the dependencies and start up our app.\n\ndotnet restore\ndotnet run\n\n\nThat's it! Our app should be running, and we can open it by navigating to \nhttp://localhost:8080.\nI also uploaded a working example to this Github repository\n[https://github.com/markvincze/jumpstart-netcore-suave].\n\nDifferent ways to run Suave\nIn the above example we are running Suave directly on top of .NET Core using the\nweb server built-in the library. However, since .NET Core is modular, that's not\nthe only option. We can also run Suave on top of the web server created for\nASP.NET Core called Kestrel, or we can even run it on top of ASP.NET Core using\na special middleware.\n\n\n\nWe can do this with the following packages.\n\n * Suave.Kestrel [https://github.com/Krzysztof-Cieslak/Suave.Kestrel]: Run Suave\n   on top of the Kestrel web server. (This is a early proof of concept\n   implementation, and doesn't seem to be actively worked on any more.)\n * Suave.AspNetCore [https://github.com/SuaveIO/Suave.AspNetCore]: Run Suave on\n   top of ASP.NET Core, using a middleware to connect the two frameworks\n   together. This library seems to be more actively developed, and with this we\n   can even combine ASP.NET and Suave endpoints in the same application.\n\nF# tooling issues\nAs described on this Github page, at the moment there are some issues with the\ncurrent version of the FSharp SDK and the Ionide extension (which is used in\nVSCode for F# development). The extension does not work with the 1.0 version of\nthe SDK.\nThe workaround to get it to work is to change the version to 1.0.0-beta-060000,\nand to add a reference to the tool dotnet-compile-fsc. I did this change in the\nrepo on the this branch\n[https://github.com/markvincze/jumpstart-netcore-suave/tree/vscode-inoide-workaround]\n.\n\nAlso, if you are using VSCode, you have to do dotnet build  from the terminal\nbefore starting the editor to make it work properly.\n\nWith the release of the 1.0 version of the new csproj-based dotnet tooling, I\nexpect all the problems related to editor integration and tooling to be fixed\nsoon.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "An introduction to get started with web development in F#, using the Suave web framework on .NET Core.",
        "author_id": "1",
        "created_at": "2017-03-12 12:48:15",
        "created_by": "1",
        "updated_at": "2017-04-21 13:31:06",
        "updated_by": "1",
        "published_at": "2017-03-12 12:51:45",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2979",
        "uuid": "661e7e2b-9f28-4617-b74e-deabd3d9c825",
        "title": "A data model exercise in two languages, part 1: C#",
        "slug": "a-data-model-exercise-in-two-languages-part-1-c",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"\\n# Introduction\\n\\nWhen I'm learning a new programming language, I usually like to do some coding exercises to get familiar with the various language features, and to get used to the syntax.\\nMany of these exercisesor *katas*are about implementing some kind of algorithm, which is a great way to learn about the control structures of the language, the conditions, loops and functions.\\nOther katas are more focused on designing a data model for a certain domain, where the goal is to utilize the various features of the type system to create a model as expressive and intuitive as possible.\\n\\nSince I've been learning F#, I've been doing some data modeling exercises to learn what F# type system is capable of. And because I come from a C# background, I often compare my solutions to their C# counterpart, to be able to grasp the differences between the data models used in a functional and an object-oriented language.\\n\\nIn this and the next post I'll take a look at a very simple data modelling exercise: creating a data type representing a card from the [standard 52-card deck](https://en.wikipedia.org/wiki/Standard_52-card_deck). I found this kata simple enough to describe it in detail in a blog post, but it has enough quirks that we can get our teeth into it.  \\nIn this first post I'll do it in C#, and in the next one I'm going to do the same in F#, where I'll try to contrast how the functional features of F# can solve problems that are cumbersome to express in OO languages.\\n\\n# The task\\n\\nThe task I'd like to solve in this simple exercise is to design a data model to represent a card in the standard 52-card deck. For now I'm only interested in representing one single card, and not a full deck.\\n\\nWe can specify a card with the following points.\\n\\n - Every card has a suit (except the Joker), which can be clubs (), diamonds (<span style=\\\"color: red\\\"></span>), hearts (<span style=\\\"color: red\\\"></span>) and spades ().\\n - A card can be either\\n  - a card with a number on it between 2 and 10, I'll call this a *value card*,\\n  - or it can be one of the *face cards*: Jack, Queen, King or Ace.  \\n  *(Note: the terminology is not 100% unambiguous, some sources don't call the Ace a face card, but I'll consider it as one for the sake of this exercise.)*\\n - There is a special card, the Joker, which does not have a suit.\\n\\n![Image illustrating the standard 52-card deck.](/content/images/2017/05/cards.png)\\n\\nI would like to implement a data model, which potentially can be used by multiple different algorithms. More concretely, let's imagine that we deliver the data model in a self-contained package, and then we can implement the logic necessary to model various card games, which all depend on this single data model.  \\nWhat this means in practice is that I would not like to mix data and logic in our data types, but rather just focus on the data. This might be different than what encapsulation in OO would suggest, but it's necessary if we want to have a data model which then can be used in several different algorithms (which is a typical practice in functional programming).\\n\\n# Implementing in C&#35;\\n\\nWhen we start to implement this data model in C#, it seems intuitive that we'll probably need an enum for the suit of a card.\\n\\n```csharp\\nenum Suit\\n{\\n    Clubs,\\n    Hearts,\\n    Diamonds,\\n    Spades\\n}\\n```\\n\\nSimilarly, I'll creat an enum type for the different kind of face types.  \\n(It's debatable whether Ace is considered a face card or not, in this model I'll assume that it is.)\\n\\n```csharp\\nenum Face\\n{\\n    Jack,\\n    Queen,\\n    King,\\n    Ace\\n}\\n```\\n\\nWith these in place I can create the actual type representing a card in the deck. I'll add a `Suit`, a `Face` and a `Value` property, saying that a card is either a face card or a value card, so that only one of those properties will have a value at any given time.  \\nSince these are represented with value types, we have to make them nullable to be able to say they might not have a value. I also added a constructor.\\n\\n```csharp\\nclass Card\\n{\\n    public Suit Suit { get; set; }\\n\\n    public Face? Face { get; set; }\\n\\n    public int? Value { get; set; }\\n\\n    public Card(Suit suit, Face? face, int? value)\\n    {\\n        Suit = suit;\\n        Face = face;\\n        Value = value;\\n    }\\n}\\n```\\n\\nThis type seems to be covering our requirements, since any card of the deck can be represented with an instance of it.\\n\\n## Avoid invalid states ( la DDD)\\n\\nWe cannot be completely satisfied yet: this data model violates an important guideline of domain-driven design (and just generally an all-around good practice): **Design our data model in a way that illegal states are not representable**.  \\nThis is beneficial for two main reasons.\\n\\n - It helps avoiding bugs we would bump into due to invalid data.\\n - It makes implementing any sort of validation logic easier, since (at least some of) the validity of our data is immediately enforced by our data model.\\n\\nOur current model doesn't satisfy this requirement, since we're able to do this:\\n\\n```csharp\\nvar card = new Card(Suit.Clubs, Face.Jack, 5);\\n```\\n\\nSince this card instance will have both of its `Face` and `Value` property set, we will have no way deciding what it actually represent. We should not allow an instance like this to be created.\\n\\nThis can be nicely solved by a simple C# pattern I like to call the *Factory method pattern*. (Note: you might find other sources using the same term to denote a slightly more complicated pattern.)  \\nWe can make our constructor private, and introduce designated public methods (the *factory methods*) for creating instances of various kinds. Since the constructor is not accessible from the outside, we just have to make sure that our factory methods initialize the instance in a way that it represents a valid value.\\n\\n```csharp\\nclass Card\\n{\\n    public Suit Suit { get; set; }\\n\\n    public Face? Face { get; set; }\\n\\n    public int? Value { get; set; }\\n\\n    private Card(Suit suit, Face? face, int? value)\\n    {\\n        Suit = suit;\\n        Face = face;\\n        Value = value;\\n    }\\n    \\n    public static Card CreateFace(Suit suit, Face face)\\n    {\\n        return new Card(suit, face, null);\\n    }\\n    \\n    public static Card CreateValue(Suit suit, int value)\\n    {\\n        return new Card(suit, null, value);\\n    }\\n}\\n```\\n\\nBetter, now we cannot use the constructor from outside, we have to use one of the two factory methods, thereby enforcing us to create only valid instances.\\n\\n```csharp\\nvar card = Card.CreateFace(Suit.Clubs, Face.Jack);\\n```\\n\\nI use this pattern all the time, not just to enforce some preconditions, but also to make code more self-documenting by introducing these expressive method names for object creation.\\n\\n## Mutability\\n\\nOf course we have a glaring problem: since our properties have public setters, there is nothing stopping us from creating a correct instance with the factory methods, but then mutate the instance afterwards to make it invalid.\\n\\n```csharp\\nvar invalid = Card.CreateFace(Suit.Clubs, Face.Jack);\\ninvalid.Value = 5;\\n```\\n\\nThis is easy to mitigate, just remove the setters to make our type immutable (which is beneficial to strive for anyway).\\n\\n```csharp\\nclass Card\\n{\\n    public Suit Suit { get; }\\n\\n    public Face? Face { get; }\\n\\n    public int? Value { get; }\\n\\n    ...\\n}\\n```\\n\\n(This is something that I could've done immediately, but I wanted to illustrate the thought process that often happens during implementing a data model in C#, wherenot to be condescending, just based on my experiencenot necessarily every developer thinks about mutability consciously.)\\n\\n## Finishing up\\n\\nWhen I first started implementing this data model, I initially forgot about the fact that we also have to support the Joker cards (this is not just for the sake of the example, I did actually forgot :)), this is the last thing we have to cover.\\n\\nWe could say that if both `Face` and `Value` are `null`, we consier the card a Joker, but that feels a bit hacky, let's introduce a boolean instead.  \\nThis is the final data model, covering all use cases.\\n\\n```csharp\\nclass Card\\n{\\n    public Suit Suit { get; }\\n\\n    public Face? Face { get; }\\n\\n    public int? Value { get; }\\n\\n    public bool IsJoker { get; }\\n\\n    private Card(Suit suit, Face? face, int? value, bool isJoker)\\n    {\\n        Suit = suit;\\n        Face = face;\\n        Value = value;\\n        IsJoker = isJoker;\\n    }\\n\\n    public static Card CreateFace(Suit suit, Face face)\\n    {\\n        return new Card(suit, face, null, false);\\n    }\\n\\n    public static Card CreateValue(Suit suit, int value)\\n    {\\n        return new Card(suit, null, value, false);\\n    }\\n\\n    public static Card CreateJoker()\\n    {\\n        return new Card(default(Suit), null, null, true);\\n    }\\n}\\n```\\n\\nI could've made the `Suit` property nullable too, but I didn't have to, since if `IsJoker` is true, we'll ignore the value of `Suit` anyway. However, this this is definitely debatable, it's one of those things where neither approach is obviously better than the other.\\n\\n# Usage\\n\\nLet's see how we would use this in an application. As an example, implement the [score calculation](https://en.wikipedia.org/wiki/Rummy#Scoring) of the card game Rummy.\\n\\n```csharp\\nint CalculateValue(Card card)\\n{\\n    if(card.IsJoker)\\n        return 0;\\n        \\n    if(card.Face.HasValue) // It's a face card\\n    {\\n        if(card.Face.Value == Face.Ace)\\n            return 15;\\n        \\n        if(card.Face.Value == Face.Queen && card.Suit == Suit.Spades)\\n            return 40;\\n        \\n        return 10;\\n    }\\n    \\n    // Now it has to be a value card\\n    if(card.Value.Value == 10)\\n        return 10;\\n    \\n    return 5;\\n}\\n```\\n\\nIt works fine, although I'm not 100% happy with this syntax, it feels a bit awkward to do the `HasValue` check to find out what the object really represents.  \\n*It feels to me that the type system does not help to express our domain, we had to do it ourselves*, we'll see in the next post how this is different in F#.\\n\\n# Possible improvements\\n\\n## Check the *kind* of the card more conveniently\\n\\nIn order to make usage a bit more convenient, we can introduce a new property in the `Card` type explicitly specifying the *kind* of our card. We can introduce a new enum for this purpose.\\n\\n```csharp\\nenum CardKind\\n{\\n    Value,\\n    Face,\\n    Joker\\n}\\n```\\n\\nThen implement the property returning the appropriate value.\\n\\n```csharp\\nclass Card\\n{\\n    public CardKind Kind\\n    {\\n        get\\n        {\\n            if(IsJoker)\\n                return CardKind.Joker;\\n            if(Face.HasValue)\\n                return CardKind.Face;\\n            return CardKind.Value;\\n        }\\n    }\\n    ...\\n}\\n```\\n\\nThis way the actual usage of the type in any given algorithm can become a bit more expressive, we can phrase it like this.\\n\\n```csharp\\nint CalculateValue(Card card)\\n{\\n    switch (card.Kind)\\n    {\\n        case CardKind.Joker:\\n            ...\\n        case CardKind.Face:\\n            ...\\n        case CardKind.Value:\\n            ...\\n    }\\n}\\n```\\n\\nWe can go one step further, andsince we are not using the `HasValue` property of our nullable types to determine the kind of the cardwe can change our properties to return a non-nullable value.  \\nSo instead of\\n\\n```csharp\\nclass Card\\n{\\n    public Face? Face { get; }\\n    ...\\n}\\n```\\n\\nwe can do something like\\n\\n```csharp\\nclass Card\\n{\\n    private Face? face;\\n    public Face Face\\n    {\\n         get\\n         {\\n             if(!face.HasValue)\\n                throw new InvalidOperationException($\\\"You can only retrieve the Face property from a Face card. This is a {Kind} card.\\\");\\n             return face.Value;\\n         }\\n    }\\n    ...\\n}\\n```\\n\\nThis way in our logic using this type we can simply write `card.Face` instead of `card.Face.Value`.\\n\\nThese changes improve the \\\"developer experience\\\" of working with this data model, but keep in mind that these all introduce more and more code we have to implement, thereby increasing the complexity of our data model, giving us more chance to make mistakes. So any improvement like these is always a tradeoff.\\n\\n## What about OO?\\n\\nIf we have learnt OO from a textbook, or at a university, we might have seen an introduction through examples like Hawk -> Bird -> Animal, or Square -> Rectangle -> Shape (and then later in the industry we probably heard many arguments against the validity of such examples, but let's put that aside for a moment :)).  \\nNow if we look at our domain, it seems to fit the same pattern, so if OO dictates creating such inheritance trees, shouldn't we represent the various kinds of card as classes deriving from each other? We could do the following:\\n\\n```csharp\\nabstract class Card { ... }\\n\\nclass FaceCard : Card { ... }\\n\\nclass ValueCard : Card { ... }\\n\\nclass Joker : Card { ... }\\n```\\n\\nWe could definitely do this. My problem with this approach is twofold.  \\nFirst, it is cumbersome to actually use this model in an algorithm. We could either do a switch-case on the type of our card.\\n\\n```csharp\\nint CalculateValue(Card card)\\n{\\n    switch (card)\\n    {\\n        case Joker j:\\n            ...\\n        case FaceCard f:\\n            ...\\n        case ValueCard v:\\n            ...\\n    }\\n}\\n```\\n\\nBut normally this is considered an anti-pattern in object oriented programming. According to pure OO, if we have to switch case on the dynamic type of our object, we are doing something wrong. (Although this is not 100% clear, especially since the ability to do this conveniently has recently been introduced in C# in the form of pattern matching.)\\n\\nThe other way to do it would be the \\\"proper OO way\\\", to introduce the `CalculateValue` method on the base class, and override it with the actual implementation in the derived types.\\n\\n```csharp\\nabstract class Card\\n{\\n    public abstract int CalculateValue();\\n    ...\\n}\\n\\nclass FaceCard : Card\\n{\\n    public override int CalculateValue()\\n    {\\n        // Implementation for a face card.\\n        ...\\n    }\\n    ...\\n}\\n...\\n```\\n\\nThis supposed to be the textbook OO solution, however, it has a problem: with this approach we cannot achieve the goal we set out to deliver, namely to implement the data model as a self-contained unit (a separate library), on which the implementations of the various algorithms can depend. Because as we would introduce the implementation of more and more different card games, all of their logic would go into these *Card classes, thereby growing and growing them in size.  \\nThis is a manifestation of one of the general arguments against OO (or specifically inheritance), that as we introduce more and more features, due to encapsulation, our OO classes tend to grow, and become large and complicated.\\n\\nBecause of these reasons I think this OO approach is not really suitable to solve our problem. The puzzle pieces of OO seemingly fall in place nicely, but this approach actually causes more problems than what it solves.\\n\\n# Conclusion\\n\\nIn this post I tried to illustrate the challenges and tough decisions we usually face when designing a data model in an object oriented language. If you have any suggestions on how to further improve this implementation, feel free to leave it as a comment!\\n\\nIn the next post I'll look at how to solve the same problem in F#, and how can its type system eliminate some problems that are difficult to express in an OO language.\"}]],\"sections\":[[10,0]]}",
        "html": "<h1 id=\"introduction\">Introduction</h1>\n<p>When I'm learning a new programming language, I usually like to do some coding exercises to get familiar with the various language features, and to get used to the syntax.<br>\nMany of these exercisesor <em>katas</em>are about implementing some kind of algorithm, which is a great way to learn about the control structures of the language, the conditions, loops and functions.<br>\nOther katas are more focused on designing a data model for a certain domain, where the goal is to utilize the various features of the type system to create a model as expressive and intuitive as possible.</p>\n<p>Since I've been learning F#, I've been doing some data modeling exercises to learn what F# type system is capable of. And because I come from a C# background, I often compare my solutions to their C# counterpart, to be able to grasp the differences between the data models used in a functional and an object-oriented language.</p>\n<p>In this and the next post I'll take a look at a very simple data modelling exercise: creating a data type representing a card from the <a href=\"https://en.wikipedia.org/wiki/Standard_52-card_deck\">standard 52-card deck</a>. I found this kata simple enough to describe it in detail in a blog post, but it has enough quirks that we can get our teeth into it.<br>\nIn this first post I'll do it in C#, and in the next one I'm going to do the same in F#, where I'll try to contrast how the functional features of F# can solve problems that are cumbersome to express in OO languages.</p>\n<h1 id=\"thetask\">The task</h1>\n<p>The task I'd like to solve in this simple exercise is to design a data model to represent a card in the standard 52-card deck. For now I'm only interested in representing one single card, and not a full deck.</p>\n<p>We can specify a card with the following points.</p>\n<ul>\n<li>Every card has a suit (except the Joker), which can be clubs (), diamonds (<span style=\"color: red\"></span>), hearts (<span style=\"color: red\"></span>) and spades ().</li>\n<li>A card can be either</li>\n<li>a card with a number on it between 2 and 10, I'll call this a <em>value card</em>,</li>\n<li>or it can be one of the <em>face cards</em>: Jack, Queen, King or Ace.<br>\n<em>(Note: the terminology is not 100% unambiguous, some sources don't call the Ace a face card, but I'll consider it as one for the sake of this exercise.)</em></li>\n<li>There is a special card, the Joker, which does not have a suit.</li>\n</ul>\n<p><img src=\"/content/images/2017/05/cards.png\" alt=\"Image illustrating the standard 52-card deck.\"></p>\n<p>I would like to implement a data model, which potentially can be used by multiple different algorithms. More concretely, let's imagine that we deliver the data model in a self-contained package, and then we can implement the logic necessary to model various card games, which all depend on this single data model.<br>\nWhat this means in practice is that I would not like to mix data and logic in our data types, but rather just focus on the data. This might be different than what encapsulation in OO would suggest, but it's necessary if we want to have a data model which then can be used in several different algorithms (which is a typical practice in functional programming).</p>\n<h1 id=\"implementinginc\">Implementing in C#</h1>\n<p>When we start to implement this data model in C#, it seems intuitive that we'll probably need an enum for the suit of a card.</p>\n<pre><code class=\"language-csharp\">enum Suit\n{\n    Clubs,\n    Hearts,\n    Diamonds,\n    Spades\n}\n</code></pre>\n<p>Similarly, I'll creat an enum type for the different kind of face types.<br>\n(It's debatable whether Ace is considered a face card or not, in this model I'll assume that it is.)</p>\n<pre><code class=\"language-csharp\">enum Face\n{\n    Jack,\n    Queen,\n    King,\n    Ace\n}\n</code></pre>\n<p>With these in place I can create the actual type representing a card in the deck. I'll add a <code>Suit</code>, a <code>Face</code> and a <code>Value</code> property, saying that a card is either a face card or a value card, so that only one of those properties will have a value at any given time.<br>\nSince these are represented with value types, we have to make them nullable to be able to say they might not have a value. I also added a constructor.</p>\n<pre><code class=\"language-csharp\">class Card\n{\n    public Suit Suit { get; set; }\n\n    public Face? Face { get; set; }\n\n    public int? Value { get; set; }\n\n    public Card(Suit suit, Face? face, int? value)\n    {\n        Suit = suit;\n        Face = face;\n        Value = value;\n    }\n}\n</code></pre>\n<p>This type seems to be covering our requirements, since any card of the deck can be represented with an instance of it.</p>\n<h2 id=\"avoidinvalidstatesladdd\">Avoid invalid states ( la DDD)</h2>\n<p>We cannot be completely satisfied yet: this data model violates an important guideline of domain-driven design (and just generally an all-around good practice): <strong>Design our data model in a way that illegal states are not representable</strong>.<br>\nThis is beneficial for two main reasons.</p>\n<ul>\n<li>It helps avoiding bugs we would bump into due to invalid data.</li>\n<li>It makes implementing any sort of validation logic easier, since (at least some of) the validity of our data is immediately enforced by our data model.</li>\n</ul>\n<p>Our current model doesn't satisfy this requirement, since we're able to do this:</p>\n<pre><code class=\"language-csharp\">var card = new Card(Suit.Clubs, Face.Jack, 5);\n</code></pre>\n<p>Since this card instance will have both of its <code>Face</code> and <code>Value</code> property set, we will have no way deciding what it actually represent. We should not allow an instance like this to be created.</p>\n<p>This can be nicely solved by a simple C# pattern I like to call the <em>Factory method pattern</em>. (Note: you might find other sources using the same term to denote a slightly more complicated pattern.)<br>\nWe can make our constructor private, and introduce designated public methods (the <em>factory methods</em>) for creating instances of various kinds. Since the constructor is not accessible from the outside, we just have to make sure that our factory methods initialize the instance in a way that it represents a valid value.</p>\n<pre><code class=\"language-csharp\">class Card\n{\n    public Suit Suit { get; set; }\n\n    public Face? Face { get; set; }\n\n    public int? Value { get; set; }\n\n    private Card(Suit suit, Face? face, int? value)\n    {\n        Suit = suit;\n        Face = face;\n        Value = value;\n    }\n    \n    public static Card CreateFace(Suit suit, Face face)\n    {\n        return new Card(suit, face, null);\n    }\n    \n    public static Card CreateValue(Suit suit, int value)\n    {\n        return new Card(suit, null, value);\n    }\n}\n</code></pre>\n<p>Better, now we cannot use the constructor from outside, we have to use one of the two factory methods, thereby enforcing us to create only valid instances.</p>\n<pre><code class=\"language-csharp\">var card = Card.CreateFace(Suit.Clubs, Face.Jack);\n</code></pre>\n<p>I use this pattern all the time, not just to enforce some preconditions, but also to make code more self-documenting by introducing these expressive method names for object creation.</p>\n<h2 id=\"mutability\">Mutability</h2>\n<p>Of course we have a glaring problem: since our properties have public setters, there is nothing stopping us from creating a correct instance with the factory methods, but then mutate the instance afterwards to make it invalid.</p>\n<pre><code class=\"language-csharp\">var invalid = Card.CreateFace(Suit.Clubs, Face.Jack);\ninvalid.Value = 5;\n</code></pre>\n<p>This is easy to mitigate, just remove the setters to make our type immutable (which is beneficial to strive for anyway).</p>\n<pre><code class=\"language-csharp\">class Card\n{\n    public Suit Suit { get; }\n\n    public Face? Face { get; }\n\n    public int? Value { get; }\n\n    ...\n}\n</code></pre>\n<p>(This is something that I could've done immediately, but I wanted to illustrate the thought process that often happens during implementing a data model in C#, wherenot to be condescending, just based on my experiencenot necessarily every developer thinks about mutability consciously.)</p>\n<h2 id=\"finishingup\">Finishing up</h2>\n<p>When I first started implementing this data model, I initially forgot about the fact that we also have to support the Joker cards (this is not just for the sake of the example, I did actually forgot :)), this is the last thing we have to cover.</p>\n<p>We could say that if both <code>Face</code> and <code>Value</code> are <code>null</code>, we consier the card a Joker, but that feels a bit hacky, let's introduce a boolean instead.<br>\nThis is the final data model, covering all use cases.</p>\n<pre><code class=\"language-csharp\">class Card\n{\n    public Suit Suit { get; }\n\n    public Face? Face { get; }\n\n    public int? Value { get; }\n\n    public bool IsJoker { get; }\n\n    private Card(Suit suit, Face? face, int? value, bool isJoker)\n    {\n        Suit = suit;\n        Face = face;\n        Value = value;\n        IsJoker = isJoker;\n    }\n\n    public static Card CreateFace(Suit suit, Face face)\n    {\n        return new Card(suit, face, null, false);\n    }\n\n    public static Card CreateValue(Suit suit, int value)\n    {\n        return new Card(suit, null, value, false);\n    }\n\n    public static Card CreateJoker()\n    {\n        return new Card(default(Suit), null, null, true);\n    }\n}\n</code></pre>\n<p>I could've made the <code>Suit</code> property nullable too, but I didn't have to, since if <code>IsJoker</code> is true, we'll ignore the value of <code>Suit</code> anyway. However, this this is definitely debatable, it's one of those things where neither approach is obviously better than the other.</p>\n<h1 id=\"usage\">Usage</h1>\n<p>Let's see how we would use this in an application. As an example, implement the <a href=\"https://en.wikipedia.org/wiki/Rummy#Scoring\">score calculation</a> of the card game Rummy.</p>\n<pre><code class=\"language-csharp\">int CalculateValue(Card card)\n{\n    if(card.IsJoker)\n        return 0;\n        \n    if(card.Face.HasValue) // It's a face card\n    {\n        if(card.Face.Value == Face.Ace)\n            return 15;\n        \n        if(card.Face.Value == Face.Queen &amp;&amp; card.Suit == Suit.Spades)\n            return 40;\n        \n        return 10;\n    }\n    \n    // Now it has to be a value card\n    if(card.Value.Value == 10)\n        return 10;\n    \n    return 5;\n}\n</code></pre>\n<p>It works fine, although I'm not 100% happy with this syntax, it feels a bit awkward to do the <code>HasValue</code> check to find out what the object really represents.<br>\n<em>It feels to me that the type system does not help to express our domain, we had to do it ourselves</em>, we'll see in the next post how this is different in F#.</p>\n<h1 id=\"possibleimprovements\">Possible improvements</h1>\n<h2 id=\"checkthekindofthecardmoreconveniently\">Check the <em>kind</em> of the card more conveniently</h2>\n<p>In order to make usage a bit more convenient, we can introduce a new property in the <code>Card</code> type explicitly specifying the <em>kind</em> of our card. We can introduce a new enum for this purpose.</p>\n<pre><code class=\"language-csharp\">enum CardKind\n{\n    Value,\n    Face,\n    Joker\n}\n</code></pre>\n<p>Then implement the property returning the appropriate value.</p>\n<pre><code class=\"language-csharp\">class Card\n{\n    public CardKind Kind\n    {\n        get\n        {\n            if(IsJoker)\n                return CardKind.Joker;\n            if(Face.HasValue)\n                return CardKind.Face;\n            return CardKind.Value;\n        }\n    }\n    ...\n}\n</code></pre>\n<p>This way the actual usage of the type in any given algorithm can become a bit more expressive, we can phrase it like this.</p>\n<pre><code class=\"language-csharp\">int CalculateValue(Card card)\n{\n    switch (card.Kind)\n    {\n        case CardKind.Joker:\n            ...\n        case CardKind.Face:\n            ...\n        case CardKind.Value:\n            ...\n    }\n}\n</code></pre>\n<p>We can go one step further, andsince we are not using the <code>HasValue</code> property of our nullable types to determine the kind of the cardwe can change our properties to return a non-nullable value.<br>\nSo instead of</p>\n<pre><code class=\"language-csharp\">class Card\n{\n    public Face? Face { get; }\n    ...\n}\n</code></pre>\n<p>we can do something like</p>\n<pre><code class=\"language-csharp\">class Card\n{\n    private Face? face;\n    public Face Face\n    {\n         get\n         {\n             if(!face.HasValue)\n                throw new InvalidOperationException($&quot;You can only retrieve the Face property from a Face card. This is a {Kind} card.&quot;);\n             return face.Value;\n         }\n    }\n    ...\n}\n</code></pre>\n<p>This way in our logic using this type we can simply write <code>card.Face</code> instead of <code>card.Face.Value</code>.</p>\n<p>These changes improve the &quot;developer experience&quot; of working with this data model, but keep in mind that these all introduce more and more code we have to implement, thereby increasing the complexity of our data model, giving us more chance to make mistakes. So any improvement like these is always a tradeoff.</p>\n<h2 id=\"whataboutoo\">What about OO?</h2>\n<p>If we have learnt OO from a textbook, or at a university, we might have seen an introduction through examples like Hawk -&gt; Bird -&gt; Animal, or Square -&gt; Rectangle -&gt; Shape (and then later in the industry we probably heard many arguments against the validity of such examples, but let's put that aside for a moment :)).<br>\nNow if we look at our domain, it seems to fit the same pattern, so if OO dictates creating such inheritance trees, shouldn't we represent the various kinds of card as classes deriving from each other? We could do the following:</p>\n<pre><code class=\"language-csharp\">abstract class Card { ... }\n\nclass FaceCard : Card { ... }\n\nclass ValueCard : Card { ... }\n\nclass Joker : Card { ... }\n</code></pre>\n<p>We could definitely do this. My problem with this approach is twofold.<br>\nFirst, it is cumbersome to actually use this model in an algorithm. We could either do a switch-case on the type of our card.</p>\n<pre><code class=\"language-csharp\">int CalculateValue(Card card)\n{\n    switch (card)\n    {\n        case Joker j:\n            ...\n        case FaceCard f:\n            ...\n        case ValueCard v:\n            ...\n    }\n}\n</code></pre>\n<p>But normally this is considered an anti-pattern in object oriented programming. According to pure OO, if we have to switch case on the dynamic type of our object, we are doing something wrong. (Although this is not 100% clear, especially since the ability to do this conveniently has recently been introduced in C# in the form of pattern matching.)</p>\n<p>The other way to do it would be the &quot;proper OO way&quot;, to introduce the <code>CalculateValue</code> method on the base class, and override it with the actual implementation in the derived types.</p>\n<pre><code class=\"language-csharp\">abstract class Card\n{\n    public abstract int CalculateValue();\n    ...\n}\n\nclass FaceCard : Card\n{\n    public override int CalculateValue()\n    {\n        // Implementation for a face card.\n        ...\n    }\n    ...\n}\n...\n</code></pre>\n<p>This supposed to be the textbook OO solution, however, it has a problem: with this approach we cannot achieve the goal we set out to deliver, namely to implement the data model as a self-contained unit (a separate library), on which the implementations of the various algorithms can depend. Because as we would introduce the implementation of more and more different card games, all of their logic would go into these *Card classes, thereby growing and growing them in size.<br>\nThis is a manifestation of one of the general arguments against OO (or specifically inheritance), that as we introduce more and more features, due to encapsulation, our OO classes tend to grow, and become large and complicated.</p>\n<p>Because of these reasons I think this OO approach is not really suitable to solve our problem. The puzzle pieces of OO seemingly fall in place nicely, but this approach actually causes more problems than what it solves.</p>\n<h1 id=\"conclusion\">Conclusion</h1>\n<p>In this post I tried to illustrate the challenges and tough decisions we usually face when designing a data model in an object oriented language. If you have any suggestions on how to further improve this implementation, feel free to leave it as a comment!</p>\n<p>In the next post I'll look at how to solve the same problem in F#, and how can its type system eliminate some problems that are difficult to express in an OO language.</p>\n",
        "comment_id": "37",
        "plaintext": "Introduction\nWhen I'm learning a new programming language, I usually like to do some coding\nexercises to get familiar with the various language features, and to get used to\nthe syntax.\nMany of these exercisesor katasare about implementing some kind of algorithm,\nwhich is a great way to learn about the control structures of the language, the\nconditions, loops and functions.\nOther katas are more focused on designing a data model for a certain domain,\nwhere the goal is to utilize the various features of the type system to create a\nmodel as expressive and intuitive as possible.\n\nSince I've been learning F#, I've been doing some data modeling exercises to\nlearn what F# type system is capable of. And because I come from a C#\nbackground, I often compare my solutions to their C# counterpart, to be able to\ngrasp the differences between the data models used in a functional and an\nobject-oriented language.\n\nIn this and the next post I'll take a look at a very simple data modelling\nexercise: creating a data type representing a card from the standard 52-card\ndeck [https://en.wikipedia.org/wiki/Standard_52-card_deck]. I found this kata\nsimple enough to describe it in detail in a blog post, but it has enough quirks\nthat we can get our teeth into it.\nIn this first post I'll do it in C#, and in the next one I'm going to do the\nsame in F#, where I'll try to contrast how the functional features of F# can\nsolve problems that are cumbersome to express in OO languages.\n\nThe task\nThe task I'd like to solve in this simple exercise is to design a data model to\nrepresent a card in the standard 52-card deck. For now I'm only interested in\nrepresenting one single card, and not a full deck.\n\nWe can specify a card with the following points.\n\n * Every card has a suit (except the Joker), which can be clubs (), diamonds (\n   ), hearts () and spades ().\n * A card can be either\n * a card with a number on it between 2 and 10, I'll call this a value card,\n * or it can be one of the face cards: Jack, Queen, King or Ace.\n   (Note: the terminology is not 100% unambiguous, some sources don't call the\n   Ace a face card, but I'll consider it as one for the sake of this exercise.)\n * There is a special card, the Joker, which does not have a suit.\n\n\n\nI would like to implement a data model, which potentially can be used by\nmultiple different algorithms. More concretely, let's imagine that we deliver\nthe data model in a self-contained package, and then we can implement the logic\nnecessary to model various card games, which all depend on this single data\nmodel.\nWhat this means in practice is that I would not like to mix data and logic in\nour data types, but rather just focus on the data. This might be different than\nwhat encapsulation in OO would suggest, but it's necessary if we want to have a\ndata model which then can be used in several different algorithms (which is a\ntypical practice in functional programming).\n\nImplementing in C#\nWhen we start to implement this data model in C#, it seems intuitive that we'll\nprobably need an enum for the suit of a card.\n\nenum Suit\n{\n    Clubs,\n    Hearts,\n    Diamonds,\n    Spades\n}\n\n\nSimilarly, I'll creat an enum type for the different kind of face types.\n(It's debatable whether Ace is considered a face card or not, in this model I'll\nassume that it is.)\n\nenum Face\n{\n    Jack,\n    Queen,\n    King,\n    Ace\n}\n\n\nWith these in place I can create the actual type representing a card in the\ndeck. I'll add a Suit, a Face  and a Value  property, saying that a card is\neither a face card or a value card, so that only one of those properties will\nhave a value at any given time.\nSince these are represented with value types, we have to make them nullable to\nbe able to say they might not have a value. I also added a constructor.\n\nclass Card\n{\n    public Suit Suit { get; set; }\n\n    public Face? Face { get; set; }\n\n    public int? Value { get; set; }\n\n    public Card(Suit suit, Face? face, int? value)\n    {\n        Suit = suit;\n        Face = face;\n        Value = value;\n    }\n}\n\n\nThis type seems to be covering our requirements, since any card of the deck can\nbe represented with an instance of it.\n\nAvoid invalid states ( la DDD)\nWe cannot be completely satisfied yet: this data model violates an important\nguideline of domain-driven design (and just generally an all-around good\npractice): Design our data model in a way that illegal states are not\nrepresentable.\nThis is beneficial for two main reasons.\n\n * It helps avoiding bugs we would bump into due to invalid data.\n * It makes implementing any sort of validation logic easier, since (at least\n   some of) the validity of our data is immediately enforced by our data model.\n\nOur current model doesn't satisfy this requirement, since we're able to do this:\n\nvar card = new Card(Suit.Clubs, Face.Jack, 5);\n\n\nSince this card instance will have both of its Face  and Value  property set, we\nwill have no way deciding what it actually represent. We should not allow an\ninstance like this to be created.\n\nThis can be nicely solved by a simple C# pattern I like to call the Factory\nmethod pattern. (Note: you might find other sources using the same term to\ndenote a slightly more complicated pattern.)\nWe can make our constructor private, and introduce designated public methods\n(the factory methods) for creating instances of various kinds. Since the\nconstructor is not accessible from the outside, we just have to make sure that\nour factory methods initialize the instance in a way that it represents a valid\nvalue.\n\nclass Card\n{\n    public Suit Suit { get; set; }\n\n    public Face? Face { get; set; }\n\n    public int? Value { get; set; }\n\n    private Card(Suit suit, Face? face, int? value)\n    {\n        Suit = suit;\n        Face = face;\n        Value = value;\n    }\n    \n    public static Card CreateFace(Suit suit, Face face)\n    {\n        return new Card(suit, face, null);\n    }\n    \n    public static Card CreateValue(Suit suit, int value)\n    {\n        return new Card(suit, null, value);\n    }\n}\n\n\nBetter, now we cannot use the constructor from outside, we have to use one of\nthe two factory methods, thereby enforcing us to create only valid instances.\n\nvar card = Card.CreateFace(Suit.Clubs, Face.Jack);\n\n\nI use this pattern all the time, not just to enforce some preconditions, but\nalso to make code more self-documenting by introducing these expressive method\nnames for object creation.\n\nMutability\nOf course we have a glaring problem: since our properties have public setters,\nthere is nothing stopping us from creating a correct instance with the factory\nmethods, but then mutate the instance afterwards to make it invalid.\n\nvar invalid = Card.CreateFace(Suit.Clubs, Face.Jack);\ninvalid.Value = 5;\n\n\nThis is easy to mitigate, just remove the setters to make our type immutable\n(which is beneficial to strive for anyway).\n\nclass Card\n{\n    public Suit Suit { get; }\n\n    public Face? Face { get; }\n\n    public int? Value { get; }\n\n    ...\n}\n\n\n(This is something that I could've done immediately, but I wanted to illustrate\nthe thought process that often happens during implementing a data model in C#,\nwherenot to be condescending, just based on my experiencenot necessarily every\ndeveloper thinks about mutability consciously.)\n\nFinishing up\nWhen I first started implementing this data model, I initially forgot about the\nfact that we also have to support the Joker cards (this is not just for the sake\nof the example, I did actually forgot :)), this is the last thing we have to\ncover.\n\nWe could say that if both Face  and Value  are null, we consier the card a\nJoker, but that feels a bit hacky, let's introduce a boolean instead.\nThis is the final data model, covering all use cases.\n\nclass Card\n{\n    public Suit Suit { get; }\n\n    public Face? Face { get; }\n\n    public int? Value { get; }\n\n    public bool IsJoker { get; }\n\n    private Card(Suit suit, Face? face, int? value, bool isJoker)\n    {\n        Suit = suit;\n        Face = face;\n        Value = value;\n        IsJoker = isJoker;\n    }\n\n    public static Card CreateFace(Suit suit, Face face)\n    {\n        return new Card(suit, face, null, false);\n    }\n\n    public static Card CreateValue(Suit suit, int value)\n    {\n        return new Card(suit, null, value, false);\n    }\n\n    public static Card CreateJoker()\n    {\n        return new Card(default(Suit), null, null, true);\n    }\n}\n\n\nI could've made the Suit  property nullable too, but I didn't have to, since if \nIsJoker  is true, we'll ignore the value of Suit  anyway. However, this this is\ndefinitely debatable, it's one of those things where neither approach is\nobviously better than the other.\n\nUsage\nLet's see how we would use this in an application. As an example, implement the \nscore calculation  of the card game Rummy.\n\nint CalculateValue(Card card)\n{\n    if(card.IsJoker)\n        return 0;\n        \n    if(card.Face.HasValue) // It's a face card\n    {\n        if(card.Face.Value == Face.Ace)\n            return 15;\n        \n        if(card.Face.Value == Face.Queen && card.Suit == Suit.Spades)\n            return 40;\n        \n        return 10;\n    }\n    \n    // Now it has to be a value card\n    if(card.Value.Value == 10)\n        return 10;\n    \n    return 5;\n}\n\n\nIt works fine, although I'm not 100% happy with this syntax, it feels a bit\nawkward to do the HasValue  check to find out what the object really represents.\nIt feels to me that the type system does not help to express our domain, we had\nto do it ourselves, we'll see in the next post how this is different in F#.\n\nPossible improvements\nCheck the kind  of the card more conveniently\nIn order to make usage a bit more convenient, we can introduce a new property in\nthe Card  type explicitly specifying the kind  of our card. We can introduce a\nnew enum for this purpose.\n\nenum CardKind\n{\n    Value,\n    Face,\n    Joker\n}\n\n\nThen implement the property returning the appropriate value.\n\nclass Card\n{\n    public CardKind Kind\n    {\n        get\n        {\n            if(IsJoker)\n                return CardKind.Joker;\n            if(Face.HasValue)\n                return CardKind.Face;\n            return CardKind.Value;\n        }\n    }\n    ...\n}\n\n\nThis way the actual usage of the type in any given algorithm can become a bit\nmore expressive, we can phrase it like this.\n\nint CalculateValue(Card card)\n{\n    switch (card.Kind)\n    {\n        case CardKind.Joker:\n            ...\n        case CardKind.Face:\n            ...\n        case CardKind.Value:\n            ...\n    }\n}\n\n\nWe can go one step further, andsince we are not using the HasValue  property of\nour nullable types to determine the kind of the cardwe can change our\nproperties to return a non-nullable value.\nSo instead of\n\nclass Card\n{\n    public Face? Face { get; }\n    ...\n}\n\n\nwe can do something like\n\nclass Card\n{\n    private Face? face;\n    public Face Face\n    {\n         get\n         {\n             if(!face.HasValue)\n                throw new InvalidOperationException($\"You can only retrieve the Face property from a Face card. This is a {Kind} card.\");\n             return face.Value;\n         }\n    }\n    ...\n}\n\n\nThis way in our logic using this type we can simply write card.Face  instead of \ncard.Face.Value.\n\nThese changes improve the \"developer experience\" of working with this data\nmodel, but keep in mind that these all introduce more and more code we have to\nimplement, thereby increasing the complexity of our data model, giving us more\nchance to make mistakes. So any improvement like these is always a tradeoff.\n\nWhat about OO?\nIf we have learnt OO from a textbook, or at a university, we might have seen an\nintroduction through examples like Hawk -> Bird -> Animal, or Square ->\nRectangle -> Shape (and then later in the industry we probably heard many\narguments against the validity of such examples, but let's put that aside for a\nmoment :)).\nNow if we look at our domain, it seems to fit the same pattern, so if OO\ndictates creating such inheritance trees, shouldn't we represent the various\nkinds of card as classes deriving from each other? We could do the following:\n\nabstract class Card { ... }\n\nclass FaceCard : Card { ... }\n\nclass ValueCard : Card { ... }\n\nclass Joker : Card { ... }\n\n\nWe could definitely do this. My problem with this approach is twofold.\nFirst, it is cumbersome to actually use this model in an algorithm. We could\neither do a switch-case on the type of our card.\n\nint CalculateValue(Card card)\n{\n    switch (card)\n    {\n        case Joker j:\n            ...\n        case FaceCard f:\n            ...\n        case ValueCard v:\n            ...\n    }\n}\n\n\nBut normally this is considered an anti-pattern in object oriented programming.\nAccording to pure OO, if we have to switch case on the dynamic type of our\nobject, we are doing something wrong. (Although this is not 100% clear,\nespecially since the ability to do this conveniently has recently been\nintroduced in C# in the form of pattern matching.)\n\nThe other way to do it would be the \"proper OO way\", to introduce the \nCalculateValue  method on the base class, and override it with the actual\nimplementation in the derived types.\n\nabstract class Card\n{\n    public abstract int CalculateValue();\n    ...\n}\n\nclass FaceCard : Card\n{\n    public override int CalculateValue()\n    {\n        // Implementation for a face card.\n        ...\n    }\n    ...\n}\n...\n\n\nThis supposed to be the textbook OO solution, however, it has a problem: with\nthis approach we cannot achieve the goal we set out to deliver, namely to\nimplement the data model as a self-contained unit (a separate library), on which\nthe implementations of the various algorithms can depend. Because as we would\nintroduce the implementation of more and more different card games, all of their\nlogic would go into these *Card classes, thereby growing and growing them in\nsize.\nThis is a manifestation of one of the general arguments against OO (or\nspecifically inheritance), that as we introduce more and more features, due to\nencapsulation, our OO classes tend to grow, and become large and complicated.\n\nBecause of these reasons I think this OO approach is not really suitable to\nsolve our problem. The puzzle pieces of OO seemingly fall in place nicely, but\nthis approach actually causes more problems than what it solves.\n\nConclusion\nIn this post I tried to illustrate the challenges and tough decisions we usually\nface when designing a data model in an object oriented language. If you have any\nsuggestions on how to further improve this implementation, feel free to leave it\nas a comment!\n\nIn the next post I'll look at how to solve the same problem in F#, and how can\nits type system eliminate some problems that are difficult to express in an OO\nlanguage.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "A simple data model exercise illustrating some challenges we encounter when designing domain models in object oriented programming languages.",
        "author_id": "1",
        "created_at": "2017-05-04 07:06:11",
        "created_by": "1",
        "updated_at": "2017-08-01 18:23:16",
        "updated_by": "1",
        "published_at": "2017-05-04 19:29:25",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de297a",
        "uuid": "9ed455e9-24da-4cc7-99cc-42eacf35ebf9",
        "title": "Two gotchas with scoped and singleton dependencies in ASP.NET Core",
        "slug": "two-gotchas-with-scoped-and-singleton-dependencies-in-asp-net-core",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"With ASP.NET Core a new built-in lightweight Dependency Injection framework was introduced in the `Microsoft.Extensions.DependencyInjection` package, thus in ASP.NET Core applications we don't necessarily need an external library such as Ninject or Unity to do DI, we can simply use the built-in package (whichalthough being framework-agnosticplays really nicely with ASP.NET Core).\\nIts feature set is rather simple compared to other more full-blown DI frameworks, but it gets the job done in most applications.\\n\\nWhen we register our dependencies, we can choose from three different lifecycle settings.\\n\\n - **Transient**: A new instance of the dependency is going to be created upon every retrieval.\\n - **Scoped**: One instance of the dependency is going to be used per scope. In ASP.NET this means that one instance is going to be created per HTTP request. This comes handy if our class depends on some property of the `HttpContext`.\\n - **Singleton**: Only one single instance of the dependency is going to be created and used for all retrievals.\\n\\nI'll introduce two gotchas related to the lifecycle of our dependencies that we can run into, and describe how we can avoid them.\\n\\n# Depending on a scoped dependency from a singleton\\n\\nIf we have a DI graph in which classes with various lifecycles depending on each other, we can run into an issue, that might be tricky to troubleshoot if we don't know where to look for the problem.\\n\\nAssume the following setup.\\n\\nWe have an `IFoo` interface registered as a *singleton*, which depends on `IBar`, which is registered as *scoped* (for example because it depends on the current HTTP request).\\nAnd from our controller (which we can consider *scoped*, since it is created per request), we require `IFoo`.\\n\\n![Class diagram illustrating the erroneous setup.](/content/images/2017/04/aspnetcore_di_issue.png)\\n\\nThe problem with this setup is that the first time we retrieve `IFoo`, a new instance of `Foo` and transitively `Bar` is going to be created. Since `Foo` is singleton, no new instances of it are going to be created on further retrievals, the single instance created at first will always be retrieved.\\nBut this meanssince the reference to `IBar` is stored by the instance of `Foo`that the singleton `Foo` is going to \\\"capture\\\" the scoped `IBar`, so no new instance of `Bar` is going to be created either, in spite of it being registered as *scoped*.\\n\\nThis is a bug in our application, since if we retrieve `IFoo` in a subsequent request, we would need a new instance of `IBar` to be created, because it might depend on the context of the HTTP request.\\n\\n## Example\\n\\nLet's illustrate this with an example. We implement an api with MVC, and we want to log some messages, prefixing every log message with the path of the current request.\\nTo do this we define the following interface.\\n\\n```csharp\\npublic interface ISmartLogger\\n{\\n    void Log(string message);\\n}\\n```\\n\\nAnd write its implementation.\\n\\n```csharp\\npublic class SmartLogger : ISmartLogger\\n{\\n    private readonly string requestPath;\\n\\n    public SmartLogger(IHttpContextAccessor httpContextAccessor)\\n    {\\n        requestPath = httpContextAccessor?.HttpContext?.Request?.Path.ToString() ?? \\\"No path\\\";\\n    }\\n\\n    public void Log(string message)\\n    {\\n        Console.WriteLine(\\\"{0}: {1}\\\", requestPath, message);\\n    }\\n}\\n```\\n\\nThe implementation is very straightforward. We depend on `IHttpContextAccessor` to access the `HttpContext`, and we save the path of the request (if there is any).\\n(**Note**: of course we don't necessarily have to do this in the constructor, we could retrieve the path on the fly in the `Log` method, but the approach I've chosen is important to illustrate the issue.)\\n\\nLet's say we have a single controller, `PeopleController`, in which we depend on our `ISmartLogger`, and log some messages.\\n\\n```csharp\\n[Route(\\\"[controller]\\\")]\\npublic class PeopleController : Controller\\n{\\n    private readonly ISmartLogger smartLogger;\\n    \\n    public PeopleController(ISmartLogger smartLogger)\\n    {\\n        this.smartLogger = smartLogger;\\n    }\\n\\n    [HttpGet(\\\"person1\\\")]\\n    public string Person1()\\n    {\\n        smartLogger.Log(\\\"Retrieving person 1\\\");\\n        return \\\"Jane Smith\\\";\\n    }\\n\\n    [HttpGet(\\\"person2\\\")]\\n    public string Person2()\\n    {\\n        smartLogger.Log(\\\"Retrieving person 2\\\");\\n        return \\\"John Doe\\\";\\n    }\\n}\\n```\\n\\nThe last thing we have to do is to register our logger in `Startup.ConfigureService()`. Since we retrieve the request path in the constructor of our logger, we have to register the service as *scoped*, so that a new instance is created on every request.\\n\\n```csharp\\npublic void ConfigureServices(IServiceCollection services)\\n{\\n    services.AddMvc();\\n    services.AddSingleton<IHttpContextAccessor, HttpContextAccessor>();\\n    services.AddScoped<ISmartLogger, SmartLogger>();\\n}\\n```\\n\\nIf our start up the application and do a couple of requests:\\n\\n```bash\\n$ curl http://localhost:5000/people/person1\\nJane Smith\\n$ curl http://localhost:5000/people/person2\\nJohn Doe\\n$ curl http://localhost:5000/people/person1\\nJane Smith\\n```\\n\\nThen in the terminal of the web app we can see that our log messages correctly contain the request path.\\n\\n```bash\\n/people/person1: Retrieving person 1\\n/people/person2: Retrieving person 2\\n/people/person1: Retrieving person 1\\n```\\n\\nSo far everything is good. But let's say later we decide to extract the retrieval of the people to a separate service, called `PeopleService`.\\n\\n```csharp\\npublic class PeopleService : IPeopleService\\n{\\n    private readonly ISmartLogger smartLogger;\\n\\n    public PeopleService(ISmartLogger smartLogger)\\n    {\\n        this.smartLogger = smartLogger;\\n    }\\n\\n    public string GetPerson1()\\n    {\\n        smartLogger.Log(\\\"Retrieving person 1\\\");\\n        return \\\"Jane Smith\\\";\\n    }\\n\\n    public string GetPerson2()\\n    {\\n        smartLogger.Log(\\\"Retrieving person 2\\\");\\n        return \\\"John Doe\\\";\\n    }\\n}\\n```\\n\\nAnd we also change our controller to depend on this service.\\n\\n```csharp\\n[Route(\\\"[controller]\\\")]\\npublic class PeopleController : Controller\\n{\\n    private readonly IPeopleService peopleService;\\n\\n    public PeopleController(IPeopleService peopleService)\\n    {\\n        this.peopleService = peopleService;\\n    }\\n\\n    [HttpGet(\\\"person1\\\")]\\n    public string Person1()\\n    {\\n        return peopleService.GetPerson1();\\n    }\\n\\n    [HttpGet(\\\"person2\\\")]\\n    public string Person2()\\n    {\\n        return peopleService.GetPerson2();\\n    }\\n}\\n```\\n\\nNote that we moved the logging from the controller to the service.\\nTo make this work, we have to also register `IPeopleService` as a dependency.\\nSince `PeopleService` itself is not concerned about the current HTTP request at all, it seems to make sense to register it as a singleton. **This is wrong** and it causes a bug. Let's see what happens if we do this.\\n\\n```csharp\\npublic void ConfigureServices(IServiceCollection services)\\n{\\n    services.AddMvc();\\n    services.AddSingleton<IHttpContextAccessor, HttpContextAccessor>();\\n    services.AddScoped<ISmartLogger, SmartLogger>();\\n    services.AddSingleton<IPeopleService, PeopleService>();\\n}\\n```\\n\\nNow if we start the server and issue the same three requests.\\n\\n```bash\\n$ curl http://localhost:5000/people/person1\\nJane Smith\\n$ curl http://localhost:5000/people/person2\\nJohn Doe\\n$ curl http://localhost:5000/people/person1\\nJane Smith\\n```\\n\\nThen this is what we'll see in the terminal of the server.\\n\\n```bash\\n/people/person1: Retrieving person 1\\n/people/person1: Retrieving person 2\\n/people/person1: Retrieving person 1\\n```\\n\\nNotice that the request path in the second log message is incorrect. The reason for this is that `PeopleService` is a singleton instance, and it \\\"captures\\\" the `SmartLogger` instantiated for the first request, so on all subsequent `Log()` calls (done from `PeopleService`) till the end of time, the only request path we'll see in the logs is the one retrieved upon the first request, `/people/person1`.\\n\\nThe solution is simple, we just have to change the lifecycle of the `IPeopleService` dependency to be scoped (or transient would also work, but it would do more instantiations than necessary):\\n\\n```csharp\\nservices.AddScoped<IPeopleService, PeopleService>();\\n```\\nIn general, we must not depend on a transient or scoped dependency (either directly or transitively) from a singleton, and we must not depend on a transient dependency from a scoped object.\\n\\nIn a more complicated application it might seem troublesome to manually look through our whole DI graph to figure out where we might have made this mistake. Luckily in the 2.0 version of ASP.NET Core this is going to be validated by the DI framework, and we'll get an exception if we mess it up (this change cannot be done in the 1.* versions, since it'd be a breaking change).\\n\\n# Inject non-singleton dependencies into middlewares\\n\\nThe second issue I'd like to describe can happen when we try to inject a non-singleton dependency into a middleware. Let's see an example right away.\\n\\nWe implement a custom middleware, in which we want to use the previously implemented `SmartLogger`. We accept `ISmartLogger` as a constructor parameter.\\n\\n```csharp\\npublic class CustomMiddleware\\n{\\n    private readonly ISmartLogger smartLogger;\\n\\n    public CustomMiddleware(RequestDelegate next, ISmartLogger smartLogger)\\n    {\\n        this.smartLogger = smartLogger;\\n    }\\n\\n    public async Task Invoke(HttpContext context)\\n    {\\n        smartLogger.Log(\\\"Custom middleware called\\\");\\n        await context.Response.WriteAsync(\\\"Custom response\\\");\\n    }\\n}\\n```\\n\\nChange `Startup.Configure` to use this middleware instead of the MVC router.\\n\\n```csharp\\npublic void Configure(IApplicationBuilder app, IHostingEnvironment env, ILoggerFactory loggerFactory)\\n{\\n    app.UseMiddleware<CustomMiddleware>();\\n    // app.UseMvc();\\n}\\n```\\n\\nIf we start the server and send two test requests:\\n\\n```bash\\n$ curl http://localhost:5000/foo\\nCustom response\\n$ curl http://localhost:5000/bar\\nCustom response\\n```\\n\\nWe'll see the following output in the terminal of the server.\\n\\n```bash\\nNo path: Custom middleware called\\nNo path: Custom middleware called\\n```\\n\\nSomething is wrong, since our logger is not printing the path of the requests.\\nThe reason for this is that only one single instance of the middleware gets created, and it's instantiated when the pipeline is set up, prior to the first request, so the `HttpContext` is not even populated yet.\\n\\nSince only one instance of the middleware is created, the injection through the constructor is not going to work for dependencies which are not singletons.\\n\\nWe can easily fix this. We can accept dependencies not just in the constructor of the middleware, but also in the `Invoke` method, so we can fix the problem by modifying the class the following way.\\n\\n```csharp\\npublic class CustomMiddleware\\n{\\n    public CustomMiddleware(RequestDelegate next)\\n    {\\n    }\\n\\n    public async Task Invoke(HttpContext context, ISmartLogger smartLogger)\\n    {\\n        smartLogger.Log(\\\"Custom middleware called\\\");\\n        await context.Response.WriteAsync(\\\"Custom response\\\");\\n    }\\n}\\n```\\n\\nThese two problems are pretty easy to run into, but luckily they are not difficult to fix if you know what to look for. I hope this post we'll save you some time when troubleshooting these DI issues in ASP.NET Core.\"}]],\"sections\":[[10,0]]}",
        "html": "<p>With ASP.NET Core a new built-in lightweight Dependency Injection framework was introduced in the <code>Microsoft.Extensions.DependencyInjection</code> package, thus in ASP.NET Core applications we don't necessarily need an external library such as Ninject or Unity to do DI, we can simply use the built-in package (whichalthough being framework-agnosticplays really nicely with ASP.NET Core).<br>\nIts feature set is rather simple compared to other more full-blown DI frameworks, but it gets the job done in most applications.</p>\n<p>When we register our dependencies, we can choose from three different lifecycle settings.</p>\n<ul>\n<li><strong>Transient</strong>: A new instance of the dependency is going to be created upon every retrieval.</li>\n<li><strong>Scoped</strong>: One instance of the dependency is going to be used per scope. In ASP.NET this means that one instance is going to be created per HTTP request. This comes handy if our class depends on some property of the <code>HttpContext</code>.</li>\n<li><strong>Singleton</strong>: Only one single instance of the dependency is going to be created and used for all retrievals.</li>\n</ul>\n<p>I'll introduce two gotchas related to the lifecycle of our dependencies that we can run into, and describe how we can avoid them.</p>\n<h1 id=\"dependingonascopeddependencyfromasingleton\">Depending on a scoped dependency from a singleton</h1>\n<p>If we have a DI graph in which classes with various lifecycles depending on each other, we can run into an issue, that might be tricky to troubleshoot if we don't know where to look for the problem.</p>\n<p>Assume the following setup.</p>\n<p>We have an <code>IFoo</code> interface registered as a <em>singleton</em>, which depends on <code>IBar</code>, which is registered as <em>scoped</em> (for example because it depends on the current HTTP request).<br>\nAnd from our controller (which we can consider <em>scoped</em>, since it is created per request), we require <code>IFoo</code>.</p>\n<p><img src=\"/content/images/2017/04/aspnetcore_di_issue.png\" alt=\"Class diagram illustrating the erroneous setup.\"></p>\n<p>The problem with this setup is that the first time we retrieve <code>IFoo</code>, a new instance of <code>Foo</code> and transitively <code>Bar</code> is going to be created. Since <code>Foo</code> is singleton, no new instances of it are going to be created on further retrievals, the single instance created at first will always be retrieved.<br>\nBut this meanssince the reference to <code>IBar</code> is stored by the instance of <code>Foo</code>that the singleton <code>Foo</code> is going to &quot;capture&quot; the scoped <code>IBar</code>, so no new instance of <code>Bar</code> is going to be created either, in spite of it being registered as <em>scoped</em>.</p>\n<p>This is a bug in our application, since if we retrieve <code>IFoo</code> in a subsequent request, we would need a new instance of <code>IBar</code> to be created, because it might depend on the context of the HTTP request.</p>\n<h2 id=\"example\">Example</h2>\n<p>Let's illustrate this with an example. We implement an api with MVC, and we want to log some messages, prefixing every log message with the path of the current request.<br>\nTo do this we define the following interface.</p>\n<pre><code class=\"language-csharp\">public interface ISmartLogger\n{\n    void Log(string message);\n}\n</code></pre>\n<p>And write its implementation.</p>\n<pre><code class=\"language-csharp\">public class SmartLogger : ISmartLogger\n{\n    private readonly string requestPath;\n\n    public SmartLogger(IHttpContextAccessor httpContextAccessor)\n    {\n        requestPath = httpContextAccessor?.HttpContext?.Request?.Path.ToString() ?? &quot;No path&quot;;\n    }\n\n    public void Log(string message)\n    {\n        Console.WriteLine(&quot;{0}: {1}&quot;, requestPath, message);\n    }\n}\n</code></pre>\n<p>The implementation is very straightforward. We depend on <code>IHttpContextAccessor</code> to access the <code>HttpContext</code>, and we save the path of the request (if there is any).<br>\n(<strong>Note</strong>: of course we don't necessarily have to do this in the constructor, we could retrieve the path on the fly in the <code>Log</code> method, but the approach I've chosen is important to illustrate the issue.)</p>\n<p>Let's say we have a single controller, <code>PeopleController</code>, in which we depend on our <code>ISmartLogger</code>, and log some messages.</p>\n<pre><code class=\"language-csharp\">[Route(&quot;[controller]&quot;)]\npublic class PeopleController : Controller\n{\n    private readonly ISmartLogger smartLogger;\n    \n    public PeopleController(ISmartLogger smartLogger)\n    {\n        this.smartLogger = smartLogger;\n    }\n\n    [HttpGet(&quot;person1&quot;)]\n    public string Person1()\n    {\n        smartLogger.Log(&quot;Retrieving person 1&quot;);\n        return &quot;Jane Smith&quot;;\n    }\n\n    [HttpGet(&quot;person2&quot;)]\n    public string Person2()\n    {\n        smartLogger.Log(&quot;Retrieving person 2&quot;);\n        return &quot;John Doe&quot;;\n    }\n}\n</code></pre>\n<p>The last thing we have to do is to register our logger in <code>Startup.ConfigureService()</code>. Since we retrieve the request path in the constructor of our logger, we have to register the service as <em>scoped</em>, so that a new instance is created on every request.</p>\n<pre><code class=\"language-csharp\">public void ConfigureServices(IServiceCollection services)\n{\n    services.AddMvc();\n    services.AddSingleton&lt;IHttpContextAccessor, HttpContextAccessor&gt;();\n    services.AddScoped&lt;ISmartLogger, SmartLogger&gt;();\n}\n</code></pre>\n<p>If our start up the application and do a couple of requests:</p>\n<pre><code class=\"language-bash\">$ curl http://localhost:5000/people/person1\nJane Smith\n$ curl http://localhost:5000/people/person2\nJohn Doe\n$ curl http://localhost:5000/people/person1\nJane Smith\n</code></pre>\n<p>Then in the terminal of the web app we can see that our log messages correctly contain the request path.</p>\n<pre><code class=\"language-bash\">/people/person1: Retrieving person 1\n/people/person2: Retrieving person 2\n/people/person1: Retrieving person 1\n</code></pre>\n<p>So far everything is good. But let's say later we decide to extract the retrieval of the people to a separate service, called <code>PeopleService</code>.</p>\n<pre><code class=\"language-csharp\">public class PeopleService : IPeopleService\n{\n    private readonly ISmartLogger smartLogger;\n\n    public PeopleService(ISmartLogger smartLogger)\n    {\n        this.smartLogger = smartLogger;\n    }\n\n    public string GetPerson1()\n    {\n        smartLogger.Log(&quot;Retrieving person 1&quot;);\n        return &quot;Jane Smith&quot;;\n    }\n\n    public string GetPerson2()\n    {\n        smartLogger.Log(&quot;Retrieving person 2&quot;);\n        return &quot;John Doe&quot;;\n    }\n}\n</code></pre>\n<p>And we also change our controller to depend on this service.</p>\n<pre><code class=\"language-csharp\">[Route(&quot;[controller]&quot;)]\npublic class PeopleController : Controller\n{\n    private readonly IPeopleService peopleService;\n\n    public PeopleController(IPeopleService peopleService)\n    {\n        this.peopleService = peopleService;\n    }\n\n    [HttpGet(&quot;person1&quot;)]\n    public string Person1()\n    {\n        return peopleService.GetPerson1();\n    }\n\n    [HttpGet(&quot;person2&quot;)]\n    public string Person2()\n    {\n        return peopleService.GetPerson2();\n    }\n}\n</code></pre>\n<p>Note that we moved the logging from the controller to the service.<br>\nTo make this work, we have to also register <code>IPeopleService</code> as a dependency.<br>\nSince <code>PeopleService</code> itself is not concerned about the current HTTP request at all, it seems to make sense to register it as a singleton. <strong>This is wrong</strong> and it causes a bug. Let's see what happens if we do this.</p>\n<pre><code class=\"language-csharp\">public void ConfigureServices(IServiceCollection services)\n{\n    services.AddMvc();\n    services.AddSingleton&lt;IHttpContextAccessor, HttpContextAccessor&gt;();\n    services.AddScoped&lt;ISmartLogger, SmartLogger&gt;();\n    services.AddSingleton&lt;IPeopleService, PeopleService&gt;();\n}\n</code></pre>\n<p>Now if we start the server and issue the same three requests.</p>\n<pre><code class=\"language-bash\">$ curl http://localhost:5000/people/person1\nJane Smith\n$ curl http://localhost:5000/people/person2\nJohn Doe\n$ curl http://localhost:5000/people/person1\nJane Smith\n</code></pre>\n<p>Then this is what we'll see in the terminal of the server.</p>\n<pre><code class=\"language-bash\">/people/person1: Retrieving person 1\n/people/person1: Retrieving person 2\n/people/person1: Retrieving person 1\n</code></pre>\n<p>Notice that the request path in the second log message is incorrect. The reason for this is that <code>PeopleService</code> is a singleton instance, and it &quot;captures&quot; the <code>SmartLogger</code> instantiated for the first request, so on all subsequent <code>Log()</code> calls (done from <code>PeopleService</code>) till the end of time, the only request path we'll see in the logs is the one retrieved upon the first request, <code>/people/person1</code>.</p>\n<p>The solution is simple, we just have to change the lifecycle of the <code>IPeopleService</code> dependency to be scoped (or transient would also work, but it would do more instantiations than necessary):</p>\n<pre><code class=\"language-csharp\">services.AddScoped&lt;IPeopleService, PeopleService&gt;();\n</code></pre>\n<p>In general, we must not depend on a transient or scoped dependency (either directly or transitively) from a singleton, and we must not depend on a transient dependency from a scoped object.</p>\n<p>In a more complicated application it might seem troublesome to manually look through our whole DI graph to figure out where we might have made this mistake. Luckily in the 2.0 version of ASP.NET Core this is going to be validated by the DI framework, and we'll get an exception if we mess it up (this change cannot be done in the 1.* versions, since it'd be a breaking change).</p>\n<h1 id=\"injectnonsingletondependenciesintomiddlewares\">Inject non-singleton dependencies into middlewares</h1>\n<p>The second issue I'd like to describe can happen when we try to inject a non-singleton dependency into a middleware. Let's see an example right away.</p>\n<p>We implement a custom middleware, in which we want to use the previously implemented <code>SmartLogger</code>. We accept <code>ISmartLogger</code> as a constructor parameter.</p>\n<pre><code class=\"language-csharp\">public class CustomMiddleware\n{\n    private readonly ISmartLogger smartLogger;\n\n    public CustomMiddleware(RequestDelegate next, ISmartLogger smartLogger)\n    {\n        this.smartLogger = smartLogger;\n    }\n\n    public async Task Invoke(HttpContext context)\n    {\n        smartLogger.Log(&quot;Custom middleware called&quot;);\n        await context.Response.WriteAsync(&quot;Custom response&quot;);\n    }\n}\n</code></pre>\n<p>Change <code>Startup.Configure</code> to use this middleware instead of the MVC router.</p>\n<pre><code class=\"language-csharp\">public void Configure(IApplicationBuilder app, IHostingEnvironment env, ILoggerFactory loggerFactory)\n{\n    app.UseMiddleware&lt;CustomMiddleware&gt;();\n    // app.UseMvc();\n}\n</code></pre>\n<p>If we start the server and send two test requests:</p>\n<pre><code class=\"language-bash\">$ curl http://localhost:5000/foo\nCustom response\n$ curl http://localhost:5000/bar\nCustom response\n</code></pre>\n<p>We'll see the following output in the terminal of the server.</p>\n<pre><code class=\"language-bash\">No path: Custom middleware called\nNo path: Custom middleware called\n</code></pre>\n<p>Something is wrong, since our logger is not printing the path of the requests.<br>\nThe reason for this is that only one single instance of the middleware gets created, and it's instantiated when the pipeline is set up, prior to the first request, so the <code>HttpContext</code> is not even populated yet.</p>\n<p>Since only one instance of the middleware is created, the injection through the constructor is not going to work for dependencies which are not singletons.</p>\n<p>We can easily fix this. We can accept dependencies not just in the constructor of the middleware, but also in the <code>Invoke</code> method, so we can fix the problem by modifying the class the following way.</p>\n<pre><code class=\"language-csharp\">public class CustomMiddleware\n{\n    public CustomMiddleware(RequestDelegate next)\n    {\n    }\n\n    public async Task Invoke(HttpContext context, ISmartLogger smartLogger)\n    {\n        smartLogger.Log(&quot;Custom middleware called&quot;);\n        await context.Response.WriteAsync(&quot;Custom response&quot;);\n    }\n}\n</code></pre>\n<p>These two problems are pretty easy to run into, but luckily they are not difficult to fix if you know what to look for. I hope this post we'll save you some time when troubleshooting these DI issues in ASP.NET Core.</p>\n",
        "comment_id": "35",
        "plaintext": "With ASP.NET Core a new built-in lightweight Dependency Injection framework was\nintroduced in the Microsoft.Extensions.DependencyInjection  package, thus in\nASP.NET Core applications we don't necessarily need an external library such as\nNinject or Unity to do DI, we can simply use the built-in package\n(whichalthough being framework-agnosticplays really nicely with ASP.NET Core).\nIts feature set is rather simple compared to other more full-blown DI\nframeworks, but it gets the job done in most applications.\n\nWhen we register our dependencies, we can choose from three different lifecycle\nsettings.\n\n * Transient: A new instance of the dependency is going to be created upon every\n   retrieval.\n * Scoped: One instance of the dependency is going to be used per scope. In\n   ASP.NET this means that one instance is going to be created per HTTP request.\n   This comes handy if our class depends on some property of the HttpContext.\n * Singleton: Only one single instance of the dependency is going to be created\n   and used for all retrievals.\n\nI'll introduce two gotchas related to the lifecycle of our dependencies that we\ncan run into, and describe how we can avoid them.\n\nDepending on a scoped dependency from a singleton\nIf we have a DI graph in which classes with various lifecycles depending on each\nother, we can run into an issue, that might be tricky to troubleshoot if we\ndon't know where to look for the problem.\n\nAssume the following setup.\n\nWe have an IFoo  interface registered as a singleton, which depends on IBar,\nwhich is registered as scoped  (for example because it depends on the current\nHTTP request).\nAnd from our controller (which we can consider scoped, since it is created per\nrequest), we require IFoo.\n\n\n\nThe problem with this setup is that the first time we retrieve IFoo, a new\ninstance of Foo  and transitively Bar  is going to be created. Since Foo  is\nsingleton, no new instances of it are going to be created on further retrievals,\nthe single instance created at first will always be retrieved.\nBut this meanssince the reference to IBar  is stored by the instance of Foo\nthat the singleton Foo  is going to \"capture\" the scoped IBar, so no new\ninstance of Bar  is going to be created either, in spite of it being registered\nas scoped.\n\nThis is a bug in our application, since if we retrieve IFoo  in a subsequent\nrequest, we would need a new instance of IBar  to be created, because it might\ndepend on the context of the HTTP request.\n\nExample\nLet's illustrate this with an example. We implement an api with MVC, and we want\nto log some messages, prefixing every log message with the path of the current\nrequest.\nTo do this we define the following interface.\n\npublic interface ISmartLogger\n{\n    void Log(string message);\n}\n\n\nAnd write its implementation.\n\npublic class SmartLogger : ISmartLogger\n{\n    private readonly string requestPath;\n\n    public SmartLogger(IHttpContextAccessor httpContextAccessor)\n    {\n        requestPath = httpContextAccessor?.HttpContext?.Request?.Path.ToString() ?? \"No path\";\n    }\n\n    public void Log(string message)\n    {\n        Console.WriteLine(\"{0}: {1}\", requestPath, message);\n    }\n}\n\n\nThe implementation is very straightforward. We depend on IHttpContextAccessor \nto access the HttpContext, and we save the path of the request (if there is\nany).\n(Note: of course we don't necessarily have to do this in the constructor, we\ncould retrieve the path on the fly in the Log  method, but the approach I've\nchosen is important to illustrate the issue.)\n\nLet's say we have a single controller, PeopleController, in which we depend on\nour ISmartLogger, and log some messages.\n\n[Route(\"[controller]\")]\npublic class PeopleController : Controller\n{\n    private readonly ISmartLogger smartLogger;\n    \n    public PeopleController(ISmartLogger smartLogger)\n    {\n        this.smartLogger = smartLogger;\n    }\n\n    [HttpGet(\"person1\")]\n    public string Person1()\n    {\n        smartLogger.Log(\"Retrieving person 1\");\n        return \"Jane Smith\";\n    }\n\n    [HttpGet(\"person2\")]\n    public string Person2()\n    {\n        smartLogger.Log(\"Retrieving person 2\");\n        return \"John Doe\";\n    }\n}\n\n\nThe last thing we have to do is to register our logger in \nStartup.ConfigureService(). Since we retrieve the request path in the\nconstructor of our logger, we have to register the service as scoped, so that a\nnew instance is created on every request.\n\npublic void ConfigureServices(IServiceCollection services)\n{\n    services.AddMvc();\n    services.AddSingleton<IHttpContextAccessor, HttpContextAccessor>();\n    services.AddScoped<ISmartLogger, SmartLogger>();\n}\n\n\nIf our start up the application and do a couple of requests:\n\n$ curl http://localhost:5000/people/person1\nJane Smith\n$ curl http://localhost:5000/people/person2\nJohn Doe\n$ curl http://localhost:5000/people/person1\nJane Smith\n\n\nThen in the terminal of the web app we can see that our log messages correctly\ncontain the request path.\n\n/people/person1: Retrieving person 1\n/people/person2: Retrieving person 2\n/people/person1: Retrieving person 1\n\n\nSo far everything is good. But let's say later we decide to extract the\nretrieval of the people to a separate service, called PeopleService.\n\npublic class PeopleService : IPeopleService\n{\n    private readonly ISmartLogger smartLogger;\n\n    public PeopleService(ISmartLogger smartLogger)\n    {\n        this.smartLogger = smartLogger;\n    }\n\n    public string GetPerson1()\n    {\n        smartLogger.Log(\"Retrieving person 1\");\n        return \"Jane Smith\";\n    }\n\n    public string GetPerson2()\n    {\n        smartLogger.Log(\"Retrieving person 2\");\n        return \"John Doe\";\n    }\n}\n\n\nAnd we also change our controller to depend on this service.\n\n[Route(\"[controller]\")]\npublic class PeopleController : Controller\n{\n    private readonly IPeopleService peopleService;\n\n    public PeopleController(IPeopleService peopleService)\n    {\n        this.peopleService = peopleService;\n    }\n\n    [HttpGet(\"person1\")]\n    public string Person1()\n    {\n        return peopleService.GetPerson1();\n    }\n\n    [HttpGet(\"person2\")]\n    public string Person2()\n    {\n        return peopleService.GetPerson2();\n    }\n}\n\n\nNote that we moved the logging from the controller to the service.\nTo make this work, we have to also register IPeopleService  as a dependency.\nSince PeopleService  itself is not concerned about the current HTTP request at\nall, it seems to make sense to register it as a singleton. This is wrong  and it\ncauses a bug. Let's see what happens if we do this.\n\npublic void ConfigureServices(IServiceCollection services)\n{\n    services.AddMvc();\n    services.AddSingleton<IHttpContextAccessor, HttpContextAccessor>();\n    services.AddScoped<ISmartLogger, SmartLogger>();\n    services.AddSingleton<IPeopleService, PeopleService>();\n}\n\n\nNow if we start the server and issue the same three requests.\n\n$ curl http://localhost:5000/people/person1\nJane Smith\n$ curl http://localhost:5000/people/person2\nJohn Doe\n$ curl http://localhost:5000/people/person1\nJane Smith\n\n\nThen this is what we'll see in the terminal of the server.\n\n/people/person1: Retrieving person 1\n/people/person1: Retrieving person 2\n/people/person1: Retrieving person 1\n\n\nNotice that the request path in the second log message is incorrect. The reason\nfor this is that PeopleService  is a singleton instance, and it \"captures\" the \nSmartLogger  instantiated for the first request, so on all subsequent Log() \ncalls (done from PeopleService) till the end of time, the only request path\nwe'll see in the logs is the one retrieved upon the first request, \n/people/person1.\n\nThe solution is simple, we just have to change the lifecycle of the \nIPeopleService  dependency to be scoped (or transient would also work, but it\nwould do more instantiations than necessary):\n\nservices.AddScoped<IPeopleService, PeopleService>();\n\n\nIn general, we must not depend on a transient or scoped dependency (either\ndirectly or transitively) from a singleton, and we must not depend on a\ntransient dependency from a scoped object.\n\nIn a more complicated application it might seem troublesome to manually look\nthrough our whole DI graph to figure out where we might have made this mistake.\nLuckily in the 2.0 version of ASP.NET Core this is going to be validated by the\nDI framework, and we'll get an exception if we mess it up (this change cannot be\ndone in the 1.* versions, since it'd be a breaking change).\n\nInject non-singleton dependencies into middlewares\nThe second issue I'd like to describe can happen when we try to inject a\nnon-singleton dependency into a middleware. Let's see an example right away.\n\nWe implement a custom middleware, in which we want to use the previously\nimplemented SmartLogger. We accept ISmartLogger  as a constructor parameter.\n\npublic class CustomMiddleware\n{\n    private readonly ISmartLogger smartLogger;\n\n    public CustomMiddleware(RequestDelegate next, ISmartLogger smartLogger)\n    {\n        this.smartLogger = smartLogger;\n    }\n\n    public async Task Invoke(HttpContext context)\n    {\n        smartLogger.Log(\"Custom middleware called\");\n        await context.Response.WriteAsync(\"Custom response\");\n    }\n}\n\n\nChange Startup.Configure  to use this middleware instead of the MVC router.\n\npublic void Configure(IApplicationBuilder app, IHostingEnvironment env, ILoggerFactory loggerFactory)\n{\n    app.UseMiddleware<CustomMiddleware>();\n    // app.UseMvc();\n}\n\n\nIf we start the server and send two test requests:\n\n$ curl http://localhost:5000/foo\nCustom response\n$ curl http://localhost:5000/bar\nCustom response\n\n\nWe'll see the following output in the terminal of the server.\n\nNo path: Custom middleware called\nNo path: Custom middleware called\n\n\nSomething is wrong, since our logger is not printing the path of the requests.\nThe reason for this is that only one single instance of the middleware gets\ncreated, and it's instantiated when the pipeline is set up, prior to the first\nrequest, so the HttpContext  is not even populated yet.\n\nSince only one instance of the middleware is created, the injection through the\nconstructor is not going to work for dependencies which are not singletons.\n\nWe can easily fix this. We can accept dependencies not just in the constructor\nof the middleware, but also in the Invoke  method, so we can fix the problem by\nmodifying the class the following way.\n\npublic class CustomMiddleware\n{\n    public CustomMiddleware(RequestDelegate next)\n    {\n    }\n\n    public async Task Invoke(HttpContext context, ISmartLogger smartLogger)\n    {\n        smartLogger.Log(\"Custom middleware called\");\n        await context.Response.WriteAsync(\"Custom response\");\n    }\n}\n\n\nThese two problems are pretty easy to run into, but luckily they are not\ndifficult to fix if you know what to look for. I hope this post we'll save you\nsome time when troubleshooting these DI issues in ASP.NET Core.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "Two possible problems (and their solutions) we can run into when registering objects with various lifecycles with the DI container of ASP.NET Core.",
        "author_id": "1",
        "created_at": "2017-04-17 13:57:45",
        "created_by": "1",
        "updated_at": "2017-04-17 14:31:27",
        "updated_by": "1",
        "published_at": "2017-04-17 14:31:27",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de297b",
        "uuid": "23afbc7a-0955-4d0b-880b-147c9ab446b2",
        "title": "Bulk updating document expiry in Couchbase",
        "slug": "bulk-updating-document-expiry-in-couchbase",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"# Introduction\\n\\nWhen using Couchbase, sometimes we want to update the expiry of all of the documents stored in a bucket.\\nIn my case it was in a project where initially we weren't sure how we want to handle document expiry (either periodically run some query on our data store, and backup and remove all the old elementsor use the expiry mechanism built into Couchbase), so we ended up deciding we'll just insert all our documents without expiry for now, and we'll figure out later how we want to handle the problem.\\nAnd granted, before we knew it, we had 17 millions documents without expiry in our bucket. Oops.\\n\\nAt that point we decided we want to use Couchbase's built-in expiry mechanism, so we'll set the `Expiry` property on every new document we store.\\n\\nAnd we realized that we also retroactively have to set the expiry of all our existing documents too. This turned out to be not exactly trivial. In this post I'll describe how I ended up doing it.\\n\\n(I was using the .NET SDK, so that's what you'll see in the code samples, but the approach described here can be used with any of the other SDKs, or even with the CLI.)\\n\\n# The approach\\n\\nI started googling to see how we can update the expiry for all documents in a bucket, and quickly found out that Couchbase has no built-in operation for updating the expiry in bulk.\\nBased on my investigation the best option was to simply query over all the document ids in our bucket, and update the expiry of them one by one.\\n\\n# Query all the keys in a bucket\\n\\nIn order to be able to iterate over all our documents, we have to be able to query the keys in a bucket, but that's not completely trivial either. Unlike SQL, we can't just iterate over all the elements by default, we have to utilize an index or a view to do that.\\nOne way to achieve this is to create a simple view, which retrieves the ids of our documents.\\n\\n## Create the view\\n\\nCreating the necessary view is really simple, we can do it from the Couchbase management console.\\nOn the \\\"Data Buckets\\\" screen click on the Views button next to our bucket.\\n\\n![The Views button on the Couchbase management console.](/content/images/2017/03/viewbutton.png)\\n\\nThen click on \\\"Create Development View\\\", and give it a name. I used the name \\\"all_keys\\\".\\n\\n![Creating a new view.](/content/images/2017/03/createview.png)\\n\\nOn the View screen we can customize our query. Actually, the query generated by default is suitable for us, because it already returns our document keys.\\nThe only thing I changed in the query was that I removed even emitting the `meta.id`, because the keys are already return by the view without this, so removing this will make our downloaded view file slightly smaller. This is the view I used.\\n\\n```javascript\\nfunction (doc, meta) {\\n  emit(null, null);\\n}\\n```\\n\\nWe can test our view by clicking on the \\\"Show Results\\\" button.\\nAfter this, go back to the \\\"Views\\\" page and click on the \\\"Publish\\\" button.\\n\\n![Publish the view.](/content/images/2017/03/publishview.png)\\n\\nBy doing this we \\\"promote\\\" the view to production, and it's gonna query our whole bucket instead of just a small portion for development. Then we can switch to the \\\"Production\\\" tab and open our published view and test it.\\n\\n## Querying the view\\n\\nThe simplest way to query a view is to call the HTTP endpoint Couchbase is providing for our view. If we go to the page of the View, we can find an example of this URL at the bottom of the page.\\n\\n![The URL of the query endpoint can be found on the View page.](/content/images/2017/03/viewurl.png)\\n\\n**Gotcha**: make sure to get the url from the *production* view, and not the *development* one, since the latter will only contain a portion of our data.\\n\\nThis is an example of the query url:\\n\\n```\\nhttp://mycouchbaseserver:8092/mybucket/_design/all_keys/_view/all_keys?connection_timeout=60000&inclusive_end=true&limit=6&skip=0&stale=false\\n```\\n\\nWhere the `limit` and `skip` arguments specify the portion of the view we want to retrieve.\\n\\n(**Querying the view through the SDK**: We can also query the view with the SDK, for example in C# we can use the `bucket.CreateQuery()` method, but to simply get all the keys I think it's easier to just simply use the HTTP endpoint.)\\n\\n## Processing the data: a failed attempt\\n\\nAt first it seemed to be a reasonable approach to simply query the view in batches (let's say 1000 documents at a time), and then update the expiry for each of them with the SDK (for example in the .NET SDK we can update the expiry with the `Touch()` method).\\n\\nThis is a pseudo-code implementation of this approach:\\n\\n```csharp\\nvar baseUrl = \\\"http://mycouchbaseserver:8092/mybucket/_design/all_keys/_view/all_keys?connection_timeout=60000&stale=false&inclusive_end=true\\\";\\nvar batchSize = 100;\\nvar cnt = 0;\\nvar expiry = TimeSpan.FromDays(10);\\n\\nwhile(true)\\n{\\n    var documents = DownloadDocuments(baseUrl + $\\\"&limit={batchSize}&skip={cnt*batchSize}\\\");\\n\\n    foreach (var doc in documents)\\n    {\\n        bucket.Touch(doc, expiry);\\n    }\\n    \\n    if (documents.Count < batchSize)\\n    {\\n        break;\\n    }\\n    \\n    cnt++;\\n}\\n```\\n\\nThe code seems to make sense, and it works too. The problem is its performance.\\nIt turns out that for some reason, executing the query takes more and more time as the number of skipped document increases (as we get further and further through our documents).\\n\\nIn my case, with 17 million elements the response time of the query was the following:\\n\\n - Querying 1000 elements at the beginning (`&skip=0&limit=1000`): under 200ms\\n - Querying 1000 elements starting from 500,000 (`&skip=500000&limit=1000`): 5 seconds\\n - Querying 1000 elements starting from 10,000,000 (`&skip=10000000&limit=1000`): 2 and a half minutes\\n\\nSo this approach was not working, as my script was progressing through the bucket, it got slower and slower, after a couple of hundred thoursand documents it was barely doing any progress.\\n\\nI investigated quite a lot about the cause of this. I suspect it's caused by not having some necessary index to be able to efficiently query any portion of my view (I also asked the question [here](https://forums.couchbase.com/t/how-do-i-get-all-keys-from-the-bucket/35/9?u=markvincze)).\\nYet, however I tried to set up primary or secondary indices, I couldn't speed the query up.\\n\\nThen I had a different idea.\\n\\n## Brute force!\\n\\nI had 17 million documents in the bucket, which sounds a lot, but actually my view is only returning the keys and nothing else, the output looks like this (depending on your keys of course, which are GUIDs in my case):\\n\\n```json\\n{\\\"id\\\":\\\"000001ba-c4c9-4691-9c48-ca4b0cb1fe80\\\",\\\"key\\\":null,\\\"value\\\":null},\\n{\\\"id\\\":\\\"0000023c-f18a-4cd2-94c2-5ad9533b7c82\\\",\\\"key\\\":null,\\\"value\\\":null},\\n{\\\"id\\\":\\\"00000275-0fd7-4aff-bd5c-626ddd087331\\\",\\\"key\\\":null,\\\"value\\\":null},\\n```\\n\\nHow large the *whole* Json can be? Well, one line is 70 characters (70 bytes in UTF8), so it shouldn't be much more than 17,000,000 * 70 = 1,190,000,000 bytes, which is ~1,134 MB. That doesn't sound too bad. Can we maybe simply download the whole view in one single go, save the Json as a file, and do the whole processing based on that file? Can Couchbase return all the 17 million documents in one go?\\nIt turns out that this works without any problem, Couchbase can return the Json for the whole view without breaking a sweat.\\n\\nI simply used `curl` to download the json with the following command.\\n\\n```bash\\ncurl -o allkeys.json http://mycouchbaseserver:8092/mybucket/_design/all_keys/_view/all_keys?inclusive_end=true&stale=ok&connection_timeout=6000000\\n```\\n\\nA couple of things to note about the URL:\\n\\n - I removed the `limit` and `skip` arguments. This is the important part, this'll make the query return all the elements.\\n - I increased the `connection_timeout`. (Didn't really calculate it, just added 3-4 zeros at the end.)\\n - Changed the value of the `stale` argument to `ok`. In theory this makes our query faster (I didn't measure), since it enables Couchbase to return stale data (which is fine in our case, since we're interested only in the keys, which shouldn't change anyway.)\\n\\nDownloading the whole view took a couple of minutes, and it resulted in a ~1.2 GB json file. So far so good!\\n\\n# Update the expiries\\n\\nIterating over the lines in this json file shouldn't be difficult with our favourite technology, we can use any of the SDKs out there (or probably we can even do it in a bash script with the CLI).\\n\\nI used the .NET SDK to implement this, with the following little console application.\\n\\n```csharp\\nusing Couchbase;\\nusing Couchbase.Configuration.Client;\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Diagnostics;\\nusing System.IO;\\nusing System.Linq;\\nusing System.Net;\\nusing System.Text.RegularExpressions;\\nusing System.Threading.Tasks;\\n\\nnamespace ExpiryUpdater\\n{\\n    class Program\\n    {\\n        private const string serverAddress = \\\"http://mycouchbaseserver:8091/pools\\\";\\n        private const string bucketName = \\\"mybucket\\\";\\n        private const int batchSize = 1000;\\n        private const int chunkSize = 10;\\n        // Regex pattern assuming that the lines in the json file are in the format\\n        // {\\\"id\\\":\\\"0000023c-f18a-4cd2-94c2-5ad9533b7c82\\\",\\\"key\\\":null,\\\"value\\\":null},\\n        private const string idPattern = \\\".*\\\\\\\"id\\\\\\\":\\\\\\\"([^\\\\\\\"]*)\\\\\\\".*\\\";\\n        private static readonly Regex idRegex = new Regex(idPattern, RegexOptions.IgnoreCase | RegexOptions.Compiled);\\n        private static readonly TimeSpan expiry = TimeSpan.FromDays(90);\\n\\n        static void Main(string[] args)\\n        {\\n            var allLines = File.ReadAllLines(@\\\"C:\\\\Temp\\\\allids.json\\\");\\n            allLines = allLines.Skip(1).Take(allLines.Length - 3).ToArray();\\n\\n            Console.WriteLine(\\\"Starting updating expiry. Batch size: {0}\\\", batchSize);\\n\\n            var config = new ClientConfiguration()\\n            {\\n                Servers = new List<Uri>\\n                {\\n                    new Uri(serverAddress)\\n                }\\n            };\\n\\n            // Make sure the SDK can handle multiple parallel connections.\\n            ServicePointManager.DefaultConnectionLimit = chunkSize;\\n            config.BucketConfigs.First().Value.PoolConfiguration.MaxSize = chunkSize;\\n            config.BucketConfigs.First().Value.PoolConfiguration.MinSize = chunkSize;\\n            ClusterHelper.Initialize(config);\\n\\n            var bucket = ClusterHelper.GetBucket(bucketName);\\n\\n            int cnt = 0;\\n            int processed = 0;\\n\\n            var sw = Stopwatch.StartNew();\\n\\n            while (true)\\n            {\\n                Console.Write(\\\"Batch #{0}. Elapsed: {1}. \\\", cnt, sw.Elapsed);\\n\\n                var batchLines = allLines.Skip(cnt * batchSize).Take(batchSize);\\n\\n                Console.WriteLine(\\n                    \\\"Items: {0}/{1} {2}% Speed: {3} docs/sec\\\",\\n                    cnt * batchSize,\\n                    allLines.Length,\\n                    (int)Math.Round((100 * cnt * batchSize) / (double)allLines.Length),\\n                    processed / sw.Elapsed.TotalSeconds);\\n\\n                Parallel.ForEach(batchLines, new ParallelOptions { MaxDegreeOfParallelism = chunkSize }, line =>\\n                {\\n                    var id = idRegex.Match(line).Groups[1].Captures[0].Value;\\n                    var result = bucket.Touch(id, expiry);\\n                    processed++;\\n\\n                    if (!result.Success)\\n                    {\\n                        throw new Exception($\\\"Touch operation failed. Message: {result.Message}\\\", result.Exception);\\n                    }\\n                });\\n\\n                if (batchLines.Count() < batchSize)\\n                {\\n                    break;\\n                }\\n\\n                cnt++;\\n            }\\n\\n            sw.Stop();\\n\\n            Console.WriteLine(\\\"Processing documents done! Total elapsed: {0}\\\", sw.Elapsed);\\n            Console.ReadKey();\\n        }\\n    }\\n}\\n```\\n\\nSome notes about this code.\\n\\n - Since it's throwaway code, there is not much error handling. If opening the file or the regex processing fails, it'll just crash with an unhandled exception. But it was fine for my purposes.\\n - `Parallel.ForEach` is **not** an ideal approach for this scenario, since this work is not CPU-bound, but IO-bound.  \\nWhat I would've rather done is to stay on one thread, start a bunch of asynchronous touch operations with `TouchAsync`, and then wait for all the tasks to finish in parallel with `Task.WaitAll()`. However, for some reason this approach didn't work for me, `TouchAsync()` throws an exception, and I couldn't figure out why, so I just stick with `Parallel.ForEach` (again, throwaway code).\\n - I think reading in the giant json file is not the most performant, when we're doing `Skip` and `Take`, and then `ToArray`, we are probably doing a copy, but I didn't want to spend more time to optimize.\\n - There is no science behind the batch size and chunk size values, I tried a couple of different ones, and these seemed to be the best.\\n\\nThe performance of this script will probably vary a lot depending on your network speed, the distance between you and your Couchbase server, the number of nodes in the cluster, and their performance, etc. In my case the script was processing ~500 documents per second, so running it for 17,000,000 million items took approximately 10 hours.\\n\\nBy running this application, we can watch the progress in the terminal window.\\nIf it crashes in the middle for any reason (for example we are disconnected from the network), or you have to turn off your machine, don't worry. Simply take a note of what was the last batch you processed, then change the `cnt` variable to that value, and start it again.\\n\\nOne thing to keep in mind is that this approach only updates the documents which are in the bucket at the time of downloading the json of the view, so this is ideal to use when we've already updated our application code to take care of the expiry of new documents.\\n\\nI hope this post can save some time for people struggling with the same issue. And if you manage to figure out a more sophisticated approach to do this, all suggestions are welcome in the comments.\"}]],\"sections\":[[10,0]]}",
        "html": "<h1 id=\"introduction\">Introduction</h1>\n<p>When using Couchbase, sometimes we want to update the expiry of all of the documents stored in a bucket.<br>\nIn my case it was in a project where initially we weren't sure how we want to handle document expiry (either periodically run some query on our data store, and backup and remove all the old elementsor use the expiry mechanism built into Couchbase), so we ended up deciding we'll just insert all our documents without expiry for now, and we'll figure out later how we want to handle the problem.<br>\nAnd granted, before we knew it, we had 17 millions documents without expiry in our bucket. Oops.</p>\n<p>At that point we decided we want to use Couchbase's built-in expiry mechanism, so we'll set the <code>Expiry</code> property on every new document we store.</p>\n<p>And we realized that we also retroactively have to set the expiry of all our existing documents too. This turned out to be not exactly trivial. In this post I'll describe how I ended up doing it.</p>\n<p>(I was using the .NET SDK, so that's what you'll see in the code samples, but the approach described here can be used with any of the other SDKs, or even with the CLI.)</p>\n<h1 id=\"theapproach\">The approach</h1>\n<p>I started googling to see how we can update the expiry for all documents in a bucket, and quickly found out that Couchbase has no built-in operation for updating the expiry in bulk.<br>\nBased on my investigation the best option was to simply query over all the document ids in our bucket, and update the expiry of them one by one.</p>\n<h1 id=\"queryallthekeysinabucket\">Query all the keys in a bucket</h1>\n<p>In order to be able to iterate over all our documents, we have to be able to query the keys in a bucket, but that's not completely trivial either. Unlike SQL, we can't just iterate over all the elements by default, we have to utilize an index or a view to do that.<br>\nOne way to achieve this is to create a simple view, which retrieves the ids of our documents.</p>\n<h2 id=\"createtheview\">Create the view</h2>\n<p>Creating the necessary view is really simple, we can do it from the Couchbase management console.<br>\nOn the &quot;Data Buckets&quot; screen click on the Views button next to our bucket.</p>\n<p><img src=\"/content/images/2017/03/viewbutton.png\" alt=\"The Views button on the Couchbase management console.\"></p>\n<p>Then click on &quot;Create Development View&quot;, and give it a name. I used the name &quot;all_keys&quot;.</p>\n<p><img src=\"/content/images/2017/03/createview.png\" alt=\"Creating a new view.\"></p>\n<p>On the View screen we can customize our query. Actually, the query generated by default is suitable for us, because it already returns our document keys.<br>\nThe only thing I changed in the query was that I removed even emitting the <code>meta.id</code>, because the keys are already return by the view without this, so removing this will make our downloaded view file slightly smaller. This is the view I used.</p>\n<pre><code class=\"language-javascript\">function (doc, meta) {\n  emit(null, null);\n}\n</code></pre>\n<p>We can test our view by clicking on the &quot;Show Results&quot; button.<br>\nAfter this, go back to the &quot;Views&quot; page and click on the &quot;Publish&quot; button.</p>\n<p><img src=\"/content/images/2017/03/publishview.png\" alt=\"Publish the view.\"></p>\n<p>By doing this we &quot;promote&quot; the view to production, and it's gonna query our whole bucket instead of just a small portion for development. Then we can switch to the &quot;Production&quot; tab and open our published view and test it.</p>\n<h2 id=\"queryingtheview\">Querying the view</h2>\n<p>The simplest way to query a view is to call the HTTP endpoint Couchbase is providing for our view. If we go to the page of the View, we can find an example of this URL at the bottom of the page.</p>\n<p><img src=\"/content/images/2017/03/viewurl.png\" alt=\"The URL of the query endpoint can be found on the View page.\"></p>\n<p><strong>Gotcha</strong>: make sure to get the url from the <em>production</em> view, and not the <em>development</em> one, since the latter will only contain a portion of our data.</p>\n<p>This is an example of the query url:</p>\n<pre><code>http://mycouchbaseserver:8092/mybucket/_design/all_keys/_view/all_keys?connection_timeout=60000&amp;inclusive_end=true&amp;limit=6&amp;skip=0&amp;stale=false\n</code></pre>\n<p>Where the <code>limit</code> and <code>skip</code> arguments specify the portion of the view we want to retrieve.</p>\n<p>(<strong>Querying the view through the SDK</strong>: We can also query the view with the SDK, for example in C# we can use the <code>bucket.CreateQuery()</code> method, but to simply get all the keys I think it's easier to just simply use the HTTP endpoint.)</p>\n<h2 id=\"processingthedataafailedattempt\">Processing the data: a failed attempt</h2>\n<p>At first it seemed to be a reasonable approach to simply query the view in batches (let's say 1000 documents at a time), and then update the expiry for each of them with the SDK (for example in the .NET SDK we can update the expiry with the <code>Touch()</code> method).</p>\n<p>This is a pseudo-code implementation of this approach:</p>\n<pre><code class=\"language-csharp\">var baseUrl = &quot;http://mycouchbaseserver:8092/mybucket/_design/all_keys/_view/all_keys?connection_timeout=60000&amp;stale=false&amp;inclusive_end=true&quot;;\nvar batchSize = 100;\nvar cnt = 0;\nvar expiry = TimeSpan.FromDays(10);\n\nwhile(true)\n{\n    var documents = DownloadDocuments(baseUrl + $&quot;&amp;limit={batchSize}&amp;skip={cnt*batchSize}&quot;);\n\n    foreach (var doc in documents)\n    {\n        bucket.Touch(doc, expiry);\n    }\n    \n    if (documents.Count &lt; batchSize)\n    {\n        break;\n    }\n    \n    cnt++;\n}\n</code></pre>\n<p>The code seems to make sense, and it works too. The problem is its performance.<br>\nIt turns out that for some reason, executing the query takes more and more time as the number of skipped document increases (as we get further and further through our documents).</p>\n<p>In my case, with 17 million elements the response time of the query was the following:</p>\n<ul>\n<li>Querying 1000 elements at the beginning (<code>&amp;skip=0&amp;limit=1000</code>): under 200ms</li>\n<li>Querying 1000 elements starting from 500,000 (<code>&amp;skip=500000&amp;limit=1000</code>): 5 seconds</li>\n<li>Querying 1000 elements starting from 10,000,000 (<code>&amp;skip=10000000&amp;limit=1000</code>): 2 and a half minutes</li>\n</ul>\n<p>So this approach was not working, as my script was progressing through the bucket, it got slower and slower, after a couple of hundred thoursand documents it was barely doing any progress.</p>\n<p>I investigated quite a lot about the cause of this. I suspect it's caused by not having some necessary index to be able to efficiently query any portion of my view (I also asked the question <a href=\"https://forums.couchbase.com/t/how-do-i-get-all-keys-from-the-bucket/35/9?u=markvincze\">here</a>).<br>\nYet, however I tried to set up primary or secondary indices, I couldn't speed the query up.</p>\n<p>Then I had a different idea.</p>\n<h2 id=\"bruteforce\">Brute force!</h2>\n<p>I had 17 million documents in the bucket, which sounds a lot, but actually my view is only returning the keys and nothing else, the output looks like this (depending on your keys of course, which are GUIDs in my case):</p>\n<pre><code class=\"language-json\">{&quot;id&quot;:&quot;000001ba-c4c9-4691-9c48-ca4b0cb1fe80&quot;,&quot;key&quot;:null,&quot;value&quot;:null},\n{&quot;id&quot;:&quot;0000023c-f18a-4cd2-94c2-5ad9533b7c82&quot;,&quot;key&quot;:null,&quot;value&quot;:null},\n{&quot;id&quot;:&quot;00000275-0fd7-4aff-bd5c-626ddd087331&quot;,&quot;key&quot;:null,&quot;value&quot;:null},\n</code></pre>\n<p>How large the <em>whole</em> Json can be? Well, one line is 70 characters (70 bytes in UTF8), so it shouldn't be much more than 17,000,000 * 70 = 1,190,000,000 bytes, which is ~1,134 MB. That doesn't sound too bad. Can we maybe simply download the whole view in one single go, save the Json as a file, and do the whole processing based on that file? Can Couchbase return all the 17 million documents in one go?<br>\nIt turns out that this works without any problem, Couchbase can return the Json for the whole view without breaking a sweat.</p>\n<p>I simply used <code>curl</code> to download the json with the following command.</p>\n<pre><code class=\"language-bash\">curl -o allkeys.json http://mycouchbaseserver:8092/mybucket/_design/all_keys/_view/all_keys?inclusive_end=true&amp;stale=ok&amp;connection_timeout=6000000\n</code></pre>\n<p>A couple of things to note about the URL:</p>\n<ul>\n<li>I removed the <code>limit</code> and <code>skip</code> arguments. This is the important part, this'll make the query return all the elements.</li>\n<li>I increased the <code>connection_timeout</code>. (Didn't really calculate it, just added 3-4 zeros at the end.)</li>\n<li>Changed the value of the <code>stale</code> argument to <code>ok</code>. In theory this makes our query faster (I didn't measure), since it enables Couchbase to return stale data (which is fine in our case, since we're interested only in the keys, which shouldn't change anyway.)</li>\n</ul>\n<p>Downloading the whole view took a couple of minutes, and it resulted in a ~1.2 GB json file. So far so good!</p>\n<h1 id=\"updatetheexpiries\">Update the expiries</h1>\n<p>Iterating over the lines in this json file shouldn't be difficult with our favourite technology, we can use any of the SDKs out there (or probably we can even do it in a bash script with the CLI).</p>\n<p>I used the .NET SDK to implement this, with the following little console application.</p>\n<pre><code class=\"language-csharp\">using Couchbase;\nusing Couchbase.Configuration.Client;\nusing System;\nusing System.Collections.Generic;\nusing System.Diagnostics;\nusing System.IO;\nusing System.Linq;\nusing System.Net;\nusing System.Text.RegularExpressions;\nusing System.Threading.Tasks;\n\nnamespace ExpiryUpdater\n{\n    class Program\n    {\n        private const string serverAddress = &quot;http://mycouchbaseserver:8091/pools&quot;;\n        private const string bucketName = &quot;mybucket&quot;;\n        private const int batchSize = 1000;\n        private const int chunkSize = 10;\n        // Regex pattern assuming that the lines in the json file are in the format\n        // {&quot;id&quot;:&quot;0000023c-f18a-4cd2-94c2-5ad9533b7c82&quot;,&quot;key&quot;:null,&quot;value&quot;:null},\n        private const string idPattern = &quot;.*\\&quot;id\\&quot;:\\&quot;([^\\&quot;]*)\\&quot;.*&quot;;\n        private static readonly Regex idRegex = new Regex(idPattern, RegexOptions.IgnoreCase | RegexOptions.Compiled);\n        private static readonly TimeSpan expiry = TimeSpan.FromDays(90);\n\n        static void Main(string[] args)\n        {\n            var allLines = File.ReadAllLines(@&quot;C:\\Temp\\allids.json&quot;);\n            allLines = allLines.Skip(1).Take(allLines.Length - 3).ToArray();\n\n            Console.WriteLine(&quot;Starting updating expiry. Batch size: {0}&quot;, batchSize);\n\n            var config = new ClientConfiguration()\n            {\n                Servers = new List&lt;Uri&gt;\n                {\n                    new Uri(serverAddress)\n                }\n            };\n\n            // Make sure the SDK can handle multiple parallel connections.\n            ServicePointManager.DefaultConnectionLimit = chunkSize;\n            config.BucketConfigs.First().Value.PoolConfiguration.MaxSize = chunkSize;\n            config.BucketConfigs.First().Value.PoolConfiguration.MinSize = chunkSize;\n            ClusterHelper.Initialize(config);\n\n            var bucket = ClusterHelper.GetBucket(bucketName);\n\n            int cnt = 0;\n            int processed = 0;\n\n            var sw = Stopwatch.StartNew();\n\n            while (true)\n            {\n                Console.Write(&quot;Batch #{0}. Elapsed: {1}. &quot;, cnt, sw.Elapsed);\n\n                var batchLines = allLines.Skip(cnt * batchSize).Take(batchSize);\n\n                Console.WriteLine(\n                    &quot;Items: {0}/{1} {2}% Speed: {3} docs/sec&quot;,\n                    cnt * batchSize,\n                    allLines.Length,\n                    (int)Math.Round((100 * cnt * batchSize) / (double)allLines.Length),\n                    processed / sw.Elapsed.TotalSeconds);\n\n                Parallel.ForEach(batchLines, new ParallelOptions { MaxDegreeOfParallelism = chunkSize }, line =&gt;\n                {\n                    var id = idRegex.Match(line).Groups[1].Captures[0].Value;\n                    var result = bucket.Touch(id, expiry);\n                    processed++;\n\n                    if (!result.Success)\n                    {\n                        throw new Exception($&quot;Touch operation failed. Message: {result.Message}&quot;, result.Exception);\n                    }\n                });\n\n                if (batchLines.Count() &lt; batchSize)\n                {\n                    break;\n                }\n\n                cnt++;\n            }\n\n            sw.Stop();\n\n            Console.WriteLine(&quot;Processing documents done! Total elapsed: {0}&quot;, sw.Elapsed);\n            Console.ReadKey();\n        }\n    }\n}\n</code></pre>\n<p>Some notes about this code.</p>\n<ul>\n<li>Since it's throwaway code, there is not much error handling. If opening the file or the regex processing fails, it'll just crash with an unhandled exception. But it was fine for my purposes.</li>\n<li><code>Parallel.ForEach</code> is <strong>not</strong> an ideal approach for this scenario, since this work is not CPU-bound, but IO-bound.<br>\nWhat I would've rather done is to stay on one thread, start a bunch of asynchronous touch operations with <code>TouchAsync</code>, and then wait for all the tasks to finish in parallel with <code>Task.WaitAll()</code>. However, for some reason this approach didn't work for me, <code>TouchAsync()</code> throws an exception, and I couldn't figure out why, so I just stick with <code>Parallel.ForEach</code> (again, throwaway code).</li>\n<li>I think reading in the giant json file is not the most performant, when we're doing <code>Skip</code> and <code>Take</code>, and then <code>ToArray</code>, we are probably doing a copy, but I didn't want to spend more time to optimize.</li>\n<li>There is no science behind the batch size and chunk size values, I tried a couple of different ones, and these seemed to be the best.</li>\n</ul>\n<p>The performance of this script will probably vary a lot depending on your network speed, the distance between you and your Couchbase server, the number of nodes in the cluster, and their performance, etc. In my case the script was processing ~500 documents per second, so running it for 17,000,000 million items took approximately 10 hours.</p>\n<p>By running this application, we can watch the progress in the terminal window.<br>\nIf it crashes in the middle for any reason (for example we are disconnected from the network), or you have to turn off your machine, don't worry. Simply take a note of what was the last batch you processed, then change the <code>cnt</code> variable to that value, and start it again.</p>\n<p>One thing to keep in mind is that this approach only updates the documents which are in the bucket at the time of downloading the json of the view, so this is ideal to use when we've already updated our application code to take care of the expiry of new documents.</p>\n<p>I hope this post can save some time for people struggling with the same issue. And if you manage to figure out a more sophisticated approach to do this, all suggestions are welcome in the comments.</p>\n",
        "comment_id": "34",
        "plaintext": "Introduction\nWhen using Couchbase, sometimes we want to update the expiry of all of the\ndocuments stored in a bucket.\nIn my case it was in a project where initially we weren't sure how we want to\nhandle document expiry (either periodically run some query on our data store,\nand backup and remove all the old elementsor use the expiry mechanism built\ninto Couchbase), so we ended up deciding we'll just insert all our documents\nwithout expiry for now, and we'll figure out later how we want to handle the\nproblem.\nAnd granted, before we knew it, we had 17 millions documents without expiry in\nour bucket. Oops.\n\nAt that point we decided we want to use Couchbase's built-in expiry mechanism,\nso we'll set the Expiry  property on every new document we store.\n\nAnd we realized that we also retroactively have to set the expiry of all our\nexisting documents too. This turned out to be not exactly trivial. In this post\nI'll describe how I ended up doing it.\n\n(I was using the .NET SDK, so that's what you'll see in the code samples, but\nthe approach described here can be used with any of the other SDKs, or even with\nthe CLI.)\n\nThe approach\nI started googling to see how we can update the expiry for all documents in a\nbucket, and quickly found out that Couchbase has no built-in operation for\nupdating the expiry in bulk.\nBased on my investigation the best option was to simply query over all the\ndocument ids in our bucket, and update the expiry of them one by one.\n\nQuery all the keys in a bucket\nIn order to be able to iterate over all our documents, we have to be able to\nquery the keys in a bucket, but that's not completely trivial either. Unlike\nSQL, we can't just iterate over all the elements by default, we have to utilize\nan index or a view to do that.\nOne way to achieve this is to create a simple view, which retrieves the ids of\nour documents.\n\nCreate the view\nCreating the necessary view is really simple, we can do it from the Couchbase\nmanagement console.\nOn the \"Data Buckets\" screen click on the Views button next to our bucket.\n\n\n\nThen click on \"Create Development View\", and give it a name. I used the name\n\"all_keys\".\n\n\n\nOn the View screen we can customize our query. Actually, the query generated by\ndefault is suitable for us, because it already returns our document keys.\nThe only thing I changed in the query was that I removed even emitting the \nmeta.id, because the keys are already return by the view without this, so\nremoving this will make our downloaded view file slightly smaller. This is the\nview I used.\n\nfunction (doc, meta) {\n  emit(null, null);\n}\n\n\nWe can test our view by clicking on the \"Show Results\" button.\nAfter this, go back to the \"Views\" page and click on the \"Publish\" button.\n\n\n\nBy doing this we \"promote\" the view to production, and it's gonna query our\nwhole bucket instead of just a small portion for development. Then we can switch\nto the \"Production\" tab and open our published view and test it.\n\nQuerying the view\nThe simplest way to query a view is to call the HTTP endpoint Couchbase is\nproviding for our view. If we go to the page of the View, we can find an example\nof this URL at the bottom of the page.\n\n\n\nGotcha: make sure to get the url from the production  view, and not the \ndevelopment  one, since the latter will only contain a portion of our data.\n\nThis is an example of the query url:\n\nhttp://mycouchbaseserver:8092/mybucket/_design/all_keys/_view/all_keys?connection_timeout=60000&inclusive_end=true&limit=6&skip=0&stale=false\n\n\nWhere the limit  and skip  arguments specify the portion of the view we want to\nretrieve.\n\n(Querying the view through the SDK: We can also query the view with the SDK, for\nexample in C# we can use the bucket.CreateQuery()  method, but to simply get all\nthe keys I think it's easier to just simply use the HTTP endpoint.)\n\nProcessing the data: a failed attempt\nAt first it seemed to be a reasonable approach to simply query the view in\nbatches (let's say 1000 documents at a time), and then update the expiry for\neach of them with the SDK (for example in the .NET SDK we can update the expiry\nwith the Touch()  method).\n\nThis is a pseudo-code implementation of this approach:\n\nvar baseUrl = \"http://mycouchbaseserver:8092/mybucket/_design/all_keys/_view/all_keys?connection_timeout=60000&stale=false&inclusive_end=true\";\nvar batchSize = 100;\nvar cnt = 0;\nvar expiry = TimeSpan.FromDays(10);\n\nwhile(true)\n{\n    var documents = DownloadDocuments(baseUrl + $\"&limit={batchSize}&skip={cnt*batchSize}\");\n\n    foreach (var doc in documents)\n    {\n        bucket.Touch(doc, expiry);\n    }\n    \n    if (documents.Count < batchSize)\n    {\n        break;\n    }\n    \n    cnt++;\n}\n\n\nThe code seems to make sense, and it works too. The problem is its performance.\nIt turns out that for some reason, executing the query takes more and more time\nas the number of skipped document increases (as we get further and further\nthrough our documents).\n\nIn my case, with 17 million elements the response time of the query was the\nfollowing:\n\n * Querying 1000 elements at the beginning (&skip=0&limit=1000): under 200ms\n * Querying 1000 elements starting from 500,000 (&skip=500000&limit=1000): 5\n   seconds\n * Querying 1000 elements starting from 10,000,000 (&skip=10000000&limit=1000):\n   2 and a half minutes\n\nSo this approach was not working, as my script was progressing through the\nbucket, it got slower and slower, after a couple of hundred thoursand documents\nit was barely doing any progress.\n\nI investigated quite a lot about the cause of this. I suspect it's caused by not\nhaving some necessary index to be able to efficiently query any portion of my\nview (I also asked the question here\n[https://forums.couchbase.com/t/how-do-i-get-all-keys-from-the-bucket/35/9?u=markvincze]\n).\nYet, however I tried to set up primary or secondary indices, I couldn't speed\nthe query up.\n\nThen I had a different idea.\n\nBrute force!\nI had 17 million documents in the bucket, which sounds a lot, but actually my\nview is only returning the keys and nothing else, the output looks like this\n(depending on your keys of course, which are GUIDs in my case):\n\n{\"id\":\"000001ba-c4c9-4691-9c48-ca4b0cb1fe80\",\"key\":null,\"value\":null},\n{\"id\":\"0000023c-f18a-4cd2-94c2-5ad9533b7c82\",\"key\":null,\"value\":null},\n{\"id\":\"00000275-0fd7-4aff-bd5c-626ddd087331\",\"key\":null,\"value\":null},\n\n\nHow large the whole  Json can be? Well, one line is 70 characters (70 bytes in\nUTF8), so it shouldn't be much more than 17,000,000 * 70 = 1,190,000,000 bytes,\nwhich is ~1,134 MB. That doesn't sound too bad. Can we maybe simply download the\nwhole view in one single go, save the Json as a file, and do the whole\nprocessing based on that file? Can Couchbase return all the 17 million documents\nin one go?\nIt turns out that this works without any problem, Couchbase can return the Json\nfor the whole view without breaking a sweat.\n\nI simply used curl  to download the json with the following command.\n\ncurl -o allkeys.json http://mycouchbaseserver:8092/mybucket/_design/all_keys/_view/all_keys?inclusive_end=true&stale=ok&connection_timeout=6000000\n\n\nA couple of things to note about the URL:\n\n * I removed the limit  and skip  arguments. This is the important part, this'll\n   make the query return all the elements.\n * I increased the connection_timeout. (Didn't really calculate it, just added\n   3-4 zeros at the end.)\n * Changed the value of the stale  argument to ok. In theory this makes our\n   query faster (I didn't measure), since it enables Couchbase to return stale\n   data (which is fine in our case, since we're interested only in the keys,\n   which shouldn't change anyway.)\n\nDownloading the whole view took a couple of minutes, and it resulted in a ~1.2\nGB json file. So far so good!\n\nUpdate the expiries\nIterating over the lines in this json file shouldn't be difficult with our\nfavourite technology, we can use any of the SDKs out there (or probably we can\neven do it in a bash script with the CLI).\n\nI used the .NET SDK to implement this, with the following little console\napplication.\n\nusing Couchbase;\nusing Couchbase.Configuration.Client;\nusing System;\nusing System.Collections.Generic;\nusing System.Diagnostics;\nusing System.IO;\nusing System.Linq;\nusing System.Net;\nusing System.Text.RegularExpressions;\nusing System.Threading.Tasks;\n\nnamespace ExpiryUpdater\n{\n    class Program\n    {\n        private const string serverAddress = \"http://mycouchbaseserver:8091/pools\";\n        private const string bucketName = \"mybucket\";\n        private const int batchSize = 1000;\n        private const int chunkSize = 10;\n        // Regex pattern assuming that the lines in the json file are in the format\n        // {\"id\":\"0000023c-f18a-4cd2-94c2-5ad9533b7c82\",\"key\":null,\"value\":null},\n        private const string idPattern = \".*\\\"id\\\":\\\"([^\\\"]*)\\\".*\";\n        private static readonly Regex idRegex = new Regex(idPattern, RegexOptions.IgnoreCase | RegexOptions.Compiled);\n        private static readonly TimeSpan expiry = TimeSpan.FromDays(90);\n\n        static void Main(string[] args)\n        {\n            var allLines = File.ReadAllLines(@\"C:\\Temp\\allids.json\");\n            allLines = allLines.Skip(1).Take(allLines.Length - 3).ToArray();\n\n            Console.WriteLine(\"Starting updating expiry. Batch size: {0}\", batchSize);\n\n            var config = new ClientConfiguration()\n            {\n                Servers = new List<Uri>\n                {\n                    new Uri(serverAddress)\n                }\n            };\n\n            // Make sure the SDK can handle multiple parallel connections.\n            ServicePointManager.DefaultConnectionLimit = chunkSize;\n            config.BucketConfigs.First().Value.PoolConfiguration.MaxSize = chunkSize;\n            config.BucketConfigs.First().Value.PoolConfiguration.MinSize = chunkSize;\n            ClusterHelper.Initialize(config);\n\n            var bucket = ClusterHelper.GetBucket(bucketName);\n\n            int cnt = 0;\n            int processed = 0;\n\n            var sw = Stopwatch.StartNew();\n\n            while (true)\n            {\n                Console.Write(\"Batch #{0}. Elapsed: {1}. \", cnt, sw.Elapsed);\n\n                var batchLines = allLines.Skip(cnt * batchSize).Take(batchSize);\n\n                Console.WriteLine(\n                    \"Items: {0}/{1} {2}% Speed: {3} docs/sec\",\n                    cnt * batchSize,\n                    allLines.Length,\n                    (int)Math.Round((100 * cnt * batchSize) / (double)allLines.Length),\n                    processed / sw.Elapsed.TotalSeconds);\n\n                Parallel.ForEach(batchLines, new ParallelOptions { MaxDegreeOfParallelism = chunkSize }, line =>\n                {\n                    var id = idRegex.Match(line).Groups[1].Captures[0].Value;\n                    var result = bucket.Touch(id, expiry);\n                    processed++;\n\n                    if (!result.Success)\n                    {\n                        throw new Exception($\"Touch operation failed. Message: {result.Message}\", result.Exception);\n                    }\n                });\n\n                if (batchLines.Count() < batchSize)\n                {\n                    break;\n                }\n\n                cnt++;\n            }\n\n            sw.Stop();\n\n            Console.WriteLine(\"Processing documents done! Total elapsed: {0}\", sw.Elapsed);\n            Console.ReadKey();\n        }\n    }\n}\n\n\nSome notes about this code.\n\n * Since it's throwaway code, there is not much error handling. If opening the\n   file or the regex processing fails, it'll just crash with an unhandled\n   exception. But it was fine for my purposes.\n * Parallel.ForEach  is not  an ideal approach for this scenario, since this\n   work is not CPU-bound, but IO-bound.\n   What I would've rather done is to stay on one thread, start a bunch of\n   asynchronous touch operations with TouchAsync, and then wait for all the\n   tasks to finish in parallel with Task.WaitAll(). However, for some reason\n   this approach didn't work for me, TouchAsync()  throws an exception, and I\n   couldn't figure out why, so I just stick with Parallel.ForEach  (again,\n   throwaway code).\n * I think reading in the giant json file is not the most performant, when we're\n   doing Skip  and Take, and then ToArray, we are probably doing a copy, but I\n   didn't want to spend more time to optimize.\n * There is no science behind the batch size and chunk size values, I tried a\n   couple of different ones, and these seemed to be the best.\n\nThe performance of this script will probably vary a lot depending on your\nnetwork speed, the distance between you and your Couchbase server, the number of\nnodes in the cluster, and their performance, etc. In my case the script was\nprocessing ~500 documents per second, so running it for 17,000,000 million items\ntook approximately 10 hours.\n\nBy running this application, we can watch the progress in the terminal window.\nIf it crashes in the middle for any reason (for example we are disconnected from\nthe network), or you have to turn off your machine, don't worry. Simply take a\nnote of what was the last batch you processed, then change the cnt  variable to\nthat value, and start it again.\n\nOne thing to keep in mind is that this approach only updates the documents which\nare in the bucket at the time of downloading the json of the view, so this is\nideal to use when we've already updated our application code to take care of the\nexpiry of new documents.\n\nI hope this post can save some time for people struggling with the same issue.\nAnd if you manage to figure out a more sophisticated approach to do this, all\nsuggestions are welcome in the comments.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "A simple approach to update the expiry of all the documents in a Couchbase bucket, using a view and a custom updater script.",
        "author_id": "1",
        "created_at": "2017-03-25 15:19:37",
        "created_by": "1",
        "updated_at": "2017-03-25 15:45:49",
        "updated_by": "1",
        "published_at": "2017-03-25 15:41:17",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de297c",
        "uuid": "a0799396-51d3-42f3-8989-dde6e57b5b01",
        "title": "Series: Jumpstart F# web development",
        "slug": "series-jumpstart-f-web-development",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"<img src=\\\"/content/images/2017/02/fsharp-logo-1.png\\\" style=\\\"float: left; max-width: 40%; position: static; webkit-transform: translateX(0%); ms-transform: translateX(0%); transform: translateX(0%);\\\" /> F# is a functional first general purpose programming language targeting the .NET Framework. Since .NET is a framework commonly used for web development (most often with C# and ASP.NET). And if we would like to do web development on the .NET framework in a more functional style, using F# is the logical choice.\\n\\nF# is a language loved by its community, which is actively working on improving its tooling and creating its own ecosystem. This also means that sometimes the experience of the development workflow is less streamlined than its C# counterpart, which is usually officially supported by Microsoft and integrated into Visual Studio.\\nIf we want to get started with web development in F#, we might need to use some workarounds to get our existing tools working, or we'll need to roll our own scripts implementing our workflow.\\n\\nIn this series I'd like to give an overview of the different ways we can use F# for web development today, and describe the steps we have to take to get started.\\nThere are several blog posts and tutorials out there about this topic, but all of them only cover the necessary configuration for a particular combination of frameworks. (And many of them are also pretty outdated.)\\nWith this series I want to collect all the up to date information needed for getting started in some of the common scenarios, and I'll include the links to the sources I used.\\n\\nI plan to cover the following scenarios.\\n(I'll update the links in this list as I publish the posts. The list might also change as I go along and encounter new ways to use F#.)\\n\\n - [F# with ASP.NET on classic .NET](/jumpstart-f-web-development-f-with-asp-net-on-classic-net)\\n - [F# with Suave.IO on classic .NET](/jumpstart-f-web-development-f-with-suave-io-on-classic-net)\\n - [F# with ASP.NET on .NET Core](/jumpstart-f-web-development-f-with-asp-net-core)\\n - [F# with Suave.IO on .NET Core](/jumpstart-f-web-development-f-with-suave-io-on-net-core)\"}]],\"sections\":[[10,0]]}",
        "html": "<p><img src=\"/content/images/2017/02/fsharp-logo-1.png\" style=\"float: left; max-width: 40%; position: static; webkit-transform: translateX(0%); ms-transform: translateX(0%); transform: translateX(0%);\" /> F# is a functional first general purpose programming language targeting the .NET Framework. Since .NET is a framework commonly used for web development (most often with C# and ASP.NET). And if we would like to do web development on the .NET framework in a more functional style, using F# is the logical choice.</p>\n<p>F# is a language loved by its community, which is actively working on improving its tooling and creating its own ecosystem. This also means that sometimes the experience of the development workflow is less streamlined than its C# counterpart, which is usually officially supported by Microsoft and integrated into Visual Studio.<br>\nIf we want to get started with web development in F#, we might need to use some workarounds to get our existing tools working, or we'll need to roll our own scripts implementing our workflow.</p>\n<p>In this series I'd like to give an overview of the different ways we can use F# for web development today, and describe the steps we have to take to get started.<br>\nThere are several blog posts and tutorials out there about this topic, but all of them only cover the necessary configuration for a particular combination of frameworks. (And many of them are also pretty outdated.)<br>\nWith this series I want to collect all the up to date information needed for getting started in some of the common scenarios, and I'll include the links to the sources I used.</p>\n<p>I plan to cover the following scenarios.<br>\n(I'll update the links in this list as I publish the posts. The list might also change as I go along and encounter new ways to use F#.)</p>\n<ul>\n<li><a href=\"/jumpstart-f-web-development-f-with-asp-net-on-classic-net\">F# with ASP.NET on classic .NET</a></li>\n<li><a href=\"/jumpstart-f-web-development-f-with-suave-io-on-classic-net\">F# with Suave.IO on classic .NET</a></li>\n<li><a href=\"/jumpstart-f-web-development-f-with-asp-net-core\">F# with ASP.NET on .NET Core</a></li>\n<li><a href=\"/jumpstart-f-web-development-f-with-suave-io-on-net-core\">F# with Suave.IO on .NET Core</a></li>\n</ul>\n",
        "comment_id": "29",
        "plaintext": "F# is a functional first general purpose programming language targeting the\n.NET Framework. Since .NET is a framework commonly used for web development\n(most often with C# and ASP.NET). And if we would like to do web development on\nthe .NET framework in a more functional style, using F# is the logical choice.\n\nF# is a language loved by its community, which is actively working on improving\nits tooling and creating its own ecosystem. This also means that sometimes the\nexperience of the development workflow is less streamlined than its C#\ncounterpart, which is usually officially supported by Microsoft and integrated\ninto Visual Studio.\nIf we want to get started with web development in F#, we might need to use some\nworkarounds to get our existing tools working, or we'll need to roll our own\nscripts implementing our workflow.\n\nIn this series I'd like to give an overview of the different ways we can use F#\nfor web development today, and describe the steps we have to take to get\nstarted.\nThere are several blog posts and tutorials out there about this topic, but all\nof them only cover the necessary configuration for a particular combination of\nframeworks. (And many of them are also pretty outdated.)\nWith this series I want to collect all the up to date information needed for\ngetting started in some of the common scenarios, and I'll include the links to\nthe sources I used.\n\nI plan to cover the following scenarios.\n(I'll update the links in this list as I publish the posts. The list might also\nchange as I go along and encounter new ways to use F#.)\n\n * F# with ASP.NET on classic .NET\n   [/jumpstart-f-web-development-f-with-asp-net-on-classic-net]\n * F# with Suave.IO on classic .NET\n   [/jumpstart-f-web-development-f-with-suave-io-on-classic-net]\n * F# with ASP.NET on .NET Core\n   [/jumpstart-f-web-development-f-with-asp-net-core]\n * F# with Suave.IO on .NET Core\n   [/jumpstart-f-web-development-f-with-suave-io-on-net-core]",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "Instructions to get started with F# web development using either ASP.NET or Suave.IO, targeting the .NET Framework or .NET Core.",
        "author_id": "1",
        "created_at": "2017-02-05 17:19:12",
        "created_by": "1",
        "updated_at": "2017-06-20 07:05:08",
        "updated_by": "1",
        "published_at": "2017-02-05 17:35:00",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de297d",
        "uuid": "02d4a320-7960-4ede-9fb5-bea9084c1344",
        "title": "You've been upgraded to the latest version of Ghost",
        "slug": "ghost-0-7",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"You've just upgraded to the latest version of Ghost and we've made a few changes that you should probably know about!\\n\\n## Woah, why does everything look different?\\n\\nAfter two years and hundreds of thousands of users, we learned a great deal about what was (and wasn't) working in the old Ghost admin user interface. What you're looking at is Ghost's first major UI refresh, with a strong focus on being more usable and robust all round.\\n\\n![New Design](https://ghost.org/images/zelda.png)\\n\\nThe main navigation menu, previously located at the top of your screen, has now moved over to the left. This makes it way easier to work with on mobile devices, and has the added benefit of providing ample space for upcoming features!\\n\\n## Lost and found: Your old posts\\n\\nFrom talking to many of you we understand that finding old posts in the admin area was a real pain; so we've added a new magical search bar which lets you quickly find posts for editing, without having to scroll endlessly. Take it for a spin!\\n\\n![Search](https://ghost.org/images/search.gif)\\n\\nQuestions? Comments? Send us a tweet [@TryGhost](https://twitter.com/tryghost)\\n\\nOh, and yes  you can safely delete this draft post!\\n\\n![Test image](/content/images/2016/03/ChaniaCrete_EN-US12835250677_1920x1200.jpg)\"}]],\"sections\":[[10,0]]}",
        "html": "<p>You've just upgraded to the latest version of Ghost and we've made a few changes that you should probably know about!</p>\n<h2 id=\"woahwhydoeseverythinglookdifferent\">Woah, why does everything look different?</h2>\n<p>After two years and hundreds of thousands of users, we learned a great deal about what was (and wasn't) working in the old Ghost admin user interface. What you're looking at is Ghost's first major UI refresh, with a strong focus on being more usable and robust all round.</p>\n<p><img src=\"https://ghost.org/images/zelda.png\" alt=\"New Design\"></p>\n<p>The main navigation menu, previously located at the top of your screen, has now moved over to the left. This makes it way easier to work with on mobile devices, and has the added benefit of providing ample space for upcoming features!</p>\n<h2 id=\"lostandfoundyouroldposts\">Lost and found: Your old posts</h2>\n<p>From talking to many of you we understand that finding old posts in the admin area was a real pain; so we've added a new magical search bar which lets you quickly find posts for editing, without having to scroll endlessly. Take it for a spin!</p>\n<p><img src=\"https://ghost.org/images/search.gif\" alt=\"Search\"></p>\n<p>Questions? Comments? Send us a tweet <a href=\"https://twitter.com/tryghost\">@TryGhost</a></p>\n<p>Oh, and yes  you can safely delete this draft post!</p>\n<p><img src=\"/content/images/2016/03/ChaniaCrete_EN-US12835250677_1920x1200.jpg\" alt=\"Test image\"></p>\n",
        "comment_id": "15",
        "plaintext": "You've just upgraded to the latest version of Ghost and we've made a few changes\nthat you should probably know about!\n\nWoah, why does everything look different?\nAfter two years and hundreds of thousands of users, we learned a great deal\nabout what was (and wasn't) working in the old Ghost admin user interface. What\nyou're looking at is Ghost's first major UI refresh, with a strong focus on\nbeing more usable and robust all round.\n\n\n\nThe main navigation menu, previously located at the top of your screen, has now\nmoved over to the left. This makes it way easier to work with on mobile devices,\nand has the added benefit of providing ample space for upcoming features!\n\nLost and found: Your old posts\nFrom talking to many of you we understand that finding old posts in the admin\narea was a real pain; so we've added a new magical search bar which lets you\nquickly find posts for editing, without having to scroll endlessly. Take it for\na spin!\n\n\n\nQuestions? Comments? Send us a tweet @TryGhost [https://twitter.com/tryghost]\n\nOh, and yes  you can safely delete this draft post!",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "draft",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": null,
        "author_id": "1",
        "created_at": "2016-03-20 14:19:48",
        "created_by": "1",
        "updated_at": "2017-08-17 21:08:47",
        "updated_by": "1",
        "published_at": "2016-03-20 14:19:48",
        "published_by": null,
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de297e",
        "uuid": "b3703078-e199-42fc-b6d0-7c4db4ab9e37",
        "title": "Tear down your ASP.NET Core api between integration tests",
        "slug": "tear-down-your-asp-net-core-api-between-integration-tests",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"The way to write integration tests for ASP.NET applications has been made much easier with the advent of ASP.NET Core. This is mainly due to the programming model becoming super modular, which means that it is really easy to spin up an instance of our whole web application for testing purposes and start sending HTTP requests to it from a simple unit test.\\n\\nThe following code illustrates a unit test method, which starts up an ASP.NET Core api, sends an HTTP request to it, and verifies the response, assuming that the api returns the string `\\\"Hello World!\\\"` on the route `/api/hello`.\\n\\n```csharp\\n[Fact]\\npublic async Task Hello_Called_HelloWorldReturned()\\n{\\n    using (var server = new TestServer(new WebHostBuilder().UseStartup<Startup>()))\\n    using (var client = server.CreateClient())\\n    {\\n        var response = await client.GetAsync(\\\"/api/hello\\\");\\n        var responseString = await response.Content.ReadAsStringAsync();\\n        \\n        Assert.Equal(\\\"Hello World!\\\", responseString);\\n    }\\n}\\n```\\n\\nKeep in mind that the above test is not a usual unit test. Nothing is stubbed or mocked, the whole web application is started up, so this is a great tool to verify that our dependency injection, configuration, routing works properly (which we would typically not cover with unit tests).\\n\\nThere is an important thing we have to watch out for, which can cause us some problems, and initially, it might be tricky to figure out why.\\n\\n# The problem\\n\\nThere are multiple ways to encounter this issue, one of the ways I came across this was when I used Quartz.NET in one of our projects, which is a library for scheduling periodically running background tasks.\\n\\nThis is an example of starting up a background task in our `Startup` class which prints a message to the screen every 5 seconds.\\n\\n```csharp\\npublic void Configure(IApplicationBuilder app, IHostingEnvironment env, ILoggerFactory loggerFactory)\\n{\\n    StartBackgroundTask().Wait();\\n\\n    ...\\n}\\n\\nprivate async Task StartBackgroundTask()\\n{\\n    StdSchedulerFactory factory = new StdSchedulerFactory();\\n    var scheduler = await factory.GetScheduler();\\n\\n    await scheduler.Start();\\n\\n    IJobDetail job = JobBuilder.Create<DummyJob>()\\n        .WithIdentity(\\\"job1\\\", \\\"group1\\\")\\n        .Build();\\n\\n    ITrigger trigger = TriggerBuilder.Create()\\n        .WithIdentity(\\\"trigger1\\\", \\\"group1\\\")\\n        .StartNow()\\n        .WithSimpleSchedule(x => x\\n            .WithIntervalInSeconds(5)\\n            .RepeatForever())\\n        .Build();\\n\\n    await scheduler.ScheduleJob(job, trigger);\\n}\\n\\npublic class DummyJob : IJob\\n{\\n    public Task Execute(IJobExecutionContext context)\\n    {\\n        Console.WriteLine(\\\"Ping!\\\");\\n        \\n        return Task.CompletedTask;\\n    }\\n}\\n```\\n\\nThen let's imagine we have two endpoints (they are not really important in this example), `/api/hello` and `/api/dummy`, for which we implement these tests.\\n\\n```\\npublic class ApiTests\\n{\\n    [Fact]\\n    public async Task Hello_Called_HelloWorldReturned()\\n    {\\n        using (var server = new TestServer(new WebHostBuilder().UseStartup<Startup>()))\\n        using (var client = server.CreateClient())\\n        {\\n            var response = await client.GetAsync(\\\"/api/hello\\\");\\n            var responseString = await response.Content.ReadAsStringAsync();\\n            \\n            Assert.Equal(\\\"Hello World!\\\", responseString);\\n        }\\n    }\\n\\n    [Fact]\\n    public async Task Dummy_Called_FooReturned()\\n    {\\n        using (var server = new TestServer(new WebHostBuilder().UseStartup<Startup>()))\\n        using (var client = server.CreateClient())\\n        {\\n            var response = await client.GetAsync(\\\"/api/dummy\\\");\\n            var responseString = await response.Content.ReadAsStringAsync();\\n            \\n            Assert.Equal(\\\"Foo\\\", responseString);\\n        }\\n    }\\n}\\n```\\n\\nIf we execute our tests with `dotnet test`, we'll get the following error:\\n\\n```\\nSystem.AggregateException : One or more errors occurred. (Unable to store Job: 'group1.job1', because one already exists with this identification.)\\n---- Quartz.ObjectAlreadyExistsException : Unable to store Job: 'group1.job1', because one already exists with this identification.\\n```\\n\\nThe reason for this exception is that even though we create a completely new Job and Factory in our `Startup`, apparently Quartz is storing some data related to these jobs statically. And we start a completely new `TestServer` in every test, but because everything happens in the same process, if the first test saves something in a static field, that will affect the subsequent tests.\\n\\nOne thing I tried to solve this was to add an attribute to disable XUnit parallel test execution.\\n\\n```csharp\\n[assembly: CollectionBehavior(DisableTestParallelization = true)]\\n```\\n\\nBut this is not enough, even if the execution is not happening in parallel, the tests can trip each other up.\\n\\n# The solution\\n\\nThe fix is to properly tear down any library or component when our application stops. This can be done by injecting `IApplicationLifetime` into out `Configure` method, and registering a handler to the `ApplicationStopped` event, and tearing down whatever library we're using, in this case, the scheduler of Quartz.NET.\\n\\n```csharp\\npublic void Configure(IApplicationBuilder app, IHostingEnvironment env, ILoggerFactory loggerFactory, IApplicationLifetime appLifetime)\\n{\\n    StartBackgroundTask(appLifetime).Wait();\\n\\n    ...\\n}\\n\\nprivate async Task StartBackgroundTask(IApplicationLifetime appLifetime)\\n{\\n    StdSchedulerFactory factory = new StdSchedulerFactory();\\n    var scheduler = await factory.GetScheduler();\\n\\n    await scheduler.Start();\\n    \\n    // Register a handler which shuts the scheduler down when the api is stopped.\\n    appLifetime.ApplicationStopped.Register(() => scheduler.Shutdown().Wait());\\n    \\n    ...\\n}\\n```\\n\\nWith this simple change, our tests are successful.\\n\\nSo far I encountered this problem with the above described Quartz library, and the SDK of Couchbase (using the `ClusterHelper` class, which does static connection pooling), but I think there are many more libraries out there which can cause this issue, and we might run into problems with our own code too if we are storing data in static fields.\\n\\nOf course, the actual code we have to write in the `ApplicationStopped` handler varies from library to library, in the case of Quartz we had to call `scheduler.Shutdown()`, with the Couchbase SDK I had to do `ClusterHelper.Close()`, in your scenario it might be something else, that has to be figured out for the specific library causing the issue.\\n\\nIf you encounter the same issue with any other library, I'd appreciate it if you could leave a comment with the way it had to be shut down to fix this problem, so that we can save time for anybody else looking for a solution.\"}]],\"sections\":[[10,0]]}",
        "html": "<p>The way to write integration tests for ASP.NET applications has been made much easier with the advent of ASP.NET Core. This is mainly due to the programming model becoming super modular, which means that it is really easy to spin up an instance of our whole web application for testing purposes and start sending HTTP requests to it from a simple unit test.</p>\n<p>The following code illustrates a unit test method, which starts up an ASP.NET Core api, sends an HTTP request to it, and verifies the response, assuming that the api returns the string <code>&quot;Hello World!&quot;</code> on the route <code>/api/hello</code>.</p>\n<pre><code class=\"language-csharp\">[Fact]\npublic async Task Hello_Called_HelloWorldReturned()\n{\n    using (var server = new TestServer(new WebHostBuilder().UseStartup&lt;Startup&gt;()))\n    using (var client = server.CreateClient())\n    {\n        var response = await client.GetAsync(&quot;/api/hello&quot;);\n        var responseString = await response.Content.ReadAsStringAsync();\n        \n        Assert.Equal(&quot;Hello World!&quot;, responseString);\n    }\n}\n</code></pre>\n<p>Keep in mind that the above test is not a usual unit test. Nothing is stubbed or mocked, the whole web application is started up, so this is a great tool to verify that our dependency injection, configuration, routing works properly (which we would typically not cover with unit tests).</p>\n<p>There is an important thing we have to watch out for, which can cause us some problems, and initially, it might be tricky to figure out why.</p>\n<h1 id=\"theproblem\">The problem</h1>\n<p>There are multiple ways to encounter this issue, one of the ways I came across this was when I used Quartz.NET in one of our projects, which is a library for scheduling periodically running background tasks.</p>\n<p>This is an example of starting up a background task in our <code>Startup</code> class which prints a message to the screen every 5 seconds.</p>\n<pre><code class=\"language-csharp\">public void Configure(IApplicationBuilder app, IHostingEnvironment env, ILoggerFactory loggerFactory)\n{\n    StartBackgroundTask().Wait();\n\n    ...\n}\n\nprivate async Task StartBackgroundTask()\n{\n    StdSchedulerFactory factory = new StdSchedulerFactory();\n    var scheduler = await factory.GetScheduler();\n\n    await scheduler.Start();\n\n    IJobDetail job = JobBuilder.Create&lt;DummyJob&gt;()\n        .WithIdentity(&quot;job1&quot;, &quot;group1&quot;)\n        .Build();\n\n    ITrigger trigger = TriggerBuilder.Create()\n        .WithIdentity(&quot;trigger1&quot;, &quot;group1&quot;)\n        .StartNow()\n        .WithSimpleSchedule(x =&gt; x\n            .WithIntervalInSeconds(5)\n            .RepeatForever())\n        .Build();\n\n    await scheduler.ScheduleJob(job, trigger);\n}\n\npublic class DummyJob : IJob\n{\n    public Task Execute(IJobExecutionContext context)\n    {\n        Console.WriteLine(&quot;Ping!&quot;);\n        \n        return Task.CompletedTask;\n    }\n}\n</code></pre>\n<p>Then let's imagine we have two endpoints (they are not really important in this example), <code>/api/hello</code> and <code>/api/dummy</code>, for which we implement these tests.</p>\n<pre><code>public class ApiTests\n{\n    [Fact]\n    public async Task Hello_Called_HelloWorldReturned()\n    {\n        using (var server = new TestServer(new WebHostBuilder().UseStartup&lt;Startup&gt;()))\n        using (var client = server.CreateClient())\n        {\n            var response = await client.GetAsync(&quot;/api/hello&quot;);\n            var responseString = await response.Content.ReadAsStringAsync();\n            \n            Assert.Equal(&quot;Hello World!&quot;, responseString);\n        }\n    }\n\n    [Fact]\n    public async Task Dummy_Called_FooReturned()\n    {\n        using (var server = new TestServer(new WebHostBuilder().UseStartup&lt;Startup&gt;()))\n        using (var client = server.CreateClient())\n        {\n            var response = await client.GetAsync(&quot;/api/dummy&quot;);\n            var responseString = await response.Content.ReadAsStringAsync();\n            \n            Assert.Equal(&quot;Foo&quot;, responseString);\n        }\n    }\n}\n</code></pre>\n<p>If we execute our tests with <code>dotnet test</code>, we'll get the following error:</p>\n<pre><code>System.AggregateException : One or more errors occurred. (Unable to store Job: 'group1.job1', because one already exists with this identification.)\n---- Quartz.ObjectAlreadyExistsException : Unable to store Job: 'group1.job1', because one already exists with this identification.\n</code></pre>\n<p>The reason for this exception is that even though we create a completely new Job and Factory in our <code>Startup</code>, apparently Quartz is storing some data related to these jobs statically. And we start a completely new <code>TestServer</code> in every test, but because everything happens in the same process, if the first test saves something in a static field, that will affect the subsequent tests.</p>\n<p>One thing I tried to solve this was to add an attribute to disable XUnit parallel test execution.</p>\n<pre><code class=\"language-csharp\">[assembly: CollectionBehavior(DisableTestParallelization = true)]\n</code></pre>\n<p>But this is not enough, even if the execution is not happening in parallel, the tests can trip each other up.</p>\n<h1 id=\"thesolution\">The solution</h1>\n<p>The fix is to properly tear down any library or component when our application stops. This can be done by injecting <code>IApplicationLifetime</code> into out <code>Configure</code> method, and registering a handler to the <code>ApplicationStopped</code> event, and tearing down whatever library we're using, in this case, the scheduler of Quartz.NET.</p>\n<pre><code class=\"language-csharp\">public void Configure(IApplicationBuilder app, IHostingEnvironment env, ILoggerFactory loggerFactory, IApplicationLifetime appLifetime)\n{\n    StartBackgroundTask(appLifetime).Wait();\n\n    ...\n}\n\nprivate async Task StartBackgroundTask(IApplicationLifetime appLifetime)\n{\n    StdSchedulerFactory factory = new StdSchedulerFactory();\n    var scheduler = await factory.GetScheduler();\n\n    await scheduler.Start();\n    \n    // Register a handler which shuts the scheduler down when the api is stopped.\n    appLifetime.ApplicationStopped.Register(() =&gt; scheduler.Shutdown().Wait());\n    \n    ...\n}\n</code></pre>\n<p>With this simple change, our tests are successful.</p>\n<p>So far I encountered this problem with the above described Quartz library, and the SDK of Couchbase (using the <code>ClusterHelper</code> class, which does static connection pooling), but I think there are many more libraries out there which can cause this issue, and we might run into problems with our own code too if we are storing data in static fields.</p>\n<p>Of course, the actual code we have to write in the <code>ApplicationStopped</code> handler varies from library to library, in the case of Quartz we had to call <code>scheduler.Shutdown()</code>, with the Couchbase SDK I had to do <code>ClusterHelper.Close()</code>, in your scenario it might be something else, that has to be figured out for the specific library causing the issue.</p>\n<p>If you encounter the same issue with any other library, I'd appreciate it if you could leave a comment with the way it had to be shut down to fix this problem, so that we can save time for anybody else looking for a solution.</p>\n",
        "comment_id": "39",
        "plaintext": "The way to write integration tests for ASP.NET applications has been made much\neasier with the advent of ASP.NET Core. This is mainly due to the programming\nmodel becoming super modular, which means that it is really easy to spin up an\ninstance of our whole web application for testing purposes and start sending\nHTTP requests to it from a simple unit test.\n\nThe following code illustrates a unit test method, which starts up an ASP.NET\nCore api, sends an HTTP request to it, and verifies the response, assuming that\nthe api returns the string \"Hello World!\"  on the route /api/hello.\n\n[Fact]\npublic async Task Hello_Called_HelloWorldReturned()\n{\n    using (var server = new TestServer(new WebHostBuilder().UseStartup<Startup>()))\n    using (var client = server.CreateClient())\n    {\n        var response = await client.GetAsync(\"/api/hello\");\n        var responseString = await response.Content.ReadAsStringAsync();\n        \n        Assert.Equal(\"Hello World!\", responseString);\n    }\n}\n\n\nKeep in mind that the above test is not a usual unit test. Nothing is stubbed or\nmocked, the whole web application is started up, so this is a great tool to\nverify that our dependency injection, configuration, routing works properly\n(which we would typically not cover with unit tests).\n\nThere is an important thing we have to watch out for, which can cause us some\nproblems, and initially, it might be tricky to figure out why.\n\nThe problem\nThere are multiple ways to encounter this issue, one of the ways I came across\nthis was when I used Quartz.NET in one of our projects, which is a library for\nscheduling periodically running background tasks.\n\nThis is an example of starting up a background task in our Startup  class which\nprints a message to the screen every 5 seconds.\n\npublic void Configure(IApplicationBuilder app, IHostingEnvironment env, ILoggerFactory loggerFactory)\n{\n    StartBackgroundTask().Wait();\n\n    ...\n}\n\nprivate async Task StartBackgroundTask()\n{\n    StdSchedulerFactory factory = new StdSchedulerFactory();\n    var scheduler = await factory.GetScheduler();\n\n    await scheduler.Start();\n\n    IJobDetail job = JobBuilder.Create<DummyJob>()\n        .WithIdentity(\"job1\", \"group1\")\n        .Build();\n\n    ITrigger trigger = TriggerBuilder.Create()\n        .WithIdentity(\"trigger1\", \"group1\")\n        .StartNow()\n        .WithSimpleSchedule(x => x\n            .WithIntervalInSeconds(5)\n            .RepeatForever())\n        .Build();\n\n    await scheduler.ScheduleJob(job, trigger);\n}\n\npublic class DummyJob : IJob\n{\n    public Task Execute(IJobExecutionContext context)\n    {\n        Console.WriteLine(\"Ping!\");\n        \n        return Task.CompletedTask;\n    }\n}\n\n\nThen let's imagine we have two endpoints (they are not really important in this\nexample), /api/hello  and /api/dummy, for which we implement these tests.\n\npublic class ApiTests\n{\n    [Fact]\n    public async Task Hello_Called_HelloWorldReturned()\n    {\n        using (var server = new TestServer(new WebHostBuilder().UseStartup<Startup>()))\n        using (var client = server.CreateClient())\n        {\n            var response = await client.GetAsync(\"/api/hello\");\n            var responseString = await response.Content.ReadAsStringAsync();\n            \n            Assert.Equal(\"Hello World!\", responseString);\n        }\n    }\n\n    [Fact]\n    public async Task Dummy_Called_FooReturned()\n    {\n        using (var server = new TestServer(new WebHostBuilder().UseStartup<Startup>()))\n        using (var client = server.CreateClient())\n        {\n            var response = await client.GetAsync(\"/api/dummy\");\n            var responseString = await response.Content.ReadAsStringAsync();\n            \n            Assert.Equal(\"Foo\", responseString);\n        }\n    }\n}\n\n\nIf we execute our tests with dotnet test, we'll get the following error:\n\nSystem.AggregateException : One or more errors occurred. (Unable to store Job: 'group1.job1', because one already exists with this identification.)\n---- Quartz.ObjectAlreadyExistsException : Unable to store Job: 'group1.job1', because one already exists with this identification.\n\n\nThe reason for this exception is that even though we create a completely new Job\nand Factory in our Startup, apparently Quartz is storing some data related to\nthese jobs statically. And we start a completely new TestServer  in every test,\nbut because everything happens in the same process, if the first test saves\nsomething in a static field, that will affect the subsequent tests.\n\nOne thing I tried to solve this was to add an attribute to disable XUnit\nparallel test execution.\n\n[assembly: CollectionBehavior(DisableTestParallelization = true)]\n\n\nBut this is not enough, even if the execution is not happening in parallel, the\ntests can trip each other up.\n\nThe solution\nThe fix is to properly tear down any library or component when our application\nstops. This can be done by injecting IApplicationLifetime  into out Configure \nmethod, and registering a handler to the ApplicationStopped  event, and tearing\ndown whatever library we're using, in this case, the scheduler of Quartz.NET.\n\npublic void Configure(IApplicationBuilder app, IHostingEnvironment env, ILoggerFactory loggerFactory, IApplicationLifetime appLifetime)\n{\n    StartBackgroundTask(appLifetime).Wait();\n\n    ...\n}\n\nprivate async Task StartBackgroundTask(IApplicationLifetime appLifetime)\n{\n    StdSchedulerFactory factory = new StdSchedulerFactory();\n    var scheduler = await factory.GetScheduler();\n\n    await scheduler.Start();\n    \n    // Register a handler which shuts the scheduler down when the api is stopped.\n    appLifetime.ApplicationStopped.Register(() => scheduler.Shutdown().Wait());\n    \n    ...\n}\n\n\nWith this simple change, our tests are successful.\n\nSo far I encountered this problem with the above described Quartz library, and\nthe SDK of Couchbase (using the ClusterHelper  class, which does static\nconnection pooling), but I think there are many more libraries out there which\ncan cause this issue, and we might run into problems with our own code too if we\nare storing data in static fields.\n\nOf course, the actual code we have to write in the ApplicationStopped  handler\nvaries from library to library, in the case of Quartz we had to call \nscheduler.Shutdown(), with the Couchbase SDK I had to do ClusterHelper.Close(),\nin your scenario it might be something else, that has to be figured out for the\nspecific library causing the issue.\n\nIf you encounter the same issue with any other library, I'd appreciate it if you\ncould leave a comment with the way it had to be shut down to fix this problem,\nso that we can save time for anybody else looking for a solution.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "Static state in an ASP.NET Core application can cause problems when running subsequent integration tests. In this post we take a look at how to solve this.",
        "author_id": "1",
        "created_at": "2017-06-21 20:20:16",
        "created_by": "1",
        "updated_at": "2017-06-22 07:23:17",
        "updated_by": "1",
        "published_at": "2017-06-21 20:20:00",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de297f",
        "uuid": "71f384e5-5f69-4752-8a71-345ee5b69b0c",
        "title": "Troubleshooting high memory usage with ASP.NET Core on Kubernetes",
        "slug": "troubleshooting-high-memory-usage-with-asp-net-core-on-kubernetes",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"At work we are running several ASP.NET Core APIs on the hosted version of Kubernetes in the Google Cloud (GCEGoogle Container Engine). In almost all of our components we noticed that they had unreasonably high memory usage. The resource limit for memory was set to 500MB, and still, many of ourrelatively smallAPIs were constantly being restarted by Kubernetes due to exceeding the memory limit.\\n\\nThese graphs show the memory usage of two of our APIs, you can see that they keep increasing until they reach the memory limit, when Kubernetes restarts them.\\n\\n![Graph showing the increasing memory usage of two APIs.](/content/images/2017/08/increasing-memory-1.png)\\n\\nWe automatically thought that our APIs had memory leaks, and spent quite a lot of time investigating the issue, checking the allocations with Visual Studio, creating memory dumps, but couldn't find anything.\\n\\nWe also tried various different ways to reproduce the problem in our development environment.\\n\\n - in dev configuration in VS\\n - on Windows with a production build\\n - on Ubuntu with a production build\\n - in Docker, using the actual production image\\n\\nBut none of these environments produced the same high (>500MB) memory consumption, they increased until 100-150MB, and then stopped.\\n\\nIn the meantime, just to mitigate the continuous restarts of our containers we increased the memory limit from 500MB to 1000MB, which led to an interesting find. After increasing the limit, the memory usage of the containers looked like this.\\n\\n![Memory usage after increasing the limit to 1000MB.](/content/images/2017/08/memory-1G.png)\\n\\nThe memory usage didn't increase infinitely any more (as I thought before), but it capped at around 600MB, and this number seemed to be pretty consistent between different container instances and restarts.\\n\\nThis was a pretty clear indication that we are not actually leaking, but simply a lot of memory is being allocated without any of it getting released. So I started looking into what .NET thinks its memory limit is when running in Kubernetes.\\n\\nKubernetes runs the applications in Docker images, and with Docker the container receives the memory limit through the `--memory` flag of the `docker run` command. So I was wondering that maybe Kubernetes is not passing in any memory limit, and the .NET process thinks that the machine has a lot of available memory.  \\nThis is not the case, we can find the contrary in [the documentation](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/).\\n\\n>The `spec.containers[].resources.limits.memory` is converted to an integer, and used as the value of the `--memory` flag in the docker run command.\\n\\nSo this seemed to be another dead end. I also tried to run the API in Docker on my machine and pass in various limits in the `--memory` flag, but a) I couldn't reproduce the ~600MB memory usage, it stayed at ~150MB and b) also didn't observe the container running over its limit, even if I specified a lower value than 150MB, the container stayed exactly under the limit.\\n\\nEarlier I also posted about this problem on Github under one of the memory leak issues related to Kestrel, and at this point, Tim Seaward sent an interesting [suggestion](https://github.com/aspnet/KestrelHttpServer/issues/1260#issuecomment-321664920) about checking what's the CPU Core count reported to my application on the various environments, since that can affect the memory usage a great deal.\\n\\nI tried printing `Environment.ProcessorCount` on all environments, and I saw the following values.\\n\\n - On my machine, just doing dotnet run, the value was `4`.\\n - On my machine, with Docker, it was `1`.\\n - On Google Cloud Kubernetes it was `8`.\\n\\nThis can finally give an explanation, because the CPU count greatly affects the amount of memory .NET will use with Server GC, as it's explained in the same issue by Ben Adams and Tim Seaward. The more CPU counts we have, the higher amount of memory we'll end up using. (It's still not 100% clear to me what's the exact relationship between the type of GC, the number of cores, and the amount of memory a .NET application will hold on a given server, though [this post](https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/fundamentals) contains some information.)\\n\\nThe suggestion was to switch from Server GC to Workstation GC, which optimizes for lower memory usage. The switch can be done by adding this flag to our csproj file.\\n\\n```xml\\n  <PropertyGroup> \\n    <ServerGarbageCollection>false</ServerGarbageCollection>\\n  </PropertyGroup>\\n```\\n\\nDoing this change and redeploying the API resulted in the following graph (the blue line shows the new deployment).\\n\\n![Memory usage after switching to Workstation GC.](/content/images/2017/08/memory-workstation.png)\\n\\nWorkstation GC made the application be much more conservative in terms of memory usage, and decreased the average from ~600 MB to ~100-150MB. Supposedly Workstation GC can do this by sacrificing some of the performance and throughput that Server GC can provide, but so far I didn't notice any degradation in speed or throughput, although this is not a particularly performance-critical API.\\n\\nThe important takeaway from this story is that the exact environment (OS, available memory, number of CPU cores) is crucial when troubleshooting memory issues because they can affect the .NET GC a great deal. And since there are many awesome folks in the .NET community happy to help, never hesitate to ask a question if you get stuck. ;)\"}]],\"sections\":[[10,0]]}",
        "html": "<p>At work we are running several ASP.NET Core APIs on the hosted version of Kubernetes in the Google Cloud (GCEGoogle Container Engine). In almost all of our components we noticed that they had unreasonably high memory usage. The resource limit for memory was set to 500MB, and still, many of ourrelatively smallAPIs were constantly being restarted by Kubernetes due to exceeding the memory limit.</p>\n<p>These graphs show the memory usage of two of our APIs, you can see that they keep increasing until they reach the memory limit, when Kubernetes restarts them.</p>\n<p><img src=\"/content/images/2017/08/increasing-memory-1.png\" alt=\"Graph showing the increasing memory usage of two APIs.\"></p>\n<p>We automatically thought that our APIs had memory leaks, and spent quite a lot of time investigating the issue, checking the allocations with Visual Studio, creating memory dumps, but couldn't find anything.</p>\n<p>We also tried various different ways to reproduce the problem in our development environment.</p>\n<ul>\n<li>in dev configuration in VS</li>\n<li>on Windows with a production build</li>\n<li>on Ubuntu with a production build</li>\n<li>in Docker, using the actual production image</li>\n</ul>\n<p>But none of these environments produced the same high (&gt;500MB) memory consumption, they increased until 100-150MB, and then stopped.</p>\n<p>In the meantime, just to mitigate the continuous restarts of our containers we increased the memory limit from 500MB to 1000MB, which led to an interesting find. After increasing the limit, the memory usage of the containers looked like this.</p>\n<p><img src=\"/content/images/2017/08/memory-1G.png\" alt=\"Memory usage after increasing the limit to 1000MB.\"></p>\n<p>The memory usage didn't increase infinitely any more (as I thought before), but it capped at around 600MB, and this number seemed to be pretty consistent between different container instances and restarts.</p>\n<p>This was a pretty clear indication that we are not actually leaking, but simply a lot of memory is being allocated without any of it getting released. So I started looking into what .NET thinks its memory limit is when running in Kubernetes.</p>\n<p>Kubernetes runs the applications in Docker images, and with Docker the container receives the memory limit through the <code>--memory</code> flag of the <code>docker run</code> command. So I was wondering that maybe Kubernetes is not passing in any memory limit, and the .NET process thinks that the machine has a lot of available memory.<br>\nThis is not the case, we can find the contrary in <a href=\"https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\">the documentation</a>.</p>\n<blockquote>\n<p>The <code>spec.containers[].resources.limits.memory</code> is converted to an integer, and used as the value of the <code>--memory</code> flag in the docker run command.</p>\n</blockquote>\n<p>So this seemed to be another dead end. I also tried to run the API in Docker on my machine and pass in various limits in the <code>--memory</code> flag, but a) I couldn't reproduce the ~600MB memory usage, it stayed at ~150MB and b) also didn't observe the container running over its limit, even if I specified a lower value than 150MB, the container stayed exactly under the limit.</p>\n<p>Earlier I also posted about this problem on Github under one of the memory leak issues related to Kestrel, and at this point, Tim Seaward sent an interesting <a href=\"https://github.com/aspnet/KestrelHttpServer/issues/1260#issuecomment-321664920\">suggestion</a> about checking what's the CPU Core count reported to my application on the various environments, since that can affect the memory usage a great deal.</p>\n<p>I tried printing <code>Environment.ProcessorCount</code> on all environments, and I saw the following values.</p>\n<ul>\n<li>On my machine, just doing dotnet run, the value was <code>4</code>.</li>\n<li>On my machine, with Docker, it was <code>1</code>.</li>\n<li>On Google Cloud Kubernetes it was <code>8</code>.</li>\n</ul>\n<p>This can finally give an explanation, because the CPU count greatly affects the amount of memory .NET will use with Server GC, as it's explained in the same issue by Ben Adams and Tim Seaward. The more CPU counts we have, the higher amount of memory we'll end up using. (It's still not 100% clear to me what's the exact relationship between the type of GC, the number of cores, and the amount of memory a .NET application will hold on a given server, though <a href=\"https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/fundamentals\">this post</a> contains some information.)</p>\n<p>The suggestion was to switch from Server GC to Workstation GC, which optimizes for lower memory usage. The switch can be done by adding this flag to our csproj file.</p>\n<pre><code class=\"language-xml\">  &lt;PropertyGroup&gt; \n    &lt;ServerGarbageCollection&gt;false&lt;/ServerGarbageCollection&gt;\n  &lt;/PropertyGroup&gt;\n</code></pre>\n<p>Doing this change and redeploying the API resulted in the following graph (the blue line shows the new deployment).</p>\n<p><img src=\"/content/images/2017/08/memory-workstation.png\" alt=\"Memory usage after switching to Workstation GC.\"></p>\n<p>Workstation GC made the application be much more conservative in terms of memory usage, and decreased the average from ~600 MB to ~100-150MB. Supposedly Workstation GC can do this by sacrificing some of the performance and throughput that Server GC can provide, but so far I didn't notice any degradation in speed or throughput, although this is not a particularly performance-critical API.</p>\n<p>The important takeaway from this story is that the exact environment (OS, available memory, number of CPU cores) is crucial when troubleshooting memory issues because they can affect the .NET GC a great deal. And since there are many awesome folks in the .NET community happy to help, never hesitate to ask a question if you get stuck. ;)</p>\n",
        "comment_id": "41",
        "plaintext": "At work we are running several ASP.NET Core APIs on the hosted version of\nKubernetes in the Google Cloud (GCEGoogle Container Engine). In almost all of\nour components we noticed that they had unreasonably high memory usage. The\nresource limit for memory was set to 500MB, and still, many of ourrelatively\nsmallAPIs were constantly being restarted by Kubernetes due to exceeding the\nmemory limit.\n\nThese graphs show the memory usage of two of our APIs, you can see that they\nkeep increasing until they reach the memory limit, when Kubernetes restarts\nthem.\n\n\n\nWe automatically thought that our APIs had memory leaks, and spent quite a lot\nof time investigating the issue, checking the allocations with Visual Studio,\ncreating memory dumps, but couldn't find anything.\n\nWe also tried various different ways to reproduce the problem in our development\nenvironment.\n\n * in dev configuration in VS\n * on Windows with a production build\n * on Ubuntu with a production build\n * in Docker, using the actual production image\n\nBut none of these environments produced the same high (>500MB) memory\nconsumption, they increased until 100-150MB, and then stopped.\n\nIn the meantime, just to mitigate the continuous restarts of our containers we\nincreased the memory limit from 500MB to 1000MB, which led to an interesting\nfind. After increasing the limit, the memory usage of the containers looked like\nthis.\n\n\n\nThe memory usage didn't increase infinitely any more (as I thought before), but\nit capped at around 600MB, and this number seemed to be pretty consistent\nbetween different container instances and restarts.\n\nThis was a pretty clear indication that we are not actually leaking, but simply\na lot of memory is being allocated without any of it getting released. So I\nstarted looking into what .NET thinks its memory limit is when running in\nKubernetes.\n\nKubernetes runs the applications in Docker images, and with Docker the container\nreceives the memory limit through the --memory  flag of the docker run  command.\nSo I was wondering that maybe Kubernetes is not passing in any memory limit, and\nthe .NET process thinks that the machine has a lot of available memory.\nThis is not the case, we can find the contrary in the documentation\n[https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/]\n.\n\nThe spec.containers[].resources.limits.memory  is converted to an integer, and\nused as the value of the --memory  flag in the docker run command.\n\nSo this seemed to be another dead end. I also tried to run the API in Docker on\nmy machine and pass in various limits in the --memory  flag, but a) I couldn't\nreproduce the ~600MB memory usage, it stayed at ~150MB and b) also didn't\nobserve the container running over its limit, even if I specified a lower value\nthan 150MB, the container stayed exactly under the limit.\n\nEarlier I also posted about this problem on Github under one of the memory leak\nissues related to Kestrel, and at this point, Tim Seaward sent an interesting \nsuggestion  about checking what's the CPU Core count reported to my application\non the various environments, since that can affect the memory usage a great\ndeal.\n\nI tried printing Environment.ProcessorCount  on all environments, and I saw the\nfollowing values.\n\n * On my machine, just doing dotnet run, the value was 4.\n * On my machine, with Docker, it was 1.\n * On Google Cloud Kubernetes it was 8.\n\nThis can finally give an explanation, because the CPU count greatly affects the\namount of memory .NET will use with Server GC, as it's explained in the same\nissue by Ben Adams and Tim Seaward. The more CPU counts we have, the higher\namount of memory we'll end up using. (It's still not 100% clear to me what's the\nexact relationship between the type of GC, the number of cores, and the amount\nof memory a .NET application will hold on a given server, though this post\n[https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/fundamentals] \n contains some information.)\n\nThe suggestion was to switch from Server GC to Workstation GC, which optimizes\nfor lower memory usage. The switch can be done by adding this flag to our csproj\nfile.\n\n  <PropertyGroup> \n    <ServerGarbageCollection>false</ServerGarbageCollection>\n  </PropertyGroup>\n\n\nDoing this change and redeploying the API resulted in the following graph (the\nblue line shows the new deployment).\n\n\n\nWorkstation GC made the application be much more conservative in terms of memory\nusage, and decreased the average from ~600 MB to ~100-150MB. Supposedly\nWorkstation GC can do this by sacrificing some of the performance and throughput\nthat Server GC can provide, but so far I didn't notice any degradation in speed\nor throughput, although this is not a particularly performance-critical API.\n\nThe important takeaway from this story is that the exact environment (OS,\navailable memory, number of CPU cores) is crucial when troubleshooting memory\nissues because they can affect the .NET GC a great deal. And since there are\nmany awesome folks in the .NET community happy to help, never hesitate to ask a\nquestion if you get stuck. ;)",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": "",
        "meta_description": "Chasing down why ASP.NET Core applications might use unreasonably much memory in Kubernetes, and how to it can be mitigated.",
        "author_id": "1",
        "created_at": "2017-08-17 21:08:48",
        "created_by": "1",
        "updated_at": "2017-08-18 09:32:20",
        "updated_by": "1",
        "published_at": "2017-08-17 21:15:39",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "59a19a20bacc574fd0de2980",
        "uuid": "e59aa278-3c0c-4a8d-97c0-73f05680c591",
        "title": "Running ASP.NET Core in auto-scaling containers? Warm up!",
        "slug": "running-asp-net-core-in-auto-scaling-containers-warm-up",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"ASP.NET Core APIs are not warmed up by default. This is easy to illustrate, let's scaffold a brand new empty api.\\n\\n```bash\\nmkdir warmuptest\\ncd warmuptest\\ndotnet new webapi\\ndotnet restore\\ndotnet run\\n```\\n\\nThen let's do two consecutive requests against the `/values` endpoint, and measure the response times. This is what we'll see.\\n\\n```\\n$ curl -o /dev/null -s -w %{time_total}\\\\\\\\n http://localhost:5000/values\\n0.594\\n\\n$ curl -o /dev/null -s -w %{time_total}\\\\\\\\n http://localhost:5000/values\\n0.000\\n\\n$ curl -o /dev/null -s -w %{time_total}\\\\\\\\n http://localhost:5000/values\\n0.000\\n```\\n\\nSo you can see that the first request takes a disproportionately long time, around half a second, while the subsequent requests are much faster, they take a couple of milliseconds (for some requests `curl` reports `0.016` or `0.015`, and for some others, it says `0.000`, so I guess the measurement is not granular enough).\\n\\nI tried also to run the api with the flag `-c Release`, I also tried to publish `dotnet publish`, and I even tried to publish a self-contained exe (by adding the `RuntimeIdentifiers` to the csproj and using the `--runtime` flag with `dotnet publish`), but all of them had the same behavior.\\n\\nThis can generally be an annoyance since the first request to our application will always be slightly slower.\\nStill, if we have a monolithic application that we deploy to a server machinelet's sayonce a day, we can probably live with this.\\n\\nOn the other hand, if we have smaller APIs (I'm trying to avoid using the term microservices ^^), and especially if we are running them in auto-scaling containers, for example in Kubernetes, this can become not just a slight annoyance, but a severe problem. Due to having many small APIs which we deploy often, we'll have more \\\"first requests\\\" than in a monolith, and if we are using auto-scaling, then this is going to happen not just after deployments, but every time our API is scaled out, or a container is recycled for any reason.\\n\\nAnd in more complex applications I encountered much worse first request response times, even more than 5 seconds. This resulted in having at least a handful of timeout errors in our logs every day.\\n\\n# The cause (?)\\n\\nLet's get it out of the way: I don't know exactly what's causing the slowness of the first request.\\n\\n - I suspected that the necessary assemblies were being loaded, so I added a piece of code to `Startup` which forced the loading of the assemblies, but that didn't help.\\n - JITting seemed to be another obvious offender. I was looking at forcing the JITting with `RuntimeHelpers.PrepareMethod`, but unfortunately that method is not available in .NET Core, so that wasn't an option.  \\nWe also looked at using [crossgen](https://github.com/dotnet/coreclr/blob/master/Documentation/building/crossgen.md) to JIT the assembly at build time (thanks to my colleague, [Jaanus](https://twitter.com/discosultan) for getting it to work and doing the test!), but that didn't help either.\\n\\nThe only thing I can think of is that this caused by ASP.NET itself warming something up internally. I tried to set the log level to the most verbose but didn't get any message indicating what was going on. (The next step, of course, would be to grab the ASP.NET source and start measuring what's happening under the hood, but I haven't done that yet, especially since the 2.0 release is just around the corner, which might bring changes to the situation.)\\n\\n# A solution\\n\\nThe only reliable solution I could find is not very scientific: after deployment, simply send a couple of warmup requests to the endpoints of the api. It's important to only send real traffic to the deployed instance (only add it to the load balancer, if we're using one) after the warmup is done so that our production system is not affected by the first slow requests.\\n\\nThe way we can do this completely depends on what kind of hosting and deployment technique we are using. If we are using a script, which deploys the API to a server instance, and then adds it to the load balancer, then we'll simply have to do some warmup callsfor example with `curl`between the deployment, and the load-balancer configuration.\\n(If the deployment itself, and then enabling production traffic to the instance is not separated during our deployment, then we cannot do this, but in that case, we don't have zero-downtime deployments, so probably don't care that much about warming up anyway.)\\n\\n## Warm up with a readiness probe in Kubernetes\\n\\nOne specific setup I'd like to show is the one when we use Kubernetes. That's the environment we're using to run our ASP.NET Core APIs at [Travix](https://www.travix.com), where I work.\\n\\nKubernetes has the concept of a [readiness probe](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#define-readiness-probes). The way it works is that a pod with a readiness probe will only get traffic once it's readiness probe has successfully returned a result. This is exactly what we need to do a warmup.\\n\\nIn order to set this up, we'll need an endpoint in our application which will act as the readiness probe. We can implement this in a designated controller, in which we'll send a dummy request to our endpoint we want to exercise.\\n\\n```csharp\\nusing System;\\nusing System.Net;\\nusing System.Net.Http;\\nusing System.Threading.Tasks;\\nusing Microsoft.AspNetCore.Mvc;\\n\\nnamespace dotnettest.Controllers\\n{\\n    [Route(\\\"[controller]\\\")]\\n    public class ReadyController : Controller\\n    {\\n        private static bool isWarmedUp = false;\\n\\n        private string GetFullUrl(string relativeUrl) =>\\n            $\\\"{Request.Scheme}://{Request.Host}{relativeUrl}\\\";\\n\\n        private async Task Warmup()\\n        {\\n            using (var httpClient = new HttpClient())\\n            {\\n                // Warm up the /values endpoint.\\n                var valuesUrl = GetFullUrl(Url.Action(\\\"Get\\\", \\\"Values\\\"));\\n                await httpClient.GetAsync(valuesUrl);\\n\\n                // Here we could warm up some other endpoints too.\\n            }\\n\\n            isWarmedUp = true;\\n        }\\n\\n        [HttpGet, HttpHead]\\n        public async Task<IActionResult> Get()\\n        {\\n            if (!isWarmedUp)\\n            {\\n                await Warmup();\\n            }\\n\\n            return Ok(\\\"API is ready!\\\");\\n        }\\n    }\\n}\\n```\\n(Note: I'm aware that the implementation is not thread-safe, but the worst thing that can happen if we send two parallel requests to this endpoint is that we'll do the warmup multiple times, which I didn't consider to be a huge issue.)\\n\\nNow if we start the API, then send a request first to `/ready`, then to our actual endpoint, the first real API request will be fast.\\n\\n```bash\\n$ curl -o /dev/null -s -w %{time_total}\\\\\\\\n http://localhost:5000/ready\\n1.094\\n\\n$ curl -o /dev/null -s -w %{time_total}\\\\\\\\n http://localhost:5000/values\\n0.000\\n```\\n\\nThe last step is to add the readiness probe to our `kubernetes.yaml` file.\\n\\n```yaml\\nreadinessProbe:\\n  httpGet:\\n    path: /ready\\n    port: 5000\\n  initialDelaySeconds: 10\\n  timeoutSeconds: 60\\n  periodSeconds: 60\\n```\\n\\nThis way Kubernetes will first send a request to `/ready`, and only start sending real traffic to our pod once that request is completed.\\n\\n## Discover our endpoints automatically\\n\\nWhen doing this warmup for all of our various APIs, it became tedious to implement sending the actual requests since, in every API, the set of endpoints to warm up is different.\\n\\nTo mitigate this, I implemented a simple [`ApiWarmer`](https://github.com/markvincze/rest-api-helpers/blob/master/src/RestApiHelpers/Warmup/ApiWarmer.cs), which does the following.\\n\\n - It discovers all the `Controller` types we have.\\n - Gets all the `GET` action methods of all the controllers.\\n - Sends a dummy request to all the endpoints.\\n\\nWhen sending the requests, it fills every input argument with a `default(T)` value, so for a string, it sends `\\\"\\\"`, for an int, it sends `0`, for a Guid, it sends `00000000-0000-0000-0000-000000000000`, etc.\\nNow, this might be problematic in your use case, but in my experience, it's usually fine. It might result in some 404s if we pass in `0` as an identifier, or a 400 Bad Request if we send an empty Guid, but as long as the warm up works, that's fine.\\n(Still, you'll have to verify if this is not a problem in your scenario, it can still be an issue for example if you're logging any `4xx` response, then this can mess up your statistics, etc.)\\n\\nIn order to use the `ApiWarmer`, we have to set up the dependency in our `Startup`.\\n\\n```csharp\\nservices.AddSingleton<IApiWarmer, ApiWarmer>();\\n```\\n\\nAnd instead of manually sending a request in `ReadyController`, use the `IApiWarmer`.\\n\\n```csharp\\n        private readonly IServiceProvider serviceProvider;\\n        private readonly IApiWarmer apiWarmer;\\n\\n        public ReadyController(IServiceProvider serviceProvider, IApiWarmer apiWarmer)\\n        {\\n            this.serviceProvider = serviceProvider;\\n            this.apiWarmer = apiWarmer;\\n        }\\n\\n        [HttpGet, HttpHead]\\n        public async Task<IActionResult> Get()\\n        {\\n            if (!isWarmedUp)\\n            {\\n                await apiWarmer.WarmUp<Startup>(serviceProvider, Url, Request, \\\"Ready\\\");\\n\\n                isWarmedUp = true;\\n            }\\n\\n            return Ok(\\\"API is ready!\\\");\\n        }\\n```\\n\\nThe last argument in the `WarmUp` call is the name of the controller from which we are initiating the warmup. This is important to pass in, otherwise, we might end up in an infinite loop of calls to `/ready`.\\n\\nThe solution we've seen is not super scientific, but it seems to work well in practice, so we can avoid getting random timeout errors on our first API requests.\\n\\nDid you find a more sophisticated way to do the warmup, or did you figure out what's the underlying cause of the first slow request? Any information is welcome in the comments! ;)\"}]],\"sections\":[[10,0]]}",
        "html": "<p>ASP.NET Core APIs are not warmed up by default. This is easy to illustrate, let's scaffold a brand new empty api.</p>\n<pre><code class=\"language-bash\">mkdir warmuptest\ncd warmuptest\ndotnet new webapi\ndotnet restore\ndotnet run\n</code></pre>\n<p>Then let's do two consecutive requests against the <code>/values</code> endpoint, and measure the response times. This is what we'll see.</p>\n<pre><code>$ curl -o /dev/null -s -w %{time_total}\\\\n http://localhost:5000/values\n0.594\n\n$ curl -o /dev/null -s -w %{time_total}\\\\n http://localhost:5000/values\n0.000\n\n$ curl -o /dev/null -s -w %{time_total}\\\\n http://localhost:5000/values\n0.000\n</code></pre>\n<p>So you can see that the first request takes a disproportionately long time, around half a second, while the subsequent requests are much faster, they take a couple of milliseconds (for some requests <code>curl</code> reports <code>0.016</code> or <code>0.015</code>, and for some others, it says <code>0.000</code>, so I guess the measurement is not granular enough).</p>\n<p>I tried also to run the api with the flag <code>-c Release</code>, I also tried to publish <code>dotnet publish</code>, and I even tried to publish a self-contained exe (by adding the <code>RuntimeIdentifiers</code> to the csproj and using the <code>--runtime</code> flag with <code>dotnet publish</code>), but all of them had the same behavior.</p>\n<p>This can generally be an annoyance since the first request to our application will always be slightly slower.<br>\nStill, if we have a monolithic application that we deploy to a server machinelet's sayonce a day, we can probably live with this.</p>\n<p>On the other hand, if we have smaller APIs (I'm trying to avoid using the term microservices ^^), and especially if we are running them in auto-scaling containers, for example in Kubernetes, this can become not just a slight annoyance, but a severe problem. Due to having many small APIs which we deploy often, we'll have more &quot;first requests&quot; than in a monolith, and if we are using auto-scaling, then this is going to happen not just after deployments, but every time our API is scaled out, or a container is recycled for any reason.</p>\n<p>And in more complex applications I encountered much worse first request response times, even more than 5 seconds. This resulted in having at least a handful of timeout errors in our logs every day.</p>\n<h1 id=\"thecause\">The cause (?)</h1>\n<p>Let's get it out of the way: I don't know exactly what's causing the slowness of the first request.</p>\n<ul>\n<li>I suspected that the necessary assemblies were being loaded, so I added a piece of code to <code>Startup</code> which forced the loading of the assemblies, but that didn't help.</li>\n<li>JITting seemed to be another obvious offender. I was looking at forcing the JITting with <code>RuntimeHelpers.PrepareMethod</code>, but unfortunately that method is not available in .NET Core, so that wasn't an option.<br>\nWe also looked at using <a href=\"https://github.com/dotnet/coreclr/blob/master/Documentation/building/crossgen.md\">crossgen</a> to JIT the assembly at build time (thanks to my colleague, <a href=\"https://twitter.com/discosultan\">Jaanus</a> for getting it to work and doing the test!), but that didn't help either.</li>\n</ul>\n<p>The only thing I can think of is that this caused by ASP.NET itself warming something up internally. I tried to set the log level to the most verbose but didn't get any message indicating what was going on. (The next step, of course, would be to grab the ASP.NET source and start measuring what's happening under the hood, but I haven't done that yet, especially since the 2.0 release is just around the corner, which might bring changes to the situation.)</p>\n<h1 id=\"asolution\">A solution</h1>\n<p>The only reliable solution I could find is not very scientific: after deployment, simply send a couple of warmup requests to the endpoints of the api. It's important to only send real traffic to the deployed instance (only add it to the load balancer, if we're using one) after the warmup is done so that our production system is not affected by the first slow requests.</p>\n<p>The way we can do this completely depends on what kind of hosting and deployment technique we are using. If we are using a script, which deploys the API to a server instance, and then adds it to the load balancer, then we'll simply have to do some warmup callsfor example with <code>curl</code>between the deployment, and the load-balancer configuration.<br>\n(If the deployment itself, and then enabling production traffic to the instance is not separated during our deployment, then we cannot do this, but in that case, we don't have zero-downtime deployments, so probably don't care that much about warming up anyway.)</p>\n<h2 id=\"warmupwithareadinessprobeinkubernetes\">Warm up with a readiness probe in Kubernetes</h2>\n<p>One specific setup I'd like to show is the one when we use Kubernetes. That's the environment we're using to run our ASP.NET Core APIs at <a href=\"https://www.travix.com\">Travix</a>, where I work.</p>\n<p>Kubernetes has the concept of a <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#define-readiness-probes\">readiness probe</a>. The way it works is that a pod with a readiness probe will only get traffic once it's readiness probe has successfully returned a result. This is exactly what we need to do a warmup.</p>\n<p>In order to set this up, we'll need an endpoint in our application which will act as the readiness probe. We can implement this in a designated controller, in which we'll send a dummy request to our endpoint we want to exercise.</p>\n<pre><code class=\"language-csharp\">using System;\nusing System.Net;\nusing System.Net.Http;\nusing System.Threading.Tasks;\nusing Microsoft.AspNetCore.Mvc;\n\nnamespace dotnettest.Controllers\n{\n    [Route(&quot;[controller]&quot;)]\n    public class ReadyController : Controller\n    {\n        private static bool isWarmedUp = false;\n\n        private string GetFullUrl(string relativeUrl) =&gt;\n            $&quot;{Request.Scheme}://{Request.Host}{relativeUrl}&quot;;\n\n        private async Task Warmup()\n        {\n            using (var httpClient = new HttpClient())\n            {\n                // Warm up the /values endpoint.\n                var valuesUrl = GetFullUrl(Url.Action(&quot;Get&quot;, &quot;Values&quot;));\n                await httpClient.GetAsync(valuesUrl);\n\n                // Here we could warm up some other endpoints too.\n            }\n\n            isWarmedUp = true;\n        }\n\n        [HttpGet, HttpHead]\n        public async Task&lt;IActionResult&gt; Get()\n        {\n            if (!isWarmedUp)\n            {\n                await Warmup();\n            }\n\n            return Ok(&quot;API is ready!&quot;);\n        }\n    }\n}\n</code></pre>\n<p>(Note: I'm aware that the implementation is not thread-safe, but the worst thing that can happen if we send two parallel requests to this endpoint is that we'll do the warmup multiple times, which I didn't consider to be a huge issue.)</p>\n<p>Now if we start the API, then send a request first to <code>/ready</code>, then to our actual endpoint, the first real API request will be fast.</p>\n<pre><code class=\"language-bash\">$ curl -o /dev/null -s -w %{time_total}\\\\n http://localhost:5000/ready\n1.094\n\n$ curl -o /dev/null -s -w %{time_total}\\\\n http://localhost:5000/values\n0.000\n</code></pre>\n<p>The last step is to add the readiness probe to our <code>kubernetes.yaml</code> file.</p>\n<pre><code class=\"language-yaml\">readinessProbe:\n  httpGet:\n    path: /ready\n    port: 5000\n  initialDelaySeconds: 10\n  timeoutSeconds: 60\n  periodSeconds: 60\n</code></pre>\n<p>This way Kubernetes will first send a request to <code>/ready</code>, and only start sending real traffic to our pod once that request is completed.</p>\n<h2 id=\"discoverourendpointsautomatically\">Discover our endpoints automatically</h2>\n<p>When doing this warmup for all of our various APIs, it became tedious to implement sending the actual requests since, in every API, the set of endpoints to warm up is different.</p>\n<p>To mitigate this, I implemented a simple <a href=\"https://github.com/markvincze/rest-api-helpers/blob/master/src/RestApiHelpers/Warmup/ApiWarmer.cs\"><code>ApiWarmer</code></a>, which does the following.</p>\n<ul>\n<li>It discovers all the <code>Controller</code> types we have.</li>\n<li>Gets all the <code>GET</code> action methods of all the controllers.</li>\n<li>Sends a dummy request to all the endpoints.</li>\n</ul>\n<p>When sending the requests, it fills every input argument with a <code>default(T)</code> value, so for a string, it sends <code>&quot;&quot;</code>, for an int, it sends <code>0</code>, for a Guid, it sends <code>00000000-0000-0000-0000-000000000000</code>, etc.<br>\nNow, this might be problematic in your use case, but in my experience, it's usually fine. It might result in some 404s if we pass in <code>0</code> as an identifier, or a 400 Bad Request if we send an empty Guid, but as long as the warm up works, that's fine.<br>\n(Still, you'll have to verify if this is not a problem in your scenario, it can still be an issue for example if you're logging any <code>4xx</code> response, then this can mess up your statistics, etc.)</p>\n<p>In order to use the <code>ApiWarmer</code>, we have to set up the dependency in our <code>Startup</code>.</p>\n<pre><code class=\"language-csharp\">services.AddSingleton&lt;IApiWarmer, ApiWarmer&gt;();\n</code></pre>\n<p>And instead of manually sending a request in <code>ReadyController</code>, use the <code>IApiWarmer</code>.</p>\n<pre><code class=\"language-csharp\">        private readonly IServiceProvider serviceProvider;\n        private readonly IApiWarmer apiWarmer;\n\n        public ReadyController(IServiceProvider serviceProvider, IApiWarmer apiWarmer)\n        {\n            this.serviceProvider = serviceProvider;\n            this.apiWarmer = apiWarmer;\n        }\n\n        [HttpGet, HttpHead]\n        public async Task&lt;IActionResult&gt; Get()\n        {\n            if (!isWarmedUp)\n            {\n                await apiWarmer.WarmUp&lt;Startup&gt;(serviceProvider, Url, Request, &quot;Ready&quot;);\n\n                isWarmedUp = true;\n            }\n\n            return Ok(&quot;API is ready!&quot;);\n        }\n</code></pre>\n<p>The last argument in the <code>WarmUp</code> call is the name of the controller from which we are initiating the warmup. This is important to pass in, otherwise, we might end up in an infinite loop of calls to <code>/ready</code>.</p>\n<p>The solution we've seen is not super scientific, but it seems to work well in practice, so we can avoid getting random timeout errors on our first API requests.</p>\n<p>Did you find a more sophisticated way to do the warmup, or did you figure out what's the underlying cause of the first slow request? Any information is welcome in the comments! ;)</p>\n",
        "comment_id": "40",
        "plaintext": "ASP.NET Core APIs are not warmed up by default. This is easy to illustrate,\nlet's scaffold a brand new empty api.\n\nmkdir warmuptest\ncd warmuptest\ndotnet new webapi\ndotnet restore\ndotnet run\n\n\nThen let's do two consecutive requests against the /values  endpoint, and\nmeasure the response times. This is what we'll see.\n\n$ curl -o /dev/null -s -w %{time_total}\\\\n http://localhost:5000/values\n0.594\n\n$ curl -o /dev/null -s -w %{time_total}\\\\n http://localhost:5000/values\n0.000\n\n$ curl -o /dev/null -s -w %{time_total}\\\\n http://localhost:5000/values\n0.000\n\n\nSo you can see that the first request takes a disproportionately long time,\naround half a second, while the subsequent requests are much faster, they take a\ncouple of milliseconds (for some requests curl  reports 0.016  or 0.015, and for\nsome others, it says 0.000, so I guess the measurement is not granular enough).\n\nI tried also to run the api with the flag -c Release, I also tried to publish \ndotnet publish, and I even tried to publish a self-contained exe (by adding the \nRuntimeIdentifiers  to the csproj and using the --runtime  flag with dotnet\npublish), but all of them had the same behavior.\n\nThis can generally be an annoyance since the first request to our application\nwill always be slightly slower.\nStill, if we have a monolithic application that we deploy to a server\nmachinelet's sayonce a day, we can probably live with this.\n\nOn the other hand, if we have smaller APIs (I'm trying to avoid using the term\nmicroservices ^^), and especially if we are running them in auto-scaling\ncontainers, for example in Kubernetes, this can become not just a slight\nannoyance, but a severe problem. Due to having many small APIs which we deploy\noften, we'll have more \"first requests\" than in a monolith, and if we are using\nauto-scaling, then this is going to happen not just after deployments, but every\ntime our API is scaled out, or a container is recycled for any reason.\n\nAnd in more complex applications I encountered much worse first request response\ntimes, even more than 5 seconds. This resulted in having at least a handful of\ntimeout errors in our logs every day.\n\nThe cause (?)\nLet's get it out of the way: I don't know exactly what's causing the slowness of\nthe first request.\n\n * I suspected that the necessary assemblies were being loaded, so I added a\n   piece of code to Startup  which forced the loading of the assemblies, but\n   that didn't help.\n * JITting seemed to be another obvious offender. I was looking at forcing the\n   JITting with RuntimeHelpers.PrepareMethod, but unfortunately that method is\n   not available in .NET Core, so that wasn't an option.\n   We also looked at using crossgen\n   [https://github.com/dotnet/coreclr/blob/master/Documentation/building/crossgen.md] \n    to JIT the assembly at build time (thanks to my colleague, Jaanus\n   [https://twitter.com/discosultan]  for getting it to work and doing the\n   test!), but that didn't help either.\n\nThe only thing I can think of is that this caused by ASP.NET itself warming\nsomething up internally. I tried to set the log level to the most verbose but\ndidn't get any message indicating what was going on. (The next step, of course,\nwould be to grab the ASP.NET source and start measuring what's happening under\nthe hood, but I haven't done that yet, especially since the 2.0 release is just\naround the corner, which might bring changes to the situation.)\n\nA solution\nThe only reliable solution I could find is not very scientific: after\ndeployment, simply send a couple of warmup requests to the endpoints of the api.\nIt's important to only send real traffic to the deployed instance (only add it\nto the load balancer, if we're using one) after the warmup is done so that our\nproduction system is not affected by the first slow requests.\n\nThe way we can do this completely depends on what kind of hosting and deployment\ntechnique we are using. If we are using a script, which deploys the API to a\nserver instance, and then adds it to the load balancer, then we'll simply have\nto do some warmup callsfor example with curlbetween the deployment, and the\nload-balancer configuration.\n(If the deployment itself, and then enabling production traffic to the instance\nis not separated during our deployment, then we cannot do this, but in that\ncase, we don't have zero-downtime deployments, so probably don't care that much\nabout warming up anyway.)\n\nWarm up with a readiness probe in Kubernetes\nOne specific setup I'd like to show is the one when we use Kubernetes. That's\nthe environment we're using to run our ASP.NET Core APIs at Travix\n[https://www.travix.com], where I work.\n\nKubernetes has the concept of a readiness probe. The way it works is that a pod\nwith a readiness probe will only get traffic once it's readiness probe has\nsuccessfully returned a result. This is exactly what we need to do a warmup.\n\nIn order to set this up, we'll need an endpoint in our application which will\nact as the readiness probe. We can implement this in a designated controller, in\nwhich we'll send a dummy request to our endpoint we want to exercise.\n\nusing System;\nusing System.Net;\nusing System.Net.Http;\nusing System.Threading.Tasks;\nusing Microsoft.AspNetCore.Mvc;\n\nnamespace dotnettest.Controllers\n{\n    [Route(\"[controller]\")]\n    public class ReadyController : Controller\n    {\n        private static bool isWarmedUp = false;\n\n        private string GetFullUrl(string relativeUrl) =>\n            $\"{Request.Scheme}://{Request.Host}{relativeUrl}\";\n\n        private async Task Warmup()\n        {\n            using (var httpClient = new HttpClient())\n            {\n                // Warm up the /values endpoint.\n                var valuesUrl = GetFullUrl(Url.Action(\"Get\", \"Values\"));\n                await httpClient.GetAsync(valuesUrl);\n\n                // Here we could warm up some other endpoints too.\n            }\n\n            isWarmedUp = true;\n        }\n\n        [HttpGet, HttpHead]\n        public async Task<IActionResult> Get()\n        {\n            if (!isWarmedUp)\n            {\n                await Warmup();\n            }\n\n            return Ok(\"API is ready!\");\n        }\n    }\n}\n\n\n(Note: I'm aware that the implementation is not thread-safe, but the worst thing\nthat can happen if we send two parallel requests to this endpoint is that we'll\ndo the warmup multiple times, which I didn't consider to be a huge issue.)\n\nNow if we start the API, then send a request first to /ready, then to our actual\nendpoint, the first real API request will be fast.\n\n$ curl -o /dev/null -s -w %{time_total}\\\\n http://localhost:5000/ready\n1.094\n\n$ curl -o /dev/null -s -w %{time_total}\\\\n http://localhost:5000/values\n0.000\n\n\nThe last step is to add the readiness probe to our kubernetes.yaml  file.\n\nreadinessProbe:\n  httpGet:\n    path: /ready\n    port: 5000\n  initialDelaySeconds: 10\n  timeoutSeconds: 60\n  periodSeconds: 60\n\n\nThis way Kubernetes will first send a request to /ready, and only start sending\nreal traffic to our pod once that request is completed.\n\nDiscover our endpoints automatically\nWhen doing this warmup for all of our various APIs, it became tedious to\nimplement sending the actual requests since, in every API, the set of endpoints\nto warm up is different.\n\nTo mitigate this, I implemented a simple ApiWarmer\n[https://github.com/markvincze/rest-api-helpers/blob/master/src/RestApiHelpers/Warmup/ApiWarmer.cs]\n, which does the following.\n\n * It discovers all the Controller  types we have.\n * Gets all the GET  action methods of all the controllers.\n * Sends a dummy request to all the endpoints.\n\nWhen sending the requests, it fills every input argument with a default(T) \nvalue, so for a string, it sends \"\", for an int, it sends 0, for a Guid, it\nsends 00000000-0000-0000-0000-000000000000, etc.\nNow, this might be problematic in your use case, but in my experience, it's\nusually fine. It might result in some 404s if we pass in 0  as an identifier, or\na 400 Bad Request if we send an empty Guid, but as long as the warm up works,\nthat's fine.\n(Still, you'll have to verify if this is not a problem in your scenario, it can\nstill be an issue for example if you're logging any 4xx  response, then this can\nmess up your statistics, etc.)\n\nIn order to use the ApiWarmer, we have to set up the dependency in our Startup.\n\nservices.AddSingleton<IApiWarmer, ApiWarmer>();\n\n\nAnd instead of manually sending a request in ReadyController, use the IApiWarmer\n.\n\n        private readonly IServiceProvider serviceProvider;\n        private readonly IApiWarmer apiWarmer;\n\n        public ReadyController(IServiceProvider serviceProvider, IApiWarmer apiWarmer)\n        {\n            this.serviceProvider = serviceProvider;\n            this.apiWarmer = apiWarmer;\n        }\n\n        [HttpGet, HttpHead]\n        public async Task<IActionResult> Get()\n        {\n            if (!isWarmedUp)\n            {\n                await apiWarmer.WarmUp<Startup>(serviceProvider, Url, Request, \"Ready\");\n\n                isWarmedUp = true;\n            }\n\n            return Ok(\"API is ready!\");\n        }\n\n\nThe last argument in the WarmUp  call is the name of the controller from which\nwe are initiating the warmup. This is important to pass in, otherwise, we might\nend up in an infinite loop of calls to /ready.\n\nThe solution we've seen is not super scientific, but it seems to work well in\npractice, so we can avoid getting random timeout errors on our first API\nrequests.\n\nDid you find a more sophisticated way to do the warmup, or did you figure out\nwhat's the underlying cause of the first slow request? Any information is\nwelcome in the comments! ;)",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "The first request to an ASP.NET Core API is always slow. This post shows a way how to warm up our application before deploying it to production.",
        "author_id": "1",
        "created_at": "2017-07-29 16:11:32",
        "created_by": "1",
        "updated_at": "2017-07-29 18:32:34",
        "updated_by": "1",
        "published_at": "2017-07-29 18:32:34",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": null,
        "codeinjection_foot": null,
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "5a0e206819ac3b1e3f6dd28e",
        "uuid": "f47f4bf7-a0b4-4fac-95af-6fdced0cde4b",
        "title": "Build and publish documentation and API reference with DocFx for .NET Core projects",
        "slug": "build-and-publish-documentation-and-api-reference-with-docfx-for-net-core-projects",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"[DocFx](http://dotnet.github.io/docfx/) is an open source tool for generating documentation and API reference, and it has great support for .NET Core projects.\\n\\nDocFx can be slightly intimidating first, because it has a really wide set of features, and the default scaffolded configuration contains quite a lot of files, which at first sight can look a bit complex.\\n\\nWith this post I'd like to give a guide about what is a minimal configuration you need if you want to set up documentation for a .NET Core project with DocFx.  \\nWe'll also take a look at how you can automatically publish the documentation. In that part I'll assume that the project is hosted on GitHub, so we can make the site site available using [GitHub Pages](https://pages.github.com/).\\n\\nFor the sake of this guide I created a dummy project called `DotnetCoreDocfxDemo`, you can find the complete source on [GitHub](https://github.com/markvincze/dotnetcore-docfx-demo).\\n\\n# Install DocFX\\n\\nIn order to scaffold a new documentation project, serve it for testing on our machine, or to build the final version of the docs, we'll need to have the `docfx` CLI installed.\\n\\nThe simplest way to do this is with Chocolatey, by executing the following command.\\n\\n```bash\\ncinst docfx -y\\n```\\n\\n(Alternatively, we can also download and extract a release from the [Releases](https://github.com/dotnet/docfx/releases) page.)\\n\\n# Set up the DocFx project\\n\\nIn the examples I'll assume that our repository has the following.\\n\\n```\\n|- src/\\n   |- MyProject/\\n      |- MyProject.csproj\\n|- test/\\n   |- MyProject.UnitTests/\\n      |- MyProject.UnitTests.csproj\\n|- MySolution.sln\\n```\\n\\nWhere the `src` folder contains our main project, and the unit test projects are in the `test` folder.\\n\\nIf we want to use DocFx, we need to set up a *project*, which consists of a bunch of configuration and content files describing our documentation site. A typical place to put this project in is a `docs` folder at the root of our repository.\\n\\nTo scaffold a default DocFx project in a `docs` directory, issue the following command at the root of the repository.\\n\\n```\\ndocfx init -q -o docs\\n```\\n\\n(`-o` specifies the output folder, while `-q` enables the \\\"quiet\\\" mode, so the command generates the default setup without asking any questions. If you'd like to revise the scaffolding options, omit the `-q` flag.)  \\nLet's review what this generated.\\n\\n```md\\n|- api/: Configuration for the generation of the API reference.\\n|- articles/: Manually written pages, this is what we can use to write actual documentation.\\n|- images/: Image assets.\\n|- src/: Folder is for storing our source code projects. (I don't use this.)\\n|- docfx.json: The main configuration file of our documentation.\\n|- index.md: The content of the index page.\\n|- toc.yml: The main segments of our site which will be displayed in the navigation bar.\\n```\\n\\nThe root of the project contains the `index.md` and `toc.yml` files, which define the content of the main page, and the list of top-level pages.  \\nOther subfolders, like `articles` and `api` also contain the same pair of files, which define the same content for those areas of the site.\\n\\n# Configure the API reference\\n\\nIn order to get the API reference generation working, we have to change our working directory to point to the folder where our source projects reside. In our example this is the `src` folder at the root of our repository (on the same level with the `docs` folder).  \\nWe can do this by adding the `cwd` property to the object in the `src` array. Since we only need to go one level up, we can simply set it to `..`.  \\n\\n```json\\n\\\"src\\\": [\\n  {\\n    \\\"files\\\": [\\n      \\\"src/**.csproj\\\"\\n    ],\\n    \\\"exclude\\\": [\\n      \\\"**/obj/**\\\",\\n      \\\"**/bin/**\\\",\\n      \\\"_site/**\\\"\\n    ],\\n    \\\"cwd\\\": \\\"..\\\"\\n  }\\n],\\n```\\n\\nThis way we are generating the API reference for every project in the `src` folder. If we want to (for example if we only want to have the reference for a subset of our projects), we can also explicitly specify the csproj files we'd like to process.\\n\\n```\\n    \\\"files\\\": [\\n      \\\"Project1/Project1.csproj\\\",\\n      \\\"Project2/Project2.csproj\\\"\\n    ]\\n```\\n\\n**Important**: If we are multi-targeting different .NET Framework versions in our project, we must explicitly specify one of the frameworks here, otherwise `docfx` won't be able to build the project.  \\nWe can do this by adding `properties` to our metadata object:\\n\\n```\\n\\\"metadata\\\": [\\n   {\\n     ...\\n     \\\"properties\\\": {\\n         \\\"TargetFramework\\\": \\\"netstandard1.3\\\"\\n     }\\n   } \\n ],\\n```\\n\\nIn the `api` folder we shouldn't manually edit the `toc.yml`, since the list of table of contents will be generated during the build based on the types we have in our assemblies.\\n\\nOn the other hand, we should change the content of the `index.md` file. This is the main page displayed when we navigate to the API reference, so we can change it to a welcome message, or a high-level overview about our types.  \\nIf we want to add a link to a certain type, we can do it by specifying the full name of the type with html extension. Example:\\n\\n```\\n[MyClass](MyProject.MyClass.html)\\n```\\n\\n*Note: `docfx` will display a warning for \\\"Invalid file link\\\" when generating the documentation, but the link will work properly.*\\n\\n# Set up our documentation pages\\n\\nBy default, an `articles` folder is generated in the DocFx project where we can add some extra documentation pages. We can do this by adding additional Markdown files to that folder, and adding them to the `toc.yml`.\\n\\nIf we want to, we can rename the sections of the site (or add additional sections). For example, I tend to rename the \\\"Articles\\\" section to \\\"Documentation\\\", and change \\\"Api Documentation\\\" to \\\"Api Reference\\\". If you'd like to change these, you should rename the folders (optional), adjust `docfx.json` to include these folders in the `build`, and change the `toc.yml` accordingly.\\n\\n# Set up our index page\\n\\nWe can customize the main index page of our site in by editing the `index.md` file at the root of the DocFx project.\\n\\nIt's a good idea to add an introduction about our project, include a link pointing to the GitHub repo, and display the usual CI and NuGet badges.\\n\\n# Testing the documentation\\n\\nWe can start locally serving the documentation site by entering the `docs` folder and issuing the following command. We can access the site by opening `http://localhost:8080` in the browser.\\n\\n```\\ndocfx --serve\\n```\\n\\n**Important**: If we are working with .NET Core, we might have to set the `VSINSTALLDIR` and `VisualStudioVersion` environment variables, otherwise DocFx won't be able to build our projects. This is something I only found in various GitHub issues, and different people reported different solutions for these problems. So it might happen that this exact configuration won't work on your machine, in that case you have to adjust these values (for example if you have a different edition of Visual Studio installed, you'll have to change the path).\\n\\nTo make this convenient, I recommend creating a script called `serveDocs` at the root of the repository, which sets these variables and then calls `docfx --serve`.\\n\\nIf you use PowerShell, then `serveDocs.ps1` should look like this.\\n\\n```powershell\\n$env:VSINSTALLDIR=\\\"C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2017\\\\Professional\\\"\\n$env:VisualStudioVersion=\\\"15.0\\\"\\n\\ndocfx docs/docfx.json --serve\\n```\\n\\nIf you use bash, then create this `serveDocs.sh`.\\n\\n```bash\\n#!/bin/sh\\nexport VSINSTALLDIR=\\\"C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2017\\\\Professional\\\"\\nexport VisualStudioVersion=\\\"15.0\\\"\\n\\ndocfx docs/docfx.json --serve\\n```\\n\\n# Publish the documentation\\n\\nIn this guide I'll show how to use [GitHub Pages](https://pages.github.com/) to host the documentation, but you can easily adjust the release script if you use a different hosting method.\\nIn order to host a static page on GitHub Pages, we just have to push the content in our repository to a branch called `gh-pages`, and the site will be available at the address `https://accountname.github.io/RepositoryName`.\\n\\n*Note: As flagged by Ben Brandt in the comments, it's important to keep in mind that the site published with GitHub Pages [is always going to be public](https://help.github.com/articles/what-is-github-pages/#guidelines-for-using-github-pages), even if it's hosted in a private repository.*\\n\\n## Create the `gh-pages` branch\\n\\nSince the `gh-pages` will only have to contain our static HTML content and nothing else, it's a good idea to create an empty orphan branch. We can do this in our repository with the following commands.  \\n**Important**: Make sure to commit everything to your work branch before you execute these commands!\\n\\n```bash\\ngit checkout --orphan gh-pages\\ngit reset .\\nrm -r *\\nrm .gitignore\\necho 'The documentation will be here.' > index.html\\ngit add index.html\\ngit commit -m 'Initialize the gh-pages orphan branch'\\ngit push -u origin gh-pages\\n```\\n\\n## Release the documentation\\n\\nWe can create a script to build the documentation and publish it to GitHub Pages. The following steps will be needed:\\n\\n1. Create a temporary folder and clone our repository into it at the `gh-pages` branch.\\n2. Remove all the files from the folder.\\n3. Build the documentation site with `docfx` into the folder.\\n4. Commit and push all the changes to `gh-pages`.\\n\\nWe can do this by creating the following `releaseDocs.sh` script at the root of our repository.\\n\\n```bash\\n#!/bin/sh\\nset -e\\n\\nexport VSINSTALLDIR=\\\"C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2017\\\\Community\\\"\\nexport VisualStudioVersion=\\\"15.0\\\"\\n\\ndocfx ./docs/docfx.json\\n\\nSOURCE_DIR=$PWD\\nTEMP_REPO_DIR=$PWD/../my-project-gh-pages\\n\\necho \\\"Removing temporary doc directory $TEMP_REPO_DIR\\\"\\nrm -rf $TEMP_REPO_DIR\\nmkdir $TEMP_REPO_DIR\\n\\necho \\\"Cloning the repo with the gh-pages branch\\\"\\ngit clone https://github.com/myaccount/my-project.git --branch gh-pages $TEMP_REPO_DIR\\n\\necho \\\"Clear repo directory\\\"\\ncd $TEMP_REPO_DIR\\ngit rm -r *\\n\\necho \\\"Copy documentation into the repo\\\"\\ncp -r $SOURCE_DIR/docs/_site/* .\\n\\necho \\\"Push the new docs to the remote branch\\\"\\ngit add . -A\\ngit commit -m \\\"Update generated documentation\\\"\\ngit push origin gh-pages\\n```\\n\\n(Note that when setting up the necessary environment variables, I'm using the path to the Community version of Visual Studio, because that's what is installed on the AppVeyor build agents. You might have to adjust this based on your environment.)\\n\\nBy calling `releaseDocs.sh` we can update and publish our documentation in a single step.\\n\\n# Automatically publish the docs with AppVeyor\\n\\nWe can include the release script in our CI-pipeline, so that the documentation gets automatically updated on every build.  \\nIn this example I'll show how to do this with AppVeyor, but you can use the same approach with other CI-systems too.\\n\\nIn order to set up an AppVeyor build, we have to add an `appveyor.yml` file to the root of our repository, in which we'll describe the steps needed to build our project (and in our case publish the documentation).\\n\\nThe only slightly tricky part is giving AppVeyor permission to push changes to our repository.  \\nThe recommended way to do this is to create a Personal Access Token on GitHub. We can follow [this guide](https://help.github.com/articles/creating-a-personal-access-token-for-the-command-line/), and the only permission we'll need is `public_repo`. (Make sure to copy the access token to the clipboard after creating it.)\\n\\nWe should handle the access token as a password, so we should encrypt it before adding it to the AppVeyor config. We can do this on the AppVeyor site by clicking on our profile image, and in the menu selecting the \\\"Encrypt data\\\" option. Now we should add this to our yaml file as an environment variable:\\n\\n```yaml\\nenvironment:\\n  github_access_token:\\n    secure: ceFfu552HHIV\\n```\\n\\nWe'll also need the email address we use as our GitHub account, and since it's better not to have it in plain text in our repository, encrypt this too and add it as an environment variable called `github_email`.\\n\\nThen we need to execute a couple of commands to actually use the access token with `git`, and set up our email and user name for pushing (following [this guide](https://www.appveyor.com/docs/how-to/git-push/)).\\n\\n```yaml\\n- git config --global credential.helper store\\n- ps: Add-Content \\\"$env:USERPROFILE\\\\.git-credentials\\\" \\\"https://$($env:github_access_token):x-oauth-basic@github.com`n\\\"\\n- git config --global user.email %github_email%\\n- git config --global user.name \\\"markvincze\\\"\\n```\\n\\nThe last thing to do in our deploy setup is to execute our `releaseDocs.sh` script.\\n\\n```yaml\\n- bash releaseDocs.sh\\n```\\n\\nThe full `appveyor.yml` (also building the solution and running the unit tests) should look like this:\\n\\n```yaml\\nskip_tags: true\\n\\nimage: Visual Studio 2017\\n\\nclone_depth: 1\\n\\nbranches:\\n  only:\\n    - master\\n\\nenvironment:\\n  github_access_token:\\n    secure: ceFfu552HHkV\\n  github_email:\\n    secure: salKyeDwuc37\\n\\ninstall:\\n  - cinst docfx\\n\\nbuild_script:\\n- dotnet restore\\n- dotnet build --configuration Release\\n\\ntest_script:\\n- dotnet test test/MyProject.UnitTests/MyProject.UnitTests.csproj --configuration Release\\n\\ndeploy_script:\\n- git config --global credential.helper store\\n- ps: Add-Content \\\"$env:USERPROFILE\\\\.git-credentials\\\" \\\"https://$($env:github_access_token):x-oauth-basic@github.com`n\\\"\\n- git config --global user.email %github_email%\\n- git config --global user.name \\\"myusername\\\"\\n- bash releaseDocs.sh\\n```\\n\\nWith this configuration in place, if we add our repository in AppVeyor, it will build our solution and publish the documentation on every new commit in the `master` branch.\\n\\n# Summary\\n\\nFollowing this guide we can easily set up automatically generated documentation and API reference for our open-source .NET Core libraries. (However, this post only shows the basic capabilities of DocFx, it has plenty more features that are not in the scope of this guide.)\\n\\nYou can find the source code of the full sample repository on [GitHub](https://github.com/markvincze/dotnetcore-docfx-demo/), and take a look at the published documentation [here](https://markvincze.github.io/dotnetcore-docfx-demo/).\"}]],\"sections\":[[10,0]]}",
        "html": "<p><a href=\"http://dotnet.github.io/docfx/\">DocFx</a> is an open source tool for generating documentation and API reference, and it has great support for .NET Core projects.</p>\n<p>DocFx can be slightly intimidating first, because it has a really wide set of features, and the default scaffolded configuration contains quite a lot of files, which at first sight can look a bit complex.</p>\n<p>With this post I'd like to give a guide about what is a minimal configuration you need if you want to set up documentation for a .NET Core project with DocFx.<br>\nWe'll also take a look at how you can automatically publish the documentation. In that part I'll assume that the project is hosted on GitHub, so we can make the site site available using <a href=\"https://pages.github.com/\">GitHub Pages</a>.</p>\n<p>For the sake of this guide I created a dummy project called <code>DotnetCoreDocfxDemo</code>, you can find the complete source on <a href=\"https://github.com/markvincze/dotnetcore-docfx-demo\">GitHub</a>.</p>\n<h1 id=\"installdocfx\">Install DocFX</h1>\n<p>In order to scaffold a new documentation project, serve it for testing on our machine, or to build the final version of the docs, we'll need to have the <code>docfx</code> CLI installed.</p>\n<p>The simplest way to do this is with Chocolatey, by executing the following command.</p>\n<pre><code class=\"language-bash\">cinst docfx -y\n</code></pre>\n<p>(Alternatively, we can also download and extract a release from the <a href=\"https://github.com/dotnet/docfx/releases\">Releases</a> page.)</p>\n<h1 id=\"setupthedocfxproject\">Set up the DocFx project</h1>\n<p>In the examples I'll assume that our repository has the following.</p>\n<pre><code>|- src/\n   |- MyProject/\n      |- MyProject.csproj\n|- test/\n   |- MyProject.UnitTests/\n      |- MyProject.UnitTests.csproj\n|- MySolution.sln\n</code></pre>\n<p>Where the <code>src</code> folder contains our main project, and the unit test projects are in the <code>test</code> folder.</p>\n<p>If we want to use DocFx, we need to set up a <em>project</em>, which consists of a bunch of configuration and content files describing our documentation site. A typical place to put this project in is a <code>docs</code> folder at the root of our repository.</p>\n<p>To scaffold a default DocFx project in a <code>docs</code> directory, issue the following command at the root of the repository.</p>\n<pre><code>docfx init -q -o docs\n</code></pre>\n<p>(<code>-o</code> specifies the output folder, while <code>-q</code> enables the &quot;quiet&quot; mode, so the command generates the default setup without asking any questions. If you'd like to revise the scaffolding options, omit the <code>-q</code> flag.)<br>\nLet's review what this generated.</p>\n<pre><code class=\"language-md\">|- api/: Configuration for the generation of the API reference.\n|- articles/: Manually written pages, this is what we can use to write actual documentation.\n|- images/: Image assets.\n|- src/: Folder is for storing our source code projects. (I don't use this.)\n|- docfx.json: The main configuration file of our documentation.\n|- index.md: The content of the index page.\n|- toc.yml: The main segments of our site which will be displayed in the navigation bar.\n</code></pre>\n<p>The root of the project contains the <code>index.md</code> and <code>toc.yml</code> files, which define the content of the main page, and the list of top-level pages.<br>\nOther subfolders, like <code>articles</code> and <code>api</code> also contain the same pair of files, which define the same content for those areas of the site.</p>\n<h1 id=\"configuretheapireference\">Configure the API reference</h1>\n<p>In order to get the API reference generation working, we have to change our working directory to point to the folder where our source projects reside. In our example this is the <code>src</code> folder at the root of our repository (on the same level with the <code>docs</code> folder).<br>\nWe can do this by adding the <code>cwd</code> property to the object in the <code>src</code> array. Since we only need to go one level up, we can simply set it to <code>..</code>.</p>\n<pre><code class=\"language-json\">&quot;src&quot;: [\n  {\n    &quot;files&quot;: [\n      &quot;src/**.csproj&quot;\n    ],\n    &quot;exclude&quot;: [\n      &quot;**/obj/**&quot;,\n      &quot;**/bin/**&quot;,\n      &quot;_site/**&quot;\n    ],\n    &quot;cwd&quot;: &quot;..&quot;\n  }\n],\n</code></pre>\n<p>This way we are generating the API reference for every project in the <code>src</code> folder. If we want to (for example if we only want to have the reference for a subset of our projects), we can also explicitly specify the csproj files we'd like to process.</p>\n<pre><code>    &quot;files&quot;: [\n      &quot;Project1/Project1.csproj&quot;,\n      &quot;Project2/Project2.csproj&quot;\n    ]\n</code></pre>\n<p><strong>Important</strong>: If we are multi-targeting different .NET Framework versions in our project, we must explicitly specify one of the frameworks here, otherwise <code>docfx</code> won't be able to build the project.<br>\nWe can do this by adding <code>properties</code> to our metadata object:</p>\n<pre><code>&quot;metadata&quot;: [\n   {\n     ...\n     &quot;properties&quot;: {\n         &quot;TargetFramework&quot;: &quot;netstandard1.3&quot;\n     }\n   } \n ],\n</code></pre>\n<p>In the <code>api</code> folder we shouldn't manually edit the <code>toc.yml</code>, since the list of table of contents will be generated during the build based on the types we have in our assemblies.</p>\n<p>On the other hand, we should change the content of the <code>index.md</code> file. This is the main page displayed when we navigate to the API reference, so we can change it to a welcome message, or a high-level overview about our types.<br>\nIf we want to add a link to a certain type, we can do it by specifying the full name of the type with html extension. Example:</p>\n<pre><code>[MyClass](MyProject.MyClass.html)\n</code></pre>\n<p><em>Note: <code>docfx</code> will display a warning for &quot;Invalid file link&quot; when generating the documentation, but the link will work properly.</em></p>\n<h1 id=\"setupourdocumentationpages\">Set up our documentation pages</h1>\n<p>By default, an <code>articles</code> folder is generated in the DocFx project where we can add some extra documentation pages. We can do this by adding additional Markdown files to that folder, and adding them to the <code>toc.yml</code>.</p>\n<p>If we want to, we can rename the sections of the site (or add additional sections). For example, I tend to rename the &quot;Articles&quot; section to &quot;Documentation&quot;, and change &quot;Api Documentation&quot; to &quot;Api Reference&quot;. If you'd like to change these, you should rename the folders (optional), adjust <code>docfx.json</code> to include these folders in the <code>build</code>, and change the <code>toc.yml</code> accordingly.</p>\n<h1 id=\"setupourindexpage\">Set up our index page</h1>\n<p>We can customize the main index page of our site in by editing the <code>index.md</code> file at the root of the DocFx project.</p>\n<p>It's a good idea to add an introduction about our project, include a link pointing to the GitHub repo, and display the usual CI and NuGet badges.</p>\n<h1 id=\"testingthedocumentation\">Testing the documentation</h1>\n<p>We can start locally serving the documentation site by entering the <code>docs</code> folder and issuing the following command. We can access the site by opening <code>http://localhost:8080</code> in the browser.</p>\n<pre><code>docfx --serve\n</code></pre>\n<p><strong>Important</strong>: If we are working with .NET Core, we might have to set the <code>VSINSTALLDIR</code> and <code>VisualStudioVersion</code> environment variables, otherwise DocFx won't be able to build our projects. This is something I only found in various GitHub issues, and different people reported different solutions for these problems. So it might happen that this exact configuration won't work on your machine, in that case you have to adjust these values (for example if you have a different edition of Visual Studio installed, you'll have to change the path).</p>\n<p>To make this convenient, I recommend creating a script called <code>serveDocs</code> at the root of the repository, which sets these variables and then calls <code>docfx --serve</code>.</p>\n<p>If you use PowerShell, then <code>serveDocs.ps1</code> should look like this.</p>\n<pre><code class=\"language-powershell\">$env:VSINSTALLDIR=&quot;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional&quot;\n$env:VisualStudioVersion=&quot;15.0&quot;\n\ndocfx docs/docfx.json --serve\n</code></pre>\n<p>If you use bash, then create this <code>serveDocs.sh</code>.</p>\n<pre><code class=\"language-bash\">#!/bin/sh\nexport VSINSTALLDIR=&quot;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional&quot;\nexport VisualStudioVersion=&quot;15.0&quot;\n\ndocfx docs/docfx.json --serve\n</code></pre>\n<h1 id=\"publishthedocumentation\">Publish the documentation</h1>\n<p>In this guide I'll show how to use <a href=\"https://pages.github.com/\">GitHub Pages</a> to host the documentation, but you can easily adjust the release script if you use a different hosting method.<br>\nIn order to host a static page on GitHub Pages, we just have to push the content in our repository to a branch called <code>gh-pages</code>, and the site will be available at the address <code>https://accountname.github.io/RepositoryName</code>.</p>\n<p><em>Note: As flagged by Ben Brandt in the comments, it's important to keep in mind that the site published with GitHub Pages <a href=\"https://help.github.com/articles/what-is-github-pages/#guidelines-for-using-github-pages\">is always going to be public</a>, even if it's hosted in a private repository.</em></p>\n<h2 id=\"createtheghpagesbranch\">Create the <code>gh-pages</code> branch</h2>\n<p>Since the <code>gh-pages</code> will only have to contain our static HTML content and nothing else, it's a good idea to create an empty orphan branch. We can do this in our repository with the following commands.<br>\n<strong>Important</strong>: Make sure to commit everything to your work branch before you execute these commands!</p>\n<pre><code class=\"language-bash\">git checkout --orphan gh-pages\ngit reset .\nrm -r *\nrm .gitignore\necho 'The documentation will be here.' &gt; index.html\ngit add index.html\ngit commit -m 'Initialize the gh-pages orphan branch'\ngit push -u origin gh-pages\n</code></pre>\n<h2 id=\"releasethedocumentation\">Release the documentation</h2>\n<p>We can create a script to build the documentation and publish it to GitHub Pages. The following steps will be needed:</p>\n<ol>\n<li>Create a temporary folder and clone our repository into it at the <code>gh-pages</code> branch.</li>\n<li>Remove all the files from the folder.</li>\n<li>Build the documentation site with <code>docfx</code> into the folder.</li>\n<li>Commit and push all the changes to <code>gh-pages</code>.</li>\n</ol>\n<p>We can do this by creating the following <code>releaseDocs.sh</code> script at the root of our repository.</p>\n<pre><code class=\"language-bash\">#!/bin/sh\nset -e\n\nexport VSINSTALLDIR=&quot;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community&quot;\nexport VisualStudioVersion=&quot;15.0&quot;\n\ndocfx ./docs/docfx.json\n\nSOURCE_DIR=$PWD\nTEMP_REPO_DIR=$PWD/../my-project-gh-pages\n\necho &quot;Removing temporary doc directory $TEMP_REPO_DIR&quot;\nrm -rf $TEMP_REPO_DIR\nmkdir $TEMP_REPO_DIR\n\necho &quot;Cloning the repo with the gh-pages branch&quot;\ngit clone https://github.com/myaccount/my-project.git --branch gh-pages $TEMP_REPO_DIR\n\necho &quot;Clear repo directory&quot;\ncd $TEMP_REPO_DIR\ngit rm -r *\n\necho &quot;Copy documentation into the repo&quot;\ncp -r $SOURCE_DIR/docs/_site/* .\n\necho &quot;Push the new docs to the remote branch&quot;\ngit add . -A\ngit commit -m &quot;Update generated documentation&quot;\ngit push origin gh-pages\n</code></pre>\n<p>(Note that when setting up the necessary environment variables, I'm using the path to the Community version of Visual Studio, because that's what is installed on the AppVeyor build agents. You might have to adjust this based on your environment.)</p>\n<p>By calling <code>releaseDocs.sh</code> we can update and publish our documentation in a single step.</p>\n<h1 id=\"automaticallypublishthedocswithappveyor\">Automatically publish the docs with AppVeyor</h1>\n<p>We can include the release script in our CI-pipeline, so that the documentation gets automatically updated on every build.<br>\nIn this example I'll show how to do this with AppVeyor, but you can use the same approach with other CI-systems too.</p>\n<p>In order to set up an AppVeyor build, we have to add an <code>appveyor.yml</code> file to the root of our repository, in which we'll describe the steps needed to build our project (and in our case publish the documentation).</p>\n<p>The only slightly tricky part is giving AppVeyor permission to push changes to our repository.<br>\nThe recommended way to do this is to create a Personal Access Token on GitHub. We can follow <a href=\"https://help.github.com/articles/creating-a-personal-access-token-for-the-command-line/\">this guide</a>, and the only permission we'll need is <code>public_repo</code>. (Make sure to copy the access token to the clipboard after creating it.)</p>\n<p>We should handle the access token as a password, so we should encrypt it before adding it to the AppVeyor config. We can do this on the AppVeyor site by clicking on our profile image, and in the menu selecting the &quot;Encrypt data&quot; option. Now we should add this to our yaml file as an environment variable:</p>\n<pre><code class=\"language-yaml\">environment:\n  github_access_token:\n    secure: ceFfu552HHIV\n</code></pre>\n<p>We'll also need the email address we use as our GitHub account, and since it's better not to have it in plain text in our repository, encrypt this too and add it as an environment variable called <code>github_email</code>.</p>\n<p>Then we need to execute a couple of commands to actually use the access token with <code>git</code>, and set up our email and user name for pushing (following <a href=\"https://www.appveyor.com/docs/how-to/git-push/\">this guide</a>).</p>\n<pre><code class=\"language-yaml\">- git config --global credential.helper store\n- ps: Add-Content &quot;$env:USERPROFILE\\.git-credentials&quot; &quot;https://$($env:github_access_token):x-oauth-basic@github.com`n&quot;\n- git config --global user.email %github_email%\n- git config --global user.name &quot;markvincze&quot;\n</code></pre>\n<p>The last thing to do in our deploy setup is to execute our <code>releaseDocs.sh</code> script.</p>\n<pre><code class=\"language-yaml\">- bash releaseDocs.sh\n</code></pre>\n<p>The full <code>appveyor.yml</code> (also building the solution and running the unit tests) should look like this:</p>\n<pre><code class=\"language-yaml\">skip_tags: true\n\nimage: Visual Studio 2017\n\nclone_depth: 1\n\nbranches:\n  only:\n    - master\n\nenvironment:\n  github_access_token:\n    secure: ceFfu552HHkV\n  github_email:\n    secure: salKyeDwuc37\n\ninstall:\n  - cinst docfx\n\nbuild_script:\n- dotnet restore\n- dotnet build --configuration Release\n\ntest_script:\n- dotnet test test/MyProject.UnitTests/MyProject.UnitTests.csproj --configuration Release\n\ndeploy_script:\n- git config --global credential.helper store\n- ps: Add-Content &quot;$env:USERPROFILE\\.git-credentials&quot; &quot;https://$($env:github_access_token):x-oauth-basic@github.com`n&quot;\n- git config --global user.email %github_email%\n- git config --global user.name &quot;myusername&quot;\n- bash releaseDocs.sh\n</code></pre>\n<p>With this configuration in place, if we add our repository in AppVeyor, it will build our solution and publish the documentation on every new commit in the <code>master</code> branch.</p>\n<h1 id=\"summary\">Summary</h1>\n<p>Following this guide we can easily set up automatically generated documentation and API reference for our open-source .NET Core libraries. (However, this post only shows the basic capabilities of DocFx, it has plenty more features that are not in the scope of this guide.)</p>\n<p>You can find the source code of the full sample repository on <a href=\"https://github.com/markvincze/dotnetcore-docfx-demo/\">GitHub</a>, and take a look at the published documentation <a href=\"https://markvincze.github.io/dotnetcore-docfx-demo/\">here</a>.</p>\n",
        "comment_id": "5a0e206819ac3b1e3f6dd28e",
        "plaintext": "DocFx [http://dotnet.github.io/docfx/]  is an open source tool for generating\ndocumentation and API reference, and it has great support for .NET Core\nprojects.\n\nDocFx can be slightly intimidating first, because it has a really wide set of\nfeatures, and the default scaffolded configuration contains quite a lot of\nfiles, which at first sight can look a bit complex.\n\nWith this post I'd like to give a guide about what is a minimal configuration\nyou need if you want to set up documentation for a .NET Core project with DocFx.\nWe'll also take a look at how you can automatically publish the documentation.\nIn that part I'll assume that the project is hosted on GitHub, so we can make\nthe site site available using GitHub Pages [https://pages.github.com/].\n\nFor the sake of this guide I created a dummy project called DotnetCoreDocfxDemo,\nyou can find the complete source on GitHub\n[https://github.com/markvincze/dotnetcore-docfx-demo].\n\nInstall DocFX\nIn order to scaffold a new documentation project, serve it for testing on our\nmachine, or to build the final version of the docs, we'll need to have the docfx \n CLI installed.\n\nThe simplest way to do this is with Chocolatey, by executing the following\ncommand.\n\ncinst docfx -y\n\n\n(Alternatively, we can also download and extract a release from the Releases\n[https://github.com/dotnet/docfx/releases]  page.)\n\nSet up the DocFx project\nIn the examples I'll assume that our repository has the following.\n\n|- src/\n   |- MyProject/\n      |- MyProject.csproj\n|- test/\n   |- MyProject.UnitTests/\n      |- MyProject.UnitTests.csproj\n|- MySolution.sln\n\n\nWhere the src  folder contains our main project, and the unit test projects are\nin the test  folder.\n\nIf we want to use DocFx, we need to set up a project, which consists of a bunch\nof configuration and content files describing our documentation site. A typical\nplace to put this project in is a docs  folder at the root of our repository.\n\nTo scaffold a default DocFx project in a docs  directory, issue the following\ncommand at the root of the repository.\n\ndocfx init -q -o docs\n\n\n(-o  specifies the output folder, while -q  enables the \"quiet\" mode, so the\ncommand generates the default setup without asking any questions. If you'd like\nto revise the scaffolding options, omit the -q  flag.)\nLet's review what this generated.\n\n|- api/: Configuration for the generation of the API reference.\n|- articles/: Manually written pages, this is what we can use to write actual documentation.\n|- images/: Image assets.\n|- src/: Folder is for storing our source code projects. (I don't use this.)\n|- docfx.json: The main configuration file of our documentation.\n|- index.md: The content of the index page.\n|- toc.yml: The main segments of our site which will be displayed in the navigation bar.\n\n\nThe root of the project contains the index.md  and toc.yml  files, which define\nthe content of the main page, and the list of top-level pages.\nOther subfolders, like articles  and api  also contain the same pair of files,\nwhich define the same content for those areas of the site.\n\nConfigure the API reference\nIn order to get the API reference generation working, we have to change our\nworking directory to point to the folder where our source projects reside. In\nour example this is the src  folder at the root of our repository (on the same\nlevel with the docs  folder).\nWe can do this by adding the cwd  property to the object in the src  array.\nSince we only need to go one level up, we can simply set it to ...\n\n\"src\": [\n  {\n    \"files\": [\n      \"src/**.csproj\"\n    ],\n    \"exclude\": [\n      \"**/obj/**\",\n      \"**/bin/**\",\n      \"_site/**\"\n    ],\n    \"cwd\": \"..\"\n  }\n],\n\n\nThis way we are generating the API reference for every project in the src \nfolder. If we want to (for example if we only want to have the reference for a\nsubset of our projects), we can also explicitly specify the csproj files we'd\nlike to process.\n\n    \"files\": [\n      \"Project1/Project1.csproj\",\n      \"Project2/Project2.csproj\"\n    ]\n\n\nImportant: If we are multi-targeting different .NET Framework versions in our\nproject, we must explicitly specify one of the frameworks here, otherwise docfx \nwon't be able to build the project.\nWe can do this by adding properties  to our metadata object:\n\n\"metadata\": [\n   {\n     ...\n     \"properties\": {\n         \"TargetFramework\": \"netstandard1.3\"\n     }\n   } \n ],\n\n\nIn the api  folder we shouldn't manually edit the toc.yml, since the list of\ntable of contents will be generated during the build based on the types we have\nin our assemblies.\n\nOn the other hand, we should change the content of the index.md  file. This is\nthe main page displayed when we navigate to the API reference, so we can change\nit to a welcome message, or a high-level overview about our types.\nIf we want to add a link to a certain type, we can do it by specifying the full\nname of the type with html extension. Example:\n\n[MyClass](MyProject.MyClass.html)\n\n\nNote: docfx  will display a warning for \"Invalid file link\" when generating the\ndocumentation, but the link will work properly.\n\nSet up our documentation pages\nBy default, an articles  folder is generated in the DocFx project where we can\nadd some extra documentation pages. We can do this by adding additional Markdown\nfiles to that folder, and adding them to the toc.yml.\n\nIf we want to, we can rename the sections of the site (or add additional\nsections). For example, I tend to rename the \"Articles\" section to\n\"Documentation\", and change \"Api Documentation\" to \"Api Reference\". If you'd\nlike to change these, you should rename the folders (optional), adjust \ndocfx.json  to include these folders in the build, and change the toc.yml \naccordingly.\n\nSet up our index page\nWe can customize the main index page of our site in by editing the index.md \nfile at the root of the DocFx project.\n\nIt's a good idea to add an introduction about our project, include a link\npointing to the GitHub repo, and display the usual CI and NuGet badges.\n\nTesting the documentation\nWe can start locally serving the documentation site by entering the docs  folder\nand issuing the following command. We can access the site by opening \nhttp://localhost:8080  in the browser.\n\ndocfx --serve\n\n\nImportant: If we are working with .NET Core, we might have to set the \nVSINSTALLDIR  and VisualStudioVersion  environment variables, otherwise DocFx\nwon't be able to build our projects. This is something I only found in various\nGitHub issues, and different people reported different solutions for these\nproblems. So it might happen that this exact configuration won't work on your\nmachine, in that case you have to adjust these values (for example if you have a\ndifferent edition of Visual Studio installed, you'll have to change the path).\n\nTo make this convenient, I recommend creating a script called serveDocs  at the\nroot of the repository, which sets these variables and then calls docfx --serve.\n\nIf you use PowerShell, then serveDocs.ps1  should look like this.\n\n$env:VSINSTALLDIR=\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\"\n$env:VisualStudioVersion=\"15.0\"\n\ndocfx docs/docfx.json --serve\n\n\nIf you use bash, then create this serveDocs.sh.\n\n#!/bin/sh\nexport VSINSTALLDIR=\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\"\nexport VisualStudioVersion=\"15.0\"\n\ndocfx docs/docfx.json --serve\n\n\nPublish the documentation\nIn this guide I'll show how to use GitHub Pages [https://pages.github.com/]  to\nhost the documentation, but you can easily adjust the release script if you use\na different hosting method.\nIn order to host a static page on GitHub Pages, we just have to push the content\nin our repository to a branch called gh-pages, and the site will be available at\nthe address https://accountname.github.io/RepositoryName.\n\nNote: As flagged by Ben Brandt in the comments, it's important to keep in mind\nthat the site published with GitHub Pages is always going to be public, even if\nit's hosted in a private repository.\n\nCreate the gh-pages  branch\nSince the gh-pages  will only have to contain our static HTML content and\nnothing else, it's a good idea to create an empty orphan branch. We can do this\nin our repository with the following commands.\nImportant: Make sure to commit everything to your work branch before you execute\nthese commands!\n\ngit checkout --orphan gh-pages\ngit reset .\nrm -r *\nrm .gitignore\necho 'The documentation will be here.' > index.html\ngit add index.html\ngit commit -m 'Initialize the gh-pages orphan branch'\ngit push -u origin gh-pages\n\n\nRelease the documentation\nWe can create a script to build the documentation and publish it to GitHub\nPages. The following steps will be needed:\n\n 1. Create a temporary folder and clone our repository into it at the gh-pages \n    branch.\n 2. Remove all the files from the folder.\n 3. Build the documentation site with docfx  into the folder.\n 4. Commit and push all the changes to gh-pages.\n\nWe can do this by creating the following releaseDocs.sh  script at the root of\nour repository.\n\n#!/bin/sh\nset -e\n\nexport VSINSTALLDIR=\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\"\nexport VisualStudioVersion=\"15.0\"\n\ndocfx ./docs/docfx.json\n\nSOURCE_DIR=$PWD\nTEMP_REPO_DIR=$PWD/../my-project-gh-pages\n\necho \"Removing temporary doc directory $TEMP_REPO_DIR\"\nrm -rf $TEMP_REPO_DIR\nmkdir $TEMP_REPO_DIR\n\necho \"Cloning the repo with the gh-pages branch\"\ngit clone https://github.com/myaccount/my-project.git --branch gh-pages $TEMP_REPO_DIR\n\necho \"Clear repo directory\"\ncd $TEMP_REPO_DIR\ngit rm -r *\n\necho \"Copy documentation into the repo\"\ncp -r $SOURCE_DIR/docs/_site/* .\n\necho \"Push the new docs to the remote branch\"\ngit add . -A\ngit commit -m \"Update generated documentation\"\ngit push origin gh-pages\n\n\n(Note that when setting up the necessary environment variables, I'm using the\npath to the Community version of Visual Studio, because that's what is installed\non the AppVeyor build agents. You might have to adjust this based on your\nenvironment.)\n\nBy calling releaseDocs.sh  we can update and publish our documentation in a\nsingle step.\n\nAutomatically publish the docs with AppVeyor\nWe can include the release script in our CI-pipeline, so that the documentation\ngets automatically updated on every build.\nIn this example I'll show how to do this with AppVeyor, but you can use the same\napproach with other CI-systems too.\n\nIn order to set up an AppVeyor build, we have to add an appveyor.yml  file to\nthe root of our repository, in which we'll describe the steps needed to build\nour project (and in our case publish the documentation).\n\nThe only slightly tricky part is giving AppVeyor permission to push changes to\nour repository.\nThe recommended way to do this is to create a Personal Access Token on GitHub.\nWe can follow this guide\n[https://help.github.com/articles/creating-a-personal-access-token-for-the-command-line/]\n, and the only permission we'll need is public_repo. (Make sure to copy the\naccess token to the clipboard after creating it.)\n\nWe should handle the access token as a password, so we should encrypt it before\nadding it to the AppVeyor config. We can do this on the AppVeyor site by\nclicking on our profile image, and in the menu selecting the \"Encrypt data\"\noption. Now we should add this to our yaml file as an environment variable:\n\nenvironment:\n  github_access_token:\n    secure: ceFfu552HHIV\n\n\nWe'll also need the email address we use as our GitHub account, and since it's\nbetter not to have it in plain text in our repository, encrypt this too and add\nit as an environment variable called github_email.\n\nThen we need to execute a couple of commands to actually use the access token\nwith git, and set up our email and user name for pushing (following this guide\n[https://www.appveyor.com/docs/how-to/git-push/]).\n\n- git config --global credential.helper store\n- ps: Add-Content \"$env:USERPROFILE\\.git-credentials\" \"https://$($env:github_access_token):x-oauth-basic@github.com`n\"\n- git config --global user.email %github_email%\n- git config --global user.name \"markvincze\"\n\n\nThe last thing to do in our deploy setup is to execute our releaseDocs.sh \nscript.\n\n- bash releaseDocs.sh\n\n\nThe full appveyor.yml  (also building the solution and running the unit tests)\nshould look like this:\n\nskip_tags: true\n\nimage: Visual Studio 2017\n\nclone_depth: 1\n\nbranches:\n  only:\n    - master\n\nenvironment:\n  github_access_token:\n    secure: ceFfu552HHkV\n  github_email:\n    secure: salKyeDwuc37\n\ninstall:\n  - cinst docfx\n\nbuild_script:\n- dotnet restore\n- dotnet build --configuration Release\n\ntest_script:\n- dotnet test test/MyProject.UnitTests/MyProject.UnitTests.csproj --configuration Release\n\ndeploy_script:\n- git config --global credential.helper store\n- ps: Add-Content \"$env:USERPROFILE\\.git-credentials\" \"https://$($env:github_access_token):x-oauth-basic@github.com`n\"\n- git config --global user.email %github_email%\n- git config --global user.name \"myusername\"\n- bash releaseDocs.sh\n\n\nWith this configuration in place, if we add our repository in AppVeyor, it will\nbuild our solution and publish the documentation on every new commit in the \nmaster  branch.\n\nSummary\nFollowing this guide we can easily set up automatically generated documentation\nand API reference for our open-source .NET Core libraries. (However, this post\nonly shows the basic capabilities of DocFx, it has plenty more features that are\nnot in the scope of this guide.)\n\nYou can find the source code of the full sample repository on GitHub\n[https://github.com/markvincze/dotnetcore-docfx-demo/], and take a look at the\npublished documentation here\n[https://markvincze.github.io/dotnetcore-docfx-demo/].",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": "Build and publish documentation for .NET Core projects with DocFx",
        "meta_description": "This guide shows how to generate and publish API documentation for a .NET Core library, using DocFx, GitHub Pages and AppVeyor.",
        "author_id": "1",
        "created_at": "2017-11-16 23:34:00",
        "created_by": "1",
        "updated_at": "2018-02-17 11:54:24",
        "updated_by": "1",
        "published_at": "2017-11-17 00:00:34",
        "published_by": "1",
        "custom_excerpt": "This guide shows how to build and publish API documentation for a .NET Core library, using DocFx, GitHub Pages and AppVeyor.",
        "codeinjection_head": "",
        "codeinjection_foot": "",
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "5a1b336219ac3b1e3f6dd297",
        "uuid": "1838d3bb-f9a9-428b-848e-070b934ec8fc",
        "title": "Introducing Code Fragments extension in Visual Studio Code for managing snippets during presentations",
        "slug": "introducing-code-fragments-extension-for-visual-studio-code",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Recently I started working on a simple Visual Studio Code extension I'd like to introduce in this post.\\n\\nOccasionally I do tech presentations, in which I usually like to do some live code demos. To make this smoother, Visual Studio has always had a really usefulalthough somewhat undocumentedfeature, which makes it possible to save snippets of code, and easily insert them during the presentation, to save some typing.  \\nWe can create such a \\\"snippet\\\" (not to be confused with actuala code snippets) by selecting a piece of code, and dropping it onto the *Toolbox* window.\\n\\nSince I started to focus more on .NET Core, I wanted to use Visual Studio **Code** instead of Visual Studio during my presentations, but it was a significant drawback for me that it didn't have this featurethere is no Toolbox window where pieces of code could be easily saved.  \\nI could've used actual code snippets, but then I'd have to remember the alias of the snippet during the demo, and also, creating a VSCode snippet is a bit too involved for this purpose. (The other alternative would be to just prepare all the demo code in a separate file, and copy paste the necessary chunks during the presentation. But I think we can all agree that this approach is way too clunky ).\\n\\nI decided to implement a VSCode extension for this specific purpose, which I called [**Code Fragments**](https://marketplace.visualstudio.com/items?itemName=markvincze.code-fragments). (I used the term fragment in order not to confuse these with actual code snippets.)\\n\\nI wanted to base the design on the experience the VS Toolbox provided, but I had to diverge due to the limitations of VSCode. Namely, I wasn't able to handle dragging and dropping a piece of code to the Explorer window, and also the other way around, items in the explorer tree views are not draggable.\\n\\nA piece of code can be saved as a fragment by selecting it in the editor, and then executing the \\\"Save selection as Code Fragment\\\" from the Command Palette (brought up with Ctrl+Shift+P) or in the context menu.\\nClicking on an existing fragment in the list inserts its content into the editor at the current cursor position.\\n\\nThis is what it looks like to save code fragments.\\n\\n![Saving a Code Fragment.](/content/images/2017/11/codefragments-save.gif)\\n\\nAnd inserting a Code Fragment to the current cursor position.\\n\\n![Inserting a Code Fragment](/content/images/2017/11/codefragments-insert.gif)\\n\\nThis really simple extension provides a very small set of functionality, but I think it's already useful for this specific purpose.  \\nI'd be happy if you gave it a try, and any feedback is welcome!\\n\\nYou can download the extension from the [Visual Studio Marketplace](https://marketplace.visualstudio.com/items?itemName=markvincze.code-fragments), and find the source code on [GitHub](https://github.com/markvincze/vscode-codeFragments).\"}]],\"sections\":[[10,0]]}",
        "html": "<p>Recently I started working on a simple Visual Studio Code extension I'd like to introduce in this post.</p>\n<p>Occasionally I do tech presentations, in which I usually like to do some live code demos. To make this smoother, Visual Studio has always had a really usefulalthough somewhat undocumentedfeature, which makes it possible to save snippets of code, and easily insert them during the presentation, to save some typing.<br>\nWe can create such a &quot;snippet&quot; (not to be confused with actuala code snippets) by selecting a piece of code, and dropping it onto the <em>Toolbox</em> window.</p>\n<p>Since I started to focus more on .NET Core, I wanted to use Visual Studio <strong>Code</strong> instead of Visual Studio during my presentations, but it was a significant drawback for me that it didn't have this featurethere is no Toolbox window where pieces of code could be easily saved.<br>\nI could've used actual code snippets, but then I'd have to remember the alias of the snippet during the demo, and also, creating a VSCode snippet is a bit too involved for this purpose. (The other alternative would be to just prepare all the demo code in a separate file, and copy paste the necessary chunks during the presentation. But I think we can all agree that this approach is way too clunky ).</p>\n<p>I decided to implement a VSCode extension for this specific purpose, which I called <a href=\"https://marketplace.visualstudio.com/items?itemName=markvincze.code-fragments\"><strong>Code Fragments</strong></a>. (I used the term fragment in order not to confuse these with actual code snippets.)</p>\n<p>I wanted to base the design on the experience the VS Toolbox provided, but I had to diverge due to the limitations of VSCode. Namely, I wasn't able to handle dragging and dropping a piece of code to the Explorer window, and also the other way around, items in the explorer tree views are not draggable.</p>\n<p>A piece of code can be saved as a fragment by selecting it in the editor, and then executing the &quot;Save selection as Code Fragment&quot; from the Command Palette (brought up with Ctrl+Shift+P) or in the context menu.<br>\nClicking on an existing fragment in the list inserts its content into the editor at the current cursor position.</p>\n<p>This is what it looks like to save code fragments.</p>\n<p><img src=\"/content/images/2017/11/codefragments-save.gif\" alt=\"Saving a Code Fragment.\"></p>\n<p>And inserting a Code Fragment to the current cursor position.</p>\n<p><img src=\"/content/images/2017/11/codefragments-insert.gif\" alt=\"Inserting a Code Fragment\"></p>\n<p>This really simple extension provides a very small set of functionality, but I think it's already useful for this specific purpose.<br>\nI'd be happy if you gave it a try, and any feedback is welcome!</p>\n<p>You can download the extension from the <a href=\"https://marketplace.visualstudio.com/items?itemName=markvincze.code-fragments\">Visual Studio Marketplace</a>, and find the source code on <a href=\"https://github.com/markvincze/vscode-codeFragments\">GitHub</a>.</p>\n",
        "comment_id": "5a1b336219ac3b1e3f6dd297",
        "plaintext": "Recently I started working on a simple Visual Studio Code extension I'd like to\nintroduce in this post.\n\nOccasionally I do tech presentations, in which I usually like to do some live\ncode demos. To make this smoother, Visual Studio has always had a really\nusefulalthough somewhat undocumentedfeature, which makes it possible to save\nsnippets of code, and easily insert them during the presentation, to save some\ntyping.\nWe can create such a \"snippet\" (not to be confused with actuala code snippets)\nby selecting a piece of code, and dropping it onto the Toolbox  window.\n\nSince I started to focus more on .NET Core, I wanted to use Visual Studio Code \ninstead of Visual Studio during my presentations, but it was a significant\ndrawback for me that it didn't have this featurethere is no Toolbox window\nwhere pieces of code could be easily saved.\nI could've used actual code snippets, but then I'd have to remember the alias of\nthe snippet during the demo, and also, creating a VSCode snippet is a bit too\ninvolved for this purpose. (The other alternative would be to just prepare all\nthe demo code in a separate file, and copy paste the necessary chunks during the\npresentation. But I think we can all agree that this approach is way too clunky\n).\n\nI decided to implement a VSCode extension for this specific purpose, which I\ncalled Code Fragments\n[https://marketplace.visualstudio.com/items?itemName=markvincze.code-fragments].\n(I used the term fragment in order not to confuse these with actual code\nsnippets.)\n\nI wanted to base the design on the experience the VS Toolbox provided, but I had\nto diverge due to the limitations of VSCode. Namely, I wasn't able to handle\ndragging and dropping a piece of code to the Explorer window, and also the other\nway around, items in the explorer tree views are not draggable.\n\nA piece of code can be saved as a fragment by selecting it in the editor, and\nthen executing the \"Save selection as Code Fragment\" from the Command Palette\n(brought up with Ctrl+Shift+P) or in the context menu.\nClicking on an existing fragment in the list inserts its content into the editor\nat the current cursor position.\n\nThis is what it looks like to save code fragments.\n\n\n\nAnd inserting a Code Fragment to the current cursor position.\n\n\n\nThis really simple extension provides a very small set of functionality, but I\nthink it's already useful for this specific purpose.\nI'd be happy if you gave it a try, and any feedback is welcome!\n\nYou can download the extension from the Visual Studio Marketplace\n[https://marketplace.visualstudio.com/items?itemName=markvincze.code-fragments],\nand find the source code on GitHub\n[https://github.com/markvincze/vscode-codeFragments].",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": "Code Fragments extension in VSCode for code demos in presentations",
        "meta_description": "Introducing a simple Visual Studio Code extension for saving pieces of code, and later easily insert them into source files, intended for coding demos.",
        "author_id": "1",
        "created_at": "2017-11-26 21:34:26",
        "created_by": "1",
        "updated_at": "2017-11-26 22:34:56",
        "updated_by": "1",
        "published_at": "2017-11-26 22:34:50",
        "published_by": "1",
        "custom_excerpt": null,
        "codeinjection_head": "",
        "codeinjection_foot": "",
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "5b422ffe19ac3b1e3f6dd2b1",
        "uuid": "edc25932-cdeb-4dfb-be29-dd468eb38106",
        "title": "Automated, portable code style checking in .NET Core projects",
        "slug": "automated-portable-code-style-checking-in-net-core-projects",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I haven't been using automated code style checking in .NET before. I sporadically experimented with StyleCop, FxCop, or the code style rules of ReSharper, but never ended up using them extensively, or introducing and distributing a maintained configuration in the organization I was working in.\\n\\nRecently having worked on JavaScript and TypeScript projects, I really started to appreciate how straightforward and established the process of linting is in those communities: it seems to be almost universal that every project in the JS and TS ecosystem is using `eslint` and `tslint` respectively, and the way to specify the linting rules is very straightforward.\\n\\nSo I decided to take a look at the options available today to do linting for .NET Core (mainly C#) projects. My preferences were the following.\\n\\n - Be able to customize the linting rules, and distribute that configuration to be used in every project in our organization.\\n - Preferably have integration for both Visual Studio and Visual Studio Code.\\n - Besides IDE integration, be able to also execute it in our CI-builds, and make the build fail if there are linting errors.\\n - Related to the above, be able to run it in the terminal not just on Windows, but also on Linux and Mac.\\n\\nReSharper provides the linting capability and the distributable configuration, but the IDE-integration is only there if you're using the ReSharper extension in Visual Studio, and it doesn't work in VSCode. Because of this I ruled ReSharper out. It can still be a viable option if everyone on your project is using VS with ReSharper, but I wanted to choose method that I can use both at work and in my open source projects.\\n\\nOther than ReSharper, the two tools I found were [StyleCopAnalyzers](https://github.com/DotNetAnalyzers/StyleCopAnalyzers), and [editorconfig](https://docs.microsoft.com/en-us/visualstudio/ide/create-portable-custom-editor-options).\\n\\nI could not manage to set up editorconfig in a way that it was reliably working, on the other hand, StyleCopAnalyzers was working fairly well, although not yet delivering all the above points.  \\nThe rest of the post will be about setting up and configuring StyleCopAnalyzers for your project, and in the end I'll include some details about my experience with editorconfig too.\\n\\nTo be able to test these tools, I created a sample project where I intentionally made some code style violations that I would want the linters to warn about. I uploaded the code [here](https://github.com/markvincze/CodeStyleCheckSample), where I created two branches, [`stylecopanalyzers`](https://github.com/markvincze/CodeStyleCheckSample/tree/stylecopanalyzers) and [`editorconfig`](https://github.com/markvincze/CodeStyleCheckSample/tree/editorconfig) for the setup of the two tools.\\n\\n# Adding StyleCopAnalyzers to your project\\n\\nIncluding StyleCopAnalyzers in your project is very simple, it's basically just adding the reference to its Nuget package. You can do this from the terminal with `dotnet`.\\n\\n```\\ndotnet add package StyleCop.Analyzers\\n```\\n\\nAfter this, if you try to build your project, you'll immediately receive the violations in the form of compiler warnings.\\n\\n```\\n$ dotnet build\\nMicrosoft (R) Build Engine version 15.7.179.6572 for .NET Core\\nCopyright (C) Microsoft Corporation. All rights reserved.\\n\\n  Restore completed in 33.08 ms for C:\\\\Workspaces\\\\Github\\\\CodeStyleCheckSample\\\\src\\\\CodeStyleCheckSample\\\\CodeStyleCheckSample.csproj.\\nMyClass.cs(20,13): warning SA1000: The keyword 'if' must be followed by a space. [C:\\\\Workspaces\\\\Github\\\\CodeStyleCheckSample\\\\src\\\\CodeStyleCheckSample\\\\CodeStyleCheckSample.csproj]\\nMyClass.cs(8,23): warning SA1401: Field must be private [C:\\\\Workspaces\\\\Github\\\\CodeStyleCheckSample\\\\src\\\\CodeStyleCheckSample\\\\CodeStyleCheckSample.csproj]\\n...\\n```\\n\\nOr, if we add the `<TreatWarningsAsErrors>true</TreatWarningsAsErrors>` flag to our project file, these will be actual errors instead of just warnings. This is what I would generally suggest to do from the start, otherwise it's very easy to pile up hundreds or thousands of warnings in a large code base, which is daunting to clean up later. (This can typically be the situation when we want to introduce linting to an existing legacy project.)\\n\\nThe linting producing errors by the `dotnet build` command makes it very easy to wire this into our CI-build, since the build errors will make our CI-build fail too, so we don't need to do anything else.\\n\\nThe errors also nicely show up in Visual Studio, where we can even quickly fix them with Roslyn quick fixes.\\n\\n![Linting errors showing up in Visual Studio](/content/images/2018/07/error-in-vs.png)\\n\\nUnfortunately, at this moment, Visual Studio Code does not support displaying the errors coming from Roslyn analyzers, as mentioned in [this comment](https://github.com/DotNetAnalyzers/StyleCopAnalyzers/issues/2739#issuecomment-403258345). Thus StyleCopAnalyzers today is the most ideal if we are using Visual Studio. If we're using VSCode, we'll have to rely on the build errors produced by `dotnet build` instead of the IDE-integration.\\n\\n# Customizing the linting rules\\n\\nWhen using `eslint` and `tslint`, we can customize the linting rules by creating an `.eslintrc` or `tslint.json` file, respectively, where we can change both the violation severity, and also customize how some of the rules work (for example whether to enforce single quotes, double quotes, or accept both for string literals).\\n\\nWe can do similar customization with StyleCopAnalyzers, but the situation is a bit more complicated. There are two different configuration files we can use for different purposes.\\n\\n## Turn rules on and off\\n\\nIn order to select which rules we even want to validate in the first place, we have to use a **Code analysis rule set file**, which has a format utilized by other analyzers, for example the built-in analyer in VS.  \\nWe can turn off some of the rules we don't need by adding the following XML file (we can choose its name) to the project.\\n\\n```xml\\n<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?>\\n<RuleSet Name=\\\"Sample rule set\\\" Description=\\\"Sample rule set illustrating customizing StyleCopAnalyzer rule severity\\\" ToolsVersion=\\\"14.0\\\">\\n  <Rules AnalyzerId=\\\"StyleCop.Analyzers\\\" RuleNamespace=\\\"StyleCop.Analyzers\\\">\\n    <Rule Id=\\\"SA1200\\\" Action=\\\"None\\\" />\\n    <Rule Id=\\\"SA1633\\\" Action=\\\"None\\\" />\\n  </Rules>\\n</RuleSet>\\n```\\n\\nIn the `<Rule>` element we have to specify the ID of the specific linting rule, and set the `Action` property to either `None`, `Info`, `Warning` or `Error`. To figure out the codes of the individual rules, we can find them in the output we get from the `dotnet` CLI.  \\nAnd we have to specify the name of our file in the `CodeAnalysisRuleSet` property in the project file.\\n\\n```xml\\n    <CodeAnalysisRuleSet>SampleRuleSet.ruleset</CodeAnalysisRuleSet>\\n```\\n\\n\\n(One strange thing about the ruleset file is that if I try to open it in VS, instead of opening the XML in the text editor, it opens the built-in GUI for editing `ruleset` files, which simply doesn't work for me at all, it doesn't display any of the rules. However, I prefer editing the XML content by hand anyway, so I didn't investigate further about this problem.)\\n\\n## Further customize certain rules\\n\\nIn order to fine-tune the behavior of certain rules, we can add a `stylecop.json` file to our project, and [add the following item](https://github.com/DotNetAnalyzers/StyleCopAnalyzers/blob/master/documentation/EnableConfiguration.md) to the project file:\\n\\n```xml\\n  <ItemGroup>\\n    <AdditionalFiles Include=\\\"stylecop.json\\\" />\\n  </ItemGroup>\\n```\\n\\nThis is a small example, in which I'm forbidding having a new line at the end of our source files.\\n\\n```json\\n{\\n  \\\"$schema\\\": \\\"https://raw.githubusercontent.com/DotNetAnalyzers/StyleCopAnalyzers/master/StyleCop.Analyzers/StyleCop.Analyzers/Settings/stylecop.schema.json\\\",\\n  \\\"settings\\\": {\\n    \\\"layoutRules\\\": {\\n      \\\"newlineAtEndOfFile\\\": \\\"omit\\\"\\n    }\\n  }\\n}\\n```\\n\\n![Rule customized with stylecop.json showing up in Visual Studio](/content/images/2018/07/stylecop-error-in-vs.png)\\n\\nYou can read about all the possible customization on [this page](https://github.com/DotNetAnalyzers/StyleCopAnalyzers/blob/master/documentation/Configuration.md). The set of rules supported today feels a bit ad-hoc and limited, but since the project is being actively developed, I expect this to improve in the future.\\n\\n## Distributing our custom configuration\\n\\nIf we're working on multiple projects in an organization, it's important to be able to distribute our custom rules to all of the projects we're working on, to keep the linting rules consistent. The naive way to do this would be to maintain our `.ruleset` and `stylecop.json` files in a central place, for example a git repository, and then from time to time, copy the two files from this repo to our actual projects. Although this approach is doable, it's not ideal. Copy pasting the files can be a bit tedious, and the versioning of our configuration will be unclear.\\n\\nA better way is to create a custom Nuget package, which references `StyleCop.Analyzers`, and also makes the projects depending on it including our custom configuration files in the correct way. This way we can use proper versioning for our linting configuration, and distribute it to the dependant projects as a Nuget dependency.\\n\\nIn order to do this properly, we need to set up a `.nuspec` and a `.props` file in our package, as it is described [in the documentation](https://github.com/DotNetAnalyzers/StyleCopAnalyzers/blob/master/documentation/Configuration.md#sharing-configuration-among-solutions).\\n\\nYou can find a working example of this setup in [this repo](https://github.com/markvincze/MyCustomStyleCopAnalyzerPackage).\\n\\n# What about editorconfig?\\n\\nWatching the [.NET Roadmap talk](https://channel9.msdn.com/Events/Build/2018/BRK2100) from the Build conference I learned thatcontrary to what I thought before`editorconfig` is not only capable of controlling basic, language-agnostic properties of our source files, such as indentation, newline at the end of a file, trimming trailing whitespace, etc., but with Visual Studio, it also supports enforcing actual C#-specific linting rules, such as qualifying fields with `this.`, sorting the user directives, etc.\\n\\nLooking at some of the projects in the JS ecosystem ([react](https://github.com/facebook/react), [vue](https://github.com/vuejs/vue), [yarn](https://github.com/yarnpkg/yarn), just to name a few) it seems pretty universal that the JS-specific linting is done through `eslint`, and editorconfig is used only to control a handful of low-level properties of the source files. So `eslint` and `editorconfig` are complementary.\\n\\nOn the other hand, now in the .NET ecosystem it seems that `StyleCopAnalyzers` and the C#-specific `editorconfig` integration in Visual Studio aren't really complementary, but they are rather competing alternatives, which seems to be a strange situation to me, since both have some shortcomings at the moment. Although I haven't heard any \\\"official\\\" information about how this will play out, so I might be misunderstanding the situation.\\n\\nNevertheless, I gave `editorconfig` a try too (you can see it in the [`editorconfig`](https://github.com/markvincze/CodeStyleCheckSample/tree/editorconfig) branch of the sample repo), but couldn't get it to work reliably. It doesn't seem to integrate into VSCode at all, and also in Visual Studio for me only some of the rules were working, some others were simply not triggered.  \\nAnd my biggest issue was that although VS picks up the `editorconfig` rules, anddepending on the configurationit might show them as Errors, but it doesn't make the actual build fail, so I couldn't find a way to also integrate into the CI-worklow.\\n\\n# Conclusion\\n\\nBased on the above I'd say that right now the preferable way to implement portable, automated code style checking for a .NET Core project is to use `StyleCopAnalyzers`, even if it doesn't directly support VSCode.\\n\\nI'm really curious how this area is going to improve in the future, and especially the role that `StyleCopAnalyzers` and `editorconfig` are going to play in linting .NET Core projects.  \\nFor me the sweet spot would be if we had one unified way of doing linting for .NET Core, with a single configuration file, VSCode support, and maybe with even core integration into the `dotnet` CLI as a separate `lint` command, so that we could run it separately from our build.\"}]],\"sections\":[[10,0]]}",
        "html": "<p>I haven't been using automated code style checking in .NET before. I sporadically experimented with StyleCop, FxCop, or the code style rules of ReSharper, but never ended up using them extensively, or introducing and distributing a maintained configuration in the organization I was working in.</p>\n<p>Recently having worked on JavaScript and TypeScript projects, I really started to appreciate how straightforward and established the process of linting is in those communities: it seems to be almost universal that every project in the JS and TS ecosystem is using <code>eslint</code> and <code>tslint</code> respectively, and the way to specify the linting rules is very straightforward.</p>\n<p>So I decided to take a look at the options available today to do linting for .NET Core (mainly C#) projects. My preferences were the following.</p>\n<ul>\n<li>Be able to customize the linting rules, and distribute that configuration to be used in every project in our organization.</li>\n<li>Preferably have integration for both Visual Studio and Visual Studio Code.</li>\n<li>Besides IDE integration, be able to also execute it in our CI-builds, and make the build fail if there are linting errors.</li>\n<li>Related to the above, be able to run it in the terminal not just on Windows, but also on Linux and Mac.</li>\n</ul>\n<p>ReSharper provides the linting capability and the distributable configuration, but the IDE-integration is only there if you're using the ReSharper extension in Visual Studio, and it doesn't work in VSCode. Because of this I ruled ReSharper out. It can still be a viable option if everyone on your project is using VS with ReSharper, but I wanted to choose method that I can use both at work and in my open source projects.</p>\n<p>Other than ReSharper, the two tools I found were <a href=\"https://github.com/DotNetAnalyzers/StyleCopAnalyzers\">StyleCopAnalyzers</a>, and <a href=\"https://docs.microsoft.com/en-us/visualstudio/ide/create-portable-custom-editor-options\">editorconfig</a>.</p>\n<p>I could not manage to set up editorconfig in a way that it was reliably working, on the other hand, StyleCopAnalyzers was working fairly well, although not yet delivering all the above points.<br>\nThe rest of the post will be about setting up and configuring StyleCopAnalyzers for your project, and in the end I'll include some details about my experience with editorconfig too.</p>\n<p>To be able to test these tools, I created a sample project where I intentionally made some code style violations that I would want the linters to warn about. I uploaded the code <a href=\"https://github.com/markvincze/CodeStyleCheckSample\">here</a>, where I created two branches, <a href=\"https://github.com/markvincze/CodeStyleCheckSample/tree/stylecopanalyzers\"><code>stylecopanalyzers</code></a> and <a href=\"https://github.com/markvincze/CodeStyleCheckSample/tree/editorconfig\"><code>editorconfig</code></a> for the setup of the two tools.</p>\n<h1 id=\"addingstylecopanalyzerstoyourproject\">Adding StyleCopAnalyzers to your project</h1>\n<p>Including StyleCopAnalyzers in your project is very simple, it's basically just adding the reference to its Nuget package. You can do this from the terminal with <code>dotnet</code>.</p>\n<pre><code>dotnet add package StyleCop.Analyzers\n</code></pre>\n<p>After this, if you try to build your project, you'll immediately receive the violations in the form of compiler warnings.</p>\n<pre><code>$ dotnet build\nMicrosoft (R) Build Engine version 15.7.179.6572 for .NET Core\nCopyright (C) Microsoft Corporation. All rights reserved.\n\n  Restore completed in 33.08 ms for C:\\Workspaces\\Github\\CodeStyleCheckSample\\src\\CodeStyleCheckSample\\CodeStyleCheckSample.csproj.\nMyClass.cs(20,13): warning SA1000: The keyword 'if' must be followed by a space. [C:\\Workspaces\\Github\\CodeStyleCheckSample\\src\\CodeStyleCheckSample\\CodeStyleCheckSample.csproj]\nMyClass.cs(8,23): warning SA1401: Field must be private [C:\\Workspaces\\Github\\CodeStyleCheckSample\\src\\CodeStyleCheckSample\\CodeStyleCheckSample.csproj]\n...\n</code></pre>\n<p>Or, if we add the <code>&lt;TreatWarningsAsErrors&gt;true&lt;/TreatWarningsAsErrors&gt;</code> flag to our project file, these will be actual errors instead of just warnings. This is what I would generally suggest to do from the start, otherwise it's very easy to pile up hundreds or thousands of warnings in a large code base, which is daunting to clean up later. (This can typically be the situation when we want to introduce linting to an existing legacy project.)</p>\n<p>The linting producing errors by the <code>dotnet build</code> command makes it very easy to wire this into our CI-build, since the build errors will make our CI-build fail too, so we don't need to do anything else.</p>\n<p>The errors also nicely show up in Visual Studio, where we can even quickly fix them with Roslyn quick fixes.</p>\n<p><img src=\"/content/images/2018/07/error-in-vs.png\" alt=\"Linting errors showing up in Visual Studio\"></p>\n<p>Unfortunately, at this moment, Visual Studio Code does not support displaying the errors coming from Roslyn analyzers, as mentioned in <a href=\"https://github.com/DotNetAnalyzers/StyleCopAnalyzers/issues/2739#issuecomment-403258345\">this comment</a>. Thus StyleCopAnalyzers today is the most ideal if we are using Visual Studio. If we're using VSCode, we'll have to rely on the build errors produced by <code>dotnet build</code> instead of the IDE-integration.</p>\n<h1 id=\"customizingthelintingrules\">Customizing the linting rules</h1>\n<p>When using <code>eslint</code> and <code>tslint</code>, we can customize the linting rules by creating an <code>.eslintrc</code> or <code>tslint.json</code> file, respectively, where we can change both the violation severity, and also customize how some of the rules work (for example whether to enforce single quotes, double quotes, or accept both for string literals).</p>\n<p>We can do similar customization with StyleCopAnalyzers, but the situation is a bit more complicated. There are two different configuration files we can use for different purposes.</p>\n<h2 id=\"turnrulesonandoff\">Turn rules on and off</h2>\n<p>In order to select which rules we even want to validate in the first place, we have to use a <strong>Code analysis rule set file</strong>, which has a format utilized by other analyzers, for example the built-in analyer in VS.<br>\nWe can turn off some of the rules we don't need by adding the following XML file (we can choose its name) to the project.</p>\n<pre><code class=\"language-xml\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;\n&lt;RuleSet Name=&quot;Sample rule set&quot; Description=&quot;Sample rule set illustrating customizing StyleCopAnalyzer rule severity&quot; ToolsVersion=&quot;14.0&quot;&gt;\n  &lt;Rules AnalyzerId=&quot;StyleCop.Analyzers&quot; RuleNamespace=&quot;StyleCop.Analyzers&quot;&gt;\n    &lt;Rule Id=&quot;SA1200&quot; Action=&quot;None&quot; /&gt;\n    &lt;Rule Id=&quot;SA1633&quot; Action=&quot;None&quot; /&gt;\n  &lt;/Rules&gt;\n&lt;/RuleSet&gt;\n</code></pre>\n<p>In the <code>&lt;Rule&gt;</code> element we have to specify the ID of the specific linting rule, and set the <code>Action</code> property to either <code>None</code>, <code>Info</code>, <code>Warning</code> or <code>Error</code>. To figure out the codes of the individual rules, we can find them in the output we get from the <code>dotnet</code> CLI.<br>\nAnd we have to specify the name of our file in the <code>CodeAnalysisRuleSet</code> property in the project file.</p>\n<pre><code class=\"language-xml\">    &lt;CodeAnalysisRuleSet&gt;SampleRuleSet.ruleset&lt;/CodeAnalysisRuleSet&gt;\n</code></pre>\n<p>(One strange thing about the ruleset file is that if I try to open it in VS, instead of opening the XML in the text editor, it opens the built-in GUI for editing <code>ruleset</code> files, which simply doesn't work for me at all, it doesn't display any of the rules. However, I prefer editing the XML content by hand anyway, so I didn't investigate further about this problem.)</p>\n<h2 id=\"furthercustomizecertainrules\">Further customize certain rules</h2>\n<p>In order to fine-tune the behavior of certain rules, we can add a <code>stylecop.json</code> file to our project, and <a href=\"https://github.com/DotNetAnalyzers/StyleCopAnalyzers/blob/master/documentation/EnableConfiguration.md\">add the following item</a> to the project file:</p>\n<pre><code class=\"language-xml\">  &lt;ItemGroup&gt;\n    &lt;AdditionalFiles Include=&quot;stylecop.json&quot; /&gt;\n  &lt;/ItemGroup&gt;\n</code></pre>\n<p>This is a small example, in which I'm forbidding having a new line at the end of our source files.</p>\n<pre><code class=\"language-json\">{\n  &quot;$schema&quot;: &quot;https://raw.githubusercontent.com/DotNetAnalyzers/StyleCopAnalyzers/master/StyleCop.Analyzers/StyleCop.Analyzers/Settings/stylecop.schema.json&quot;,\n  &quot;settings&quot;: {\n    &quot;layoutRules&quot;: {\n      &quot;newlineAtEndOfFile&quot;: &quot;omit&quot;\n    }\n  }\n}\n</code></pre>\n<p><img src=\"/content/images/2018/07/stylecop-error-in-vs.png\" alt=\"Rule customized with stylecop.json showing up in Visual Studio\"></p>\n<p>You can read about all the possible customization on <a href=\"https://github.com/DotNetAnalyzers/StyleCopAnalyzers/blob/master/documentation/Configuration.md\">this page</a>. The set of rules supported today feels a bit ad-hoc and limited, but since the project is being actively developed, I expect this to improve in the future.</p>\n<h2 id=\"distributingourcustomconfiguration\">Distributing our custom configuration</h2>\n<p>If we're working on multiple projects in an organization, it's important to be able to distribute our custom rules to all of the projects we're working on, to keep the linting rules consistent. The naive way to do this would be to maintain our <code>.ruleset</code> and <code>stylecop.json</code> files in a central place, for example a git repository, and then from time to time, copy the two files from this repo to our actual projects. Although this approach is doable, it's not ideal. Copy pasting the files can be a bit tedious, and the versioning of our configuration will be unclear.</p>\n<p>A better way is to create a custom Nuget package, which references <code>StyleCop.Analyzers</code>, and also makes the projects depending on it including our custom configuration files in the correct way. This way we can use proper versioning for our linting configuration, and distribute it to the dependant projects as a Nuget dependency.</p>\n<p>In order to do this properly, we need to set up a <code>.nuspec</code> and a <code>.props</code> file in our package, as it is described <a href=\"https://github.com/DotNetAnalyzers/StyleCopAnalyzers/blob/master/documentation/Configuration.md#sharing-configuration-among-solutions\">in the documentation</a>.</p>\n<p>You can find a working example of this setup in <a href=\"https://github.com/markvincze/MyCustomStyleCopAnalyzerPackage\">this repo</a>.</p>\n<h1 id=\"whatabouteditorconfig\">What about editorconfig?</h1>\n<p>Watching the <a href=\"https://channel9.msdn.com/Events/Build/2018/BRK2100\">.NET Roadmap talk</a> from the Build conference I learned thatcontrary to what I thought before<code>editorconfig</code> is not only capable of controlling basic, language-agnostic properties of our source files, such as indentation, newline at the end of a file, trimming trailing whitespace, etc., but with Visual Studio, it also supports enforcing actual C#-specific linting rules, such as qualifying fields with <code>this.</code>, sorting the user directives, etc.</p>\n<p>Looking at some of the projects in the JS ecosystem (<a href=\"https://github.com/facebook/react\">react</a>, <a href=\"https://github.com/vuejs/vue\">vue</a>, <a href=\"https://github.com/yarnpkg/yarn\">yarn</a>, just to name a few) it seems pretty universal that the JS-specific linting is done through <code>eslint</code>, and editorconfig is used only to control a handful of low-level properties of the source files. So <code>eslint</code> and <code>editorconfig</code> are complementary.</p>\n<p>On the other hand, now in the .NET ecosystem it seems that <code>StyleCopAnalyzers</code> and the C#-specific <code>editorconfig</code> integration in Visual Studio aren't really complementary, but they are rather competing alternatives, which seems to be a strange situation to me, since both have some shortcomings at the moment. Although I haven't heard any &quot;official&quot; information about how this will play out, so I might be misunderstanding the situation.</p>\n<p>Nevertheless, I gave <code>editorconfig</code> a try too (you can see it in the <a href=\"https://github.com/markvincze/CodeStyleCheckSample/tree/editorconfig\"><code>editorconfig</code></a> branch of the sample repo), but couldn't get it to work reliably. It doesn't seem to integrate into VSCode at all, and also in Visual Studio for me only some of the rules were working, some others were simply not triggered.<br>\nAnd my biggest issue was that although VS picks up the <code>editorconfig</code> rules, anddepending on the configurationit might show them as Errors, but it doesn't make the actual build fail, so I couldn't find a way to also integrate into the CI-worklow.</p>\n<h1 id=\"conclusion\">Conclusion</h1>\n<p>Based on the above I'd say that right now the preferable way to implement portable, automated code style checking for a .NET Core project is to use <code>StyleCopAnalyzers</code>, even if it doesn't directly support VSCode.</p>\n<p>I'm really curious how this area is going to improve in the future, and especially the role that <code>StyleCopAnalyzers</code> and <code>editorconfig</code> are going to play in linting .NET Core projects.<br>\nFor me the sweet spot would be if we had one unified way of doing linting for .NET Core, with a single configuration file, VSCode support, and maybe with even core integration into the <code>dotnet</code> CLI as a separate <code>lint</code> command, so that we could run it separately from our build.</p>\n",
        "comment_id": "5b422ffe19ac3b1e3f6dd2b1",
        "plaintext": "I haven't been using automated code style checking in .NET before. I\nsporadically experimented with StyleCop, FxCop, or the code style rules of\nReSharper, but never ended up using them extensively, or introducing and\ndistributing a maintained configuration in the organization I was working in.\n\nRecently having worked on JavaScript and TypeScript projects, I really started\nto appreciate how straightforward and established the process of linting is in\nthose communities: it seems to be almost universal that every project in the JS\nand TS ecosystem is using eslint  and tslint  respectively, and the way to\nspecify the linting rules is very straightforward.\n\nSo I decided to take a look at the options available today to do linting for\n.NET Core (mainly C#) projects. My preferences were the following.\n\n * Be able to customize the linting rules, and distribute that configuration to\n   be used in every project in our organization.\n * Preferably have integration for both Visual Studio and Visual Studio Code.\n * Besides IDE integration, be able to also execute it in our CI-builds, and\n   make the build fail if there are linting errors.\n * Related to the above, be able to run it in the terminal not just on Windows,\n   but also on Linux and Mac.\n\nReSharper provides the linting capability and the distributable configuration,\nbut the IDE-integration is only there if you're using the ReSharper extension in\nVisual Studio, and it doesn't work in VSCode. Because of this I ruled ReSharper\nout. It can still be a viable option if everyone on your project is using VS\nwith ReSharper, but I wanted to choose method that I can use both at work and in\nmy open source projects.\n\nOther than ReSharper, the two tools I found were StyleCopAnalyzers\n[https://github.com/DotNetAnalyzers/StyleCopAnalyzers], and editorconfig\n[https://docs.microsoft.com/en-us/visualstudio/ide/create-portable-custom-editor-options]\n.\n\nI could not manage to set up editorconfig in a way that it was reliably working,\non the other hand, StyleCopAnalyzers was working fairly well, although not yet\ndelivering all the above points.\nThe rest of the post will be about setting up and configuring StyleCopAnalyzers\nfor your project, and in the end I'll include some details about my experience\nwith editorconfig too.\n\nTo be able to test these tools, I created a sample project where I intentionally\nmade some code style violations that I would want the linters to warn about. I\nuploaded the code here [https://github.com/markvincze/CodeStyleCheckSample],\nwhere I created two branches, stylecopanalyzers\n[https://github.com/markvincze/CodeStyleCheckSample/tree/stylecopanalyzers]  and\n editorconfig\n[https://github.com/markvincze/CodeStyleCheckSample/tree/editorconfig]  for the\nsetup of the two tools.\n\nAdding StyleCopAnalyzers to your project\nIncluding StyleCopAnalyzers in your project is very simple, it's basically just\nadding the reference to its Nuget package. You can do this from the terminal\nwith dotnet.\n\ndotnet add package StyleCop.Analyzers\n\n\nAfter this, if you try to build your project, you'll immediately receive the\nviolations in the form of compiler warnings.\n\n$ dotnet build\nMicrosoft (R) Build Engine version 15.7.179.6572 for .NET Core\nCopyright (C) Microsoft Corporation. All rights reserved.\n\n  Restore completed in 33.08 ms for C:\\Workspaces\\Github\\CodeStyleCheckSample\\src\\CodeStyleCheckSample\\CodeStyleCheckSample.csproj.\nMyClass.cs(20,13): warning SA1000: The keyword 'if' must be followed by a space. [C:\\Workspaces\\Github\\CodeStyleCheckSample\\src\\CodeStyleCheckSample\\CodeStyleCheckSample.csproj]\nMyClass.cs(8,23): warning SA1401: Field must be private [C:\\Workspaces\\Github\\CodeStyleCheckSample\\src\\CodeStyleCheckSample\\CodeStyleCheckSample.csproj]\n...\n\n\nOr, if we add the <TreatWarningsAsErrors>true</TreatWarningsAsErrors>  flag to\nour project file, these will be actual errors instead of just warnings. This is\nwhat I would generally suggest to do from the start, otherwise it's very easy to\npile up hundreds or thousands of warnings in a large code base, which is\ndaunting to clean up later. (This can typically be the situation when we want to\nintroduce linting to an existing legacy project.)\n\nThe linting producing errors by the dotnet build  command makes it very easy to\nwire this into our CI-build, since the build errors will make our CI-build fail\ntoo, so we don't need to do anything else.\n\nThe errors also nicely show up in Visual Studio, where we can even quickly fix\nthem with Roslyn quick fixes.\n\n\n\nUnfortunately, at this moment, Visual Studio Code does not support displaying\nthe errors coming from Roslyn analyzers, as mentioned in this comment. Thus\nStyleCopAnalyzers today is the most ideal if we are using Visual Studio. If\nwe're using VSCode, we'll have to rely on the build errors produced by dotnet\nbuild  instead of the IDE-integration.\n\nCustomizing the linting rules\nWhen using eslint  and tslint, we can customize the linting rules by creating an\n .eslintrc  or tslint.json  file, respectively, where we can change both the\nviolation severity, and also customize how some of the rules work (for example\nwhether to enforce single quotes, double quotes, or accept both for string\nliterals).\n\nWe can do similar customization with StyleCopAnalyzers, but the situation is a\nbit more complicated. There are two different configuration files we can use for\ndifferent purposes.\n\nTurn rules on and off\nIn order to select which rules we even want to validate in the first place, we\nhave to use a Code analysis rule set file, which has a format utilized by other\nanalyzers, for example the built-in analyer in VS.\nWe can turn off some of the rules we don't need by adding the following XML file\n(we can choose its name) to the project.\n\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<RuleSet Name=\"Sample rule set\" Description=\"Sample rule set illustrating customizing StyleCopAnalyzer rule severity\" ToolsVersion=\"14.0\">\n  <Rules AnalyzerId=\"StyleCop.Analyzers\" RuleNamespace=\"StyleCop.Analyzers\">\n    <Rule Id=\"SA1200\" Action=\"None\" />\n    <Rule Id=\"SA1633\" Action=\"None\" />\n  </Rules>\n</RuleSet>\n\n\nIn the <Rule>  element we have to specify the ID of the specific linting rule,\nand set the Action  property to either None, Info, Warning  or Error. To figure\nout the codes of the individual rules, we can find them in the output we get\nfrom the dotnet  CLI.\nAnd we have to specify the name of our file in the CodeAnalysisRuleSet  property\nin the project file.\n\n    <CodeAnalysisRuleSet>SampleRuleSet.ruleset</CodeAnalysisRuleSet>\n\n\n(One strange thing about the ruleset file is that if I try to open it in VS,\ninstead of opening the XML in the text editor, it opens the built-in GUI for\nediting ruleset  files, which simply doesn't work for me at all, it doesn't\ndisplay any of the rules. However, I prefer editing the XML content by hand\nanyway, so I didn't investigate further about this problem.)\n\nFurther customize certain rules\nIn order to fine-tune the behavior of certain rules, we can add a stylecop.json \nfile to our project, and add the following item\n[https://github.com/DotNetAnalyzers/StyleCopAnalyzers/blob/master/documentation/EnableConfiguration.md] \n to the project file:\n\n  <ItemGroup>\n    <AdditionalFiles Include=\"stylecop.json\" />\n  </ItemGroup>\n\n\nThis is a small example, in which I'm forbidding having a new line at the end of\nour source files.\n\n{\n  \"$schema\": \"https://raw.githubusercontent.com/DotNetAnalyzers/StyleCopAnalyzers/master/StyleCop.Analyzers/StyleCop.Analyzers/Settings/stylecop.schema.json\",\n  \"settings\": {\n    \"layoutRules\": {\n      \"newlineAtEndOfFile\": \"omit\"\n    }\n  }\n}\n\n\n\n\nYou can read about all the possible customization on this page\n[https://github.com/DotNetAnalyzers/StyleCopAnalyzers/blob/master/documentation/Configuration.md]\n. The set of rules supported today feels a bit ad-hoc and limited, but since the\nproject is being actively developed, I expect this to improve in the future.\n\nDistributing our custom configuration\nIf we're working on multiple projects in an organization, it's important to be\nable to distribute our custom rules to all of the projects we're working on, to\nkeep the linting rules consistent. The naive way to do this would be to maintain\nour .ruleset  and stylecop.json  files in a central place, for example a git\nrepository, and then from time to time, copy the two files from this repo to our\nactual projects. Although this approach is doable, it's not ideal. Copy pasting\nthe files can be a bit tedious, and the versioning of our configuration will be\nunclear.\n\nA better way is to create a custom Nuget package, which references \nStyleCop.Analyzers, and also makes the projects depending on it including our\ncustom configuration files in the correct way. This way we can use proper\nversioning for our linting configuration, and distribute it to the dependant\nprojects as a Nuget dependency.\n\nIn order to do this properly, we need to set up a .nuspec  and a .props  file in\nour package, as it is described in the documentation.\n\nYou can find a working example of this setup in this repo\n[https://github.com/markvincze/MyCustomStyleCopAnalyzerPackage].\n\nWhat about editorconfig?\nWatching the .NET Roadmap talk\n[https://channel9.msdn.com/Events/Build/2018/BRK2100]  from the Build conference\nI learned thatcontrary to what I thought beforeeditorconfig  is not only\ncapable of controlling basic, language-agnostic properties of our source files,\nsuch as indentation, newline at the end of a file, trimming trailing whitespace,\netc., but with Visual Studio, it also supports enforcing actual C#-specific\nlinting rules, such as qualifying fields with this., sorting the user\ndirectives, etc.\n\nLooking at some of the projects in the JS ecosystem (react\n[https://github.com/facebook/react], vue [https://github.com/vuejs/vue], yarn\n[https://github.com/yarnpkg/yarn], just to name a few) it seems pretty universal\nthat the JS-specific linting is done through eslint, and editorconfig is used\nonly to control a handful of low-level properties of the source files. So eslint \n and editorconfig  are complementary.\n\nOn the other hand, now in the .NET ecosystem it seems that StyleCopAnalyzers \nand the C#-specific editorconfig  integration in Visual Studio aren't really\ncomplementary, but they are rather competing alternatives, which seems to be a\nstrange situation to me, since both have some shortcomings at the moment.\nAlthough I haven't heard any \"official\" information about how this will play\nout, so I might be misunderstanding the situation.\n\nNevertheless, I gave editorconfig  a try too (you can see it in the editorconfig\n[https://github.com/markvincze/CodeStyleCheckSample/tree/editorconfig]  branch\nof the sample repo), but couldn't get it to work reliably. It doesn't seem to\nintegrate into VSCode at all, and also in Visual Studio for me only some of the\nrules were working, some others were simply not triggered.\nAnd my biggest issue was that although VS picks up the editorconfig  rules,\nanddepending on the configurationit might show them as Errors, but it doesn't\nmake the actual build fail, so I couldn't find a way to also integrate into the\nCI-worklow.\n\nConclusion\nBased on the above I'd say that right now the preferable way to implement\nportable, automated code style checking for a .NET Core project is to use \nStyleCopAnalyzers, even if it doesn't directly support VSCode.\n\nI'm really curious how this area is going to improve in the future, and\nespecially the role that StyleCopAnalyzers  and editorconfig  are going to play\nin linting .NET Core projects.\nFor me the sweet spot would be if we had one unified way of doing linting for\n.NET Core, with a single configuration file, VSCode support, and maybe with even\ncore integration into the dotnet  CLI as a separate lint  command, so that we\ncould run it separately from our build.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": "Automated, portable code style checking in .NET Core projects",
        "meta_description": "A quick introduction to setting up automated code style checking for .NET Core projects with StyleCopAnalyzers and editorconfig.",
        "author_id": "1",
        "created_at": "2018-07-08 15:38:38",
        "created_by": "1",
        "updated_at": "2018-07-09 07:55:11",
        "updated_by": "1",
        "published_at": "2018-07-08 15:48:55",
        "published_by": "1",
        "custom_excerpt": "A quick introduction to setting up automated code style checking for .NET Core projects with StyleCopAnalyzers and editorconfig.",
        "codeinjection_head": "",
        "codeinjection_foot": "",
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "5b5c8545e67a0c102087288f",
        "uuid": "587de3ea-0855-4afb-8085-c7b8a8d77ec8",
        "title": "How to gracefully fall back to cache on 5xx responses with Varnish",
        "slug": "how-to-gracefully-fall-back-to-cache-on-5xx-responses-with-varnish",
        "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"# Introduction\\n\\nVarnish is a widely used reverse proxy and *HTTP accelerator*. It sits in front of an HTTP service, and caches the responses to improve the response times observed by the clients, and possibly to reduce the load on the upstream backend service.\\n\\nBesides bringing performance improvements, Varnish can be also useful for shielding the clients from any outage of the backend service, since if the upstream is temporarily out of service, Varnish can keep serving the cached content, so the clients don't have to notice anything (if the data they need is already present in the cache).\\n\\nIn order to use the basic caching capability, and to also gracefully fall back when the backend is down, we have to control two properties of the response object.\\n\\n - `beresp.ttl`: This is the amount of time until Varnish keeps an object in cache, and serves it up from there to the clients until it tries to fetch it again from the upstream.\\n - `beresp.grace`: This is the amount of time an object is kept in the cache even after the ttl is expired. This is interpreted on top of the ttl, so for example if both `ttl` and `grace` is set to `1m`, then the objects can stay in the cache for a total of 2 minutes.\\n\\nIf a request comes in for which the response is already in the cache, and the `ttl` has not passed yet, then Varnish returns the cached value to the client without trying to fetch a fresh one from the backend.\\n\\nOn the first request that comes after `ttl` expired, if `grace` is set and is not expired yet, then Varnish will immediately return the cached value to the client, and at the same time it will trigger a background fetch for a fresh response. If the fetch is successful, it replaces the stale object in the cache with the fresh one.\\n\\n![Sequence diagram showing how Varnish returns the cached content to the clients.](/content/images/2018/07/happy-flow.png)\\n\\nThis behavior means that if the backend is not available, that won't affect the end clients until the end of the `grace` period, since Varnish will always return the cached object to the client, and only do a background fetch asynchronously. Which will fail if the backend is down, but that won't hurt the object already stored in the cache.  \\nAnd if the `grace` period is over, Varnish will purge the cached object, and won't return it to clients any more. So if the backend is down, the clients will receive a `503 Service Unavailable` response from Varnish.\\n\\n![Sequence diagram showing how Varnish behaves when the backend is down during the grace period.](/content/images/2018/07/backend-down.png)\\n\\nIn practice, `grace` can be much longer than `ttl`. For example we can set `ttl` to a couple of minutes to make sure the clients regularly get fresh data, but we can set `grace` to multiple hours, to give us time to fix the backend in case of an outage.\\n\\n# 5xx responses\\n\\nBy default Varnish only falls back to the cached values during the grace period if the backend can't be connected to, or if the request times out. If the backend returns a `5xx` error, Varnish considers that normal response, and returns it to the client (it might even store it in the cache, depending on our configuration).  \\nThis is an understandable default, it might be confusing if our clients did not receive some server errors which could contain valuable information about the problem with the backend.\\n\\nOn the other hand, we might also consider 5xx responses outages, and want to gracefully fall back to the cached values instead. I wanted to achieve the following objectives.\\n\\n - Never cache `5xx` responses. This is important, because this'll make Varnish try to fetch the response again upon every request. (It would be wasteful to cache a `500` response for 10 minutes, when the outage might only last 1-2 minutes.)\\n - If we are in the grace period, and the backend returns a `5xx`, fall back to the cached response and send that to the client.\\n - If we don't have the response in the cache at all, then return to the client the actual `5xx` error that was given by the backend.\\n\\nThis can be achieved with Varnish pretty easily, but it needs some configuration.\\n\\n# Configuration\\n\\nIn order to customize the behavior of Varnish, we have to edit its configuration file (which usually resides at `/etc/varnish/default.vcl`).\\n\\nThe config file is using the VCL syntax, which is a C-like domain specific language specifically for the customization of Varnish. It took me some getting used to before I could wrap my head around it, an actual production configuration can get complicated, which can be a bit intimidating at first. In this post I won't talk about the basics, but will just rather show the bits necessary for this particular setup. If you'd like to learn the basics of VCL, I recommend reading a comprehensive guide, for example [The Varnish Book](https://info.varnish-software.com/the-varnish-book).\\n\\nIn the VCL configuration we can customize various *subroutines*, with which we can hook into varios steps of the request lifecycle. The only subrouting we're going to look at is `sub vcl_backend_response`, which is triggered right after a response was received from the backend.\\n\\nA very simple version of this would be the following, which enables caching for 1 minute, and enables graceful fallback for 10 minutes on top of that.\\n\\n```\\nsub vcl_backend_response {\\n    set beresp.ttl = 1m;\\n    set beresp.grace = 10m;\\n}\\n```\\n\\nIn order to achieve the objective of gracefull fallback on `5xx` errors, we'll need to use the following additional tools.\\n\\n - `beresp.status`: With this we can check the status code returned by the backend. This will be used to handle `5xx` responses specially.\\n - `bereq.is_bgfetch`: This field shows whether this was a backend request sent asynchronously when the client received a response already from the cache, or if it's a synchronous request that was sent when we didn't find the response in the cache.\\n - `beresp.uncacheable`: We can set this to true to prevent a response from being cached.\\n - `abandon`: This is a *return keyword* that we can use to make Varnish completely abandon the request, and return a *synthesized* `503` to the client (or just do nothing, in case of an asynchronous background fetch).\\n\\nWith all these tools in our belt we can implement the requirements with the following logic.\\n\\n```\\nsub vcl_backend_response {\\n    set beresp.ttl = 1m;\\n    set beresp.grace = 10m;\\n\\n    # This block will make sure that if the upstream returns a 5xx, but we have the response in the cache (even if it's expired),\\n    # we fall back to the cached value (until the grace period is over).\\n    if (beresp.status == 500 || beresp.status == 502 || beresp.status == 503 || beresp.status == 504)\\n    {\\n        # This check is important. If is_bgfetch is true, it means that we've found and returned the cached object to the client,\\n        # and triggered an asynchoronus background update. In that case, if it was a 5xx, we have to abandon, otherwise the previously cached object\\n        # would be erased from the cache (even if we set uncacheable to true).\\n        if (bereq.is_bgfetch)\\n        {\\n            return (abandon);\\n        }\\n\\n        # We should never cache a 5xx response.\\n        set beresp.uncacheable = true;\\n    }\\n}\\n```\\n\\nIn the above code I'm only handling the standard `500`, `502`, `503` and `504` codes, but of course you can extend it if your backend is returning something else.\\n\\nThe key of the solution is using `abandon` in combination with `if (bereq.is_bgfetch)`. If we returned `abandon` on every `5xx` without checking `bereq.is_bgfetch`, then Varnish would always return its built-in `503` response to the client instead of the `5xx` sent by the backend, so our client could never see the actual backend errors.  \\n\\n**Important**: The field `bereq.is_bgfetch` is only available starting from Varnish 5.2.0. And depending on our OS version and installation method, we might be on an earlier version, so this is important to check with `varnishd -V`.\\n\\nThe configuration syntax of Varnish takes a bit of getting used to, but if set up properly, it provides an invaluable tool for improving the performance of our services, and meanwhile protecting our clients from outages. And I hope this post will be helpful when handling server side errors.\"}]],\"sections\":[[10,0]]}",
        "html": "<h1 id=\"introduction\">Introduction</h1>\n<p>Varnish is a widely used reverse proxy and <em>HTTP accelerator</em>. It sits in front of an HTTP service, and caches the responses to improve the response times observed by the clients, and possibly to reduce the load on the upstream backend service.</p>\n<p>Besides bringing performance improvements, Varnish can be also useful for shielding the clients from any outage of the backend service, since if the upstream is temporarily out of service, Varnish can keep serving the cached content, so the clients don't have to notice anything (if the data they need is already present in the cache).</p>\n<p>In order to use the basic caching capability, and to also gracefully fall back when the backend is down, we have to control two properties of the response object.</p>\n<ul>\n<li><code>beresp.ttl</code>: This is the amount of time until Varnish keeps an object in cache, and serves it up from there to the clients until it tries to fetch it again from the upstream.</li>\n<li><code>beresp.grace</code>: This is the amount of time an object is kept in the cache even after the ttl is expired. This is interpreted on top of the ttl, so for example if both <code>ttl</code> and <code>grace</code> is set to <code>1m</code>, then the objects can stay in the cache for a total of 2 minutes.</li>\n</ul>\n<p>If a request comes in for which the response is already in the cache, and the <code>ttl</code> has not passed yet, then Varnish returns the cached value to the client without trying to fetch a fresh one from the backend.</p>\n<p>On the first request that comes after <code>ttl</code> expired, if <code>grace</code> is set and is not expired yet, then Varnish will immediately return the cached value to the client, and at the same time it will trigger a background fetch for a fresh response. If the fetch is successful, it replaces the stale object in the cache with the fresh one.</p>\n<p><img src=\"/content/images/2018/07/happy-flow.png\" alt=\"Sequence diagram showing how Varnish returns the cached content to the clients.\"></p>\n<p>This behavior means that if the backend is not available, that won't affect the end clients until the end of the <code>grace</code> period, since Varnish will always return the cached object to the client, and only do a background fetch asynchronously. Which will fail if the backend is down, but that won't hurt the object already stored in the cache.<br>\nAnd if the <code>grace</code> period is over, Varnish will purge the cached object, and won't return it to clients any more. So if the backend is down, the clients will receive a <code>503 Service Unavailable</code> response from Varnish.</p>\n<p><img src=\"/content/images/2018/07/backend-down.png\" alt=\"Sequence diagram showing how Varnish behaves when the backend is down during the grace period.\"></p>\n<p>In practice, <code>grace</code> can be much longer than <code>ttl</code>. For example we can set <code>ttl</code> to a couple of minutes to make sure the clients regularly get fresh data, but we can set <code>grace</code> to multiple hours, to give us time to fix the backend in case of an outage.</p>\n<h1 id=\"5xxresponses\">5xx responses</h1>\n<p>By default Varnish only falls back to the cached values during the grace period if the backend can't be connected to, or if the request times out. If the backend returns a <code>5xx</code> error, Varnish considers that normal response, and returns it to the client (it might even store it in the cache, depending on our configuration).<br>\nThis is an understandable default, it might be confusing if our clients did not receive some server errors which could contain valuable information about the problem with the backend.</p>\n<p>On the other hand, we might also consider 5xx responses outages, and want to gracefully fall back to the cached values instead. I wanted to achieve the following objectives.</p>\n<ul>\n<li>Never cache <code>5xx</code> responses. This is important, because this'll make Varnish try to fetch the response again upon every request. (It would be wasteful to cache a <code>500</code> response for 10 minutes, when the outage might only last 1-2 minutes.)</li>\n<li>If we are in the grace period, and the backend returns a <code>5xx</code>, fall back to the cached response and send that to the client.</li>\n<li>If we don't have the response in the cache at all, then return to the client the actual <code>5xx</code> error that was given by the backend.</li>\n</ul>\n<p>This can be achieved with Varnish pretty easily, but it needs some configuration.</p>\n<h1 id=\"configuration\">Configuration</h1>\n<p>In order to customize the behavior of Varnish, we have to edit its configuration file (which usually resides at <code>/etc/varnish/default.vcl</code>).</p>\n<p>The config file is using the VCL syntax, which is a C-like domain specific language specifically for the customization of Varnish. It took me some getting used to before I could wrap my head around it, an actual production configuration can get complicated, which can be a bit intimidating at first. In this post I won't talk about the basics, but will just rather show the bits necessary for this particular setup. If you'd like to learn the basics of VCL, I recommend reading a comprehensive guide, for example <a href=\"https://info.varnish-software.com/the-varnish-book\">The Varnish Book</a>.</p>\n<p>In the VCL configuration we can customize various <em>subroutines</em>, with which we can hook into varios steps of the request lifecycle. The only subrouting we're going to look at is <code>sub vcl_backend_response</code>, which is triggered right after a response was received from the backend.</p>\n<p>A very simple version of this would be the following, which enables caching for 1 minute, and enables graceful fallback for 10 minutes on top of that.</p>\n<pre><code>sub vcl_backend_response {\n    set beresp.ttl = 1m;\n    set beresp.grace = 10m;\n}\n</code></pre>\n<p>In order to achieve the objective of gracefull fallback on <code>5xx</code> errors, we'll need to use the following additional tools.</p>\n<ul>\n<li><code>beresp.status</code>: With this we can check the status code returned by the backend. This will be used to handle <code>5xx</code> responses specially.</li>\n<li><code>bereq.is_bgfetch</code>: This field shows whether this was a backend request sent asynchronously when the client received a response already from the cache, or if it's a synchronous request that was sent when we didn't find the response in the cache.</li>\n<li><code>beresp.uncacheable</code>: We can set this to true to prevent a response from being cached.</li>\n<li><code>abandon</code>: This is a <em>return keyword</em> that we can use to make Varnish completely abandon the request, and return a <em>synthesized</em> <code>503</code> to the client (or just do nothing, in case of an asynchronous background fetch).</li>\n</ul>\n<p>With all these tools in our belt we can implement the requirements with the following logic.</p>\n<pre><code>sub vcl_backend_response {\n    set beresp.ttl = 1m;\n    set beresp.grace = 10m;\n\n    # This block will make sure that if the upstream returns a 5xx, but we have the response in the cache (even if it's expired),\n    # we fall back to the cached value (until the grace period is over).\n    if (beresp.status == 500 || beresp.status == 502 || beresp.status == 503 || beresp.status == 504)\n    {\n        # This check is important. If is_bgfetch is true, it means that we've found and returned the cached object to the client,\n        # and triggered an asynchoronus background update. In that case, if it was a 5xx, we have to abandon, otherwise the previously cached object\n        # would be erased from the cache (even if we set uncacheable to true).\n        if (bereq.is_bgfetch)\n        {\n            return (abandon);\n        }\n\n        # We should never cache a 5xx response.\n        set beresp.uncacheable = true;\n    }\n}\n</code></pre>\n<p>In the above code I'm only handling the standard <code>500</code>, <code>502</code>, <code>503</code> and <code>504</code> codes, but of course you can extend it if your backend is returning something else.</p>\n<p>The key of the solution is using <code>abandon</code> in combination with <code>if (bereq.is_bgfetch)</code>. If we returned <code>abandon</code> on every <code>5xx</code> without checking <code>bereq.is_bgfetch</code>, then Varnish would always return its built-in <code>503</code> response to the client instead of the <code>5xx</code> sent by the backend, so our client could never see the actual backend errors.</p>\n<p><strong>Important</strong>: The field <code>bereq.is_bgfetch</code> is only available starting from Varnish 5.2.0. And depending on our OS version and installation method, we might be on an earlier version, so this is important to check with <code>varnishd -V</code>.</p>\n<p>The configuration syntax of Varnish takes a bit of getting used to, but if set up properly, it provides an invaluable tool for improving the performance of our services, and meanwhile protecting our clients from outages. And I hope this post will be helpful when handling server side errors.</p>\n",
        "comment_id": "5b5c8545e67a0c102087288f",
        "plaintext": "Introduction\nVarnish is a widely used reverse proxy and HTTP accelerator. It sits in front of\nan HTTP service, and caches the responses to improve the response times observed\nby the clients, and possibly to reduce the load on the upstream backend service.\n\nBesides bringing performance improvements, Varnish can be also useful for\nshielding the clients from any outage of the backend service, since if the\nupstream is temporarily out of service, Varnish can keep serving the cached\ncontent, so the clients don't have to notice anything (if the data they need is\nalready present in the cache).\n\nIn order to use the basic caching capability, and to also gracefully fall back\nwhen the backend is down, we have to control two properties of the response\nobject.\n\n * beresp.ttl: This is the amount of time until Varnish keeps an object in\n   cache, and serves it up from there to the clients until it tries to fetch it\n   again from the upstream.\n * beresp.grace: This is the amount of time an object is kept in the cache even\n   after the ttl is expired. This is interpreted on top of the ttl, so for\n   example if both ttl  and grace  is set to 1m, then the objects can stay in\n   the cache for a total of 2 minutes.\n\nIf a request comes in for which the response is already in the cache, and the \nttl  has not passed yet, then Varnish returns the cached value to the client\nwithout trying to fetch a fresh one from the backend.\n\nOn the first request that comes after ttl  expired, if grace  is set and is not\nexpired yet, then Varnish will immediately return the cached value to the\nclient, and at the same time it will trigger a background fetch for a fresh\nresponse. If the fetch is successful, it replaces the stale object in the cache\nwith the fresh one.\n\n\n\nThis behavior means that if the backend is not available, that won't affect the\nend clients until the end of the grace  period, since Varnish will always return\nthe cached object to the client, and only do a background fetch asynchronously.\nWhich will fail if the backend is down, but that won't hurt the object already\nstored in the cache.\nAnd if the grace  period is over, Varnish will purge the cached object, and\nwon't return it to clients any more. So if the backend is down, the clients will\nreceive a 503 Service Unavailable  response from Varnish.\n\n\n\nIn practice, grace  can be much longer than ttl. For example we can set ttl  to\na couple of minutes to make sure the clients regularly get fresh data, but we\ncan set grace  to multiple hours, to give us time to fix the backend in case of\nan outage.\n\n5xx responses\nBy default Varnish only falls back to the cached values during the grace period\nif the backend can't be connected to, or if the request times out. If the\nbackend returns a 5xx  error, Varnish considers that normal response, and\nreturns it to the client (it might even store it in the cache, depending on our\nconfiguration).\nThis is an understandable default, it might be confusing if our clients did not\nreceive some server errors which could contain valuable information about the\nproblem with the backend.\n\nOn the other hand, we might also consider 5xx responses outages, and want to\ngracefully fall back to the cached values instead. I wanted to achieve the\nfollowing objectives.\n\n * Never cache 5xx  responses. This is important, because this'll make Varnish\n   try to fetch the response again upon every request. (It would be wasteful to\n   cache a 500  response for 10 minutes, when the outage might only last 1-2\n   minutes.)\n * If we are in the grace period, and the backend returns a 5xx, fall back to\n   the cached response and send that to the client.\n * If we don't have the response in the cache at all, then return to the client\n   the actual 5xx  error that was given by the backend.\n\nThis can be achieved with Varnish pretty easily, but it needs some\nconfiguration.\n\nConfiguration\nIn order to customize the behavior of Varnish, we have to edit its configuration\nfile (which usually resides at /etc/varnish/default.vcl).\n\nThe config file is using the VCL syntax, which is a C-like domain specific\nlanguage specifically for the customization of Varnish. It took me some getting\nused to before I could wrap my head around it, an actual production\nconfiguration can get complicated, which can be a bit intimidating at first. In\nthis post I won't talk about the basics, but will just rather show the bits\nnecessary for this particular setup. If you'd like to learn the basics of VCL, I\nrecommend reading a comprehensive guide, for example The Varnish Book\n[https://info.varnish-software.com/the-varnish-book].\n\nIn the VCL configuration we can customize various subroutines, with which we can\nhook into varios steps of the request lifecycle. The only subrouting we're going\nto look at is sub vcl_backend_response, which is triggered right after a\nresponse was received from the backend.\n\nA very simple version of this would be the following, which enables caching for\n1 minute, and enables graceful fallback for 10 minutes on top of that.\n\nsub vcl_backend_response {\n    set beresp.ttl = 1m;\n    set beresp.grace = 10m;\n}\n\n\nIn order to achieve the objective of gracefull fallback on 5xx  errors, we'll\nneed to use the following additional tools.\n\n * beresp.status: With this we can check the status code returned by the\n   backend. This will be used to handle 5xx  responses specially.\n * bereq.is_bgfetch: This field shows whether this was a backend request sent\n   asynchronously when the client received a response already from the cache, or\n   if it's a synchronous request that was sent when we didn't find the response\n   in the cache.\n * beresp.uncacheable: We can set this to true to prevent a response from being\n   cached.\n * abandon: This is a return keyword  that we can use to make Varnish completely\n   abandon the request, and return a synthesized  503  to the client (or just do\n   nothing, in case of an asynchronous background fetch).\n\nWith all these tools in our belt we can implement the requirements with the\nfollowing logic.\n\nsub vcl_backend_response {\n    set beresp.ttl = 1m;\n    set beresp.grace = 10m;\n\n    # This block will make sure that if the upstream returns a 5xx, but we have the response in the cache (even if it's expired),\n    # we fall back to the cached value (until the grace period is over).\n    if (beresp.status == 500 || beresp.status == 502 || beresp.status == 503 || beresp.status == 504)\n    {\n        # This check is important. If is_bgfetch is true, it means that we've found and returned the cached object to the client,\n        # and triggered an asynchoronus background update. In that case, if it was a 5xx, we have to abandon, otherwise the previously cached object\n        # would be erased from the cache (even if we set uncacheable to true).\n        if (bereq.is_bgfetch)\n        {\n            return (abandon);\n        }\n\n        # We should never cache a 5xx response.\n        set beresp.uncacheable = true;\n    }\n}\n\n\nIn the above code I'm only handling the standard 500, 502, 503  and 504  codes,\nbut of course you can extend it if your backend is returning something else.\n\nThe key of the solution is using abandon  in combination with if\n(bereq.is_bgfetch). If we returned abandon  on every 5xx  without checking \nbereq.is_bgfetch, then Varnish would always return its built-in 503  response to\nthe client instead of the 5xx  sent by the backend, so our client could never\nsee the actual backend errors.\n\nImportant: The field bereq.is_bgfetch  is only available starting from Varnish\n5.2.0. And depending on our OS version and installation method, we might be on\nan earlier version, so this is important to check with varnishd -V.\n\nThe configuration syntax of Varnish takes a bit of getting used to, but if set\nup properly, it provides an invaluable tool for improving the performance of our\nservices, and meanwhile protecting our clients from outages. And I hope this\npost will be helpful when handling server side errors.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "Varnish can gracefully fall  back to cached values in case our backend is down. This post describes how we can handle 5xx errors this way.",
        "author_id": "1",
        "created_at": "2018-07-28 15:01:25",
        "created_by": "1",
        "updated_at": "2018-07-30 08:25:44",
        "updated_by": "1",
        "published_at": "2018-07-28 19:12:24",
        "published_by": "1",
        "custom_excerpt": "Varnish can gracefully fall  back to cached values in case our backend is down. This post describes how we can handle 5xx errors this way.",
        "codeinjection_head": "",
        "codeinjection_foot": "",
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "5bb0a47f63bf8a66478d78d6",
        "uuid": "17896c79-8918-46ff-99ef-24acc1f3bd06",
        "title": "How to use Envoy as a Load Balancer in Kubernetes",
        "slug": "how-to-use-envoy-as-a-load-balancer-in-kubernetes",
        "mobiledoc": "{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"```yaml\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: myapp\\nspec:\\n  clusterIP: None\\n  ports:\\n  - name: http\\n    port: 80\\n    targetPort: http\\n    protocol: TCP\\n  selector:\\n    app: myapp\\n```\"}],[\"markdown\",{\"markdown\":\"```bash\\n$ nslookup myapp\\nServer:         10.40.0.10\\nAddress:        10.40.0.10#53\\n\\nNon-authoritative answer:\\nName:   myapp.namespace.svc.cluster.local\\nAddress: 10.36.224.5\\nName:   myapp.namespace.svc.cluster.local\\nAddress: 10.38.187.17\\nName:   myapp.namespace.svc.cluster.local\\nAddress: 10.38.1.8\\n```\"}],[\"markdown\",{\"markdown\":\"```yaml\\nadmin:\\n  access_log_path: /tmp/admin_access.log\\n  address:\\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\\n\\nstatic_resources:\\n  listeners:\\n  - name: listener_0\\n    address:\\n      socket_address: { address: 0.0.0.0, port_value: 80 }\\n    filter_chains:\\n    - filters:\\n      - name: envoy.http_connection_manager\\n        config:\\n          stat_prefix: ingress_http\\n          route_config:\\n            name: local_route\\n            virtual_hosts:\\n            - name: local_service\\n              domains: [\\\"*\\\"]\\n              routes:\\n              - match: { prefix: \\\"/\\\" }\\n                route: { host_rewrite: myapp, cluster: myapp_cluster, timeout: 60s }\\n          http_filters:\\n          - name: envoy.router\\n  clusters:\\n  - name: myapp_cluster\\n    connect_timeout: 0.25s\\n    type: STRICT_DNS\\n    dns_lookup_family: V4_ONLY\\n    lb_policy: LEAST_REQUEST\\n    hosts: [{ socket_address: { address: myapp, port_value: 80 }}]\\n```\"}],[\"markdown\",{\"markdown\":\"```dockerfile\\nFROM envoyproxy/envoy:latest\\nCOPY envoy.yaml /etc/envoy.yaml\\nCMD /usr/local/bin/envoy -c /etc/envoy.yaml\\n```\"}],[\"code\",{\"code\":\"$ docker build -t markvincze/myapp-envoy:1 .\\n$ docker push markvincze/myapp-envoy:1\"}],[\"markdown\",{\"markdown\":\"```yaml\\nadmin:\\n  access_log_path: /tmp/admin_access.log\\n  address:\\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\\n\\nstatic_resources:\\n  listeners:\\n  - name: listener_0\\n    address:\\n      socket_address: { address: 0.0.0.0, port_value: 80 }\\n    filter_chains:\\n    - filters:\\n      - name: envoy.http_connection_manager\\n        config:\\n          stat_prefix: ingress_http\\n          route_config:\\n            name: local_route\\n            virtual_hosts:\\n            - name: local_service\\n              domains: [\\\"*\\\"]\\n              routes:\\n              - match: { prefix: \\\"/\\\" }\\n                route: { host_rewrite: myapp, cluster: myapp_cluster, timeout: 60s }\\n          http_filters:\\n          - name: envoy.router\\n  clusters:\\n  - name: myapp_cluster\\n    connect_timeout: 0.25s\\n    type: STRICT_DNS\\n    dns_lookup_family: V4_ONLY\\n    lb_policy: ${ENVOY_LB_ALG}\\n    hosts: [{ socket_address: { address: ${SERVICE_NAME}, port_value: 80 }}]\\n```\"}],[\"markdown\",{\"markdown\":\"```bash\\n#!/bin/sh\\nset -e\\n\\necho \\\"Generating envoy.yaml config file...\\\"\\ncat /tmpl/envoy.yaml.tmpl | envsubst \\\\$ENVOY_LB_ALG,\\\\$SERVICE_NAME > /etc/envoy.yaml\\n\\necho \\\"Starting Envoy...\\\"\\n/usr/local/bin/envoy -c /etc/envoy.yaml\\n```\"}],[\"code\",{\"code\":\"FROM envoyproxy/envoy:latest\\n\\nCOPY envoy.yaml /tmpl/envoy.yaml.tmpl\\nCOPY docker-entrypoint.sh /\\n\\nRUN chmod 500 /docker-entrypoint.sh\\n\\nRUN apt-get update && \\\\\\n    apt-get install gettext -y\\n\\nENTRYPOINT [\\\"/docker-entrypoint.sh\\\"]\"}],[\"markdown\",{\"markdown\":\"```yaml\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: myapp-envoy\\n  labels:\\n    app: myapp-envoy\\nspec:\\n  type: ClusterIP\\n  ports:\\n  - name: http\\n    port: 80\\n    targetPort: http\\n    protocol: TCP\\n  selector:\\n    app: myapp-envoy\\n---\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: myapp-envoy\\n  labels:\\n    app: myapp-envoy\\nspec:\\n  selector:\\n    matchLabels:\\n      app: myapp-envoy\\n  template:\\n    metadata:\\n      labels:\\n        app: myapp-envoy\\n    spec:\\n      containers:\\n      - name: myapp-envoy\\n        image: mydockerhub/mycustomenvoy\\n        imagePullPolicy: Always\\n        env:\\n        - name: \\\"ENVOY_LB_ALG\\\"\\n          value: \\\"LEAST_REQUEST\\\"\\n        - name: \\\"SERVICE_NAME\\\"\\n          value: \\\"myapp\\\"\\n        ports:\\n        - name: http\\n          containerPort: 80\\n        - name: envoy-admin\\n          containerPort: 9901\\n```\"}],[\"image\",{\"src\":\"/content/images/2018/10/architecture.png\"}],[\"image\",{\"src\":\"/content/images/2018/10/image.png\",\"caption\":\"\"}],[\"image\",{\"src\":\"/content/images/2018/10/image-1.png\"}],[\"image\",{\"src\":\"/content/images/2018/10/image-2.png\"}]],\"markups\":[[\"a\",[\"href\",\"https://www.nginx.com/\"]],[\"a\",[\"href\",\"http://www.haproxy.org/\"]],[\"a\",[\"href\",\"https://github.com/Netflix/zuul\"]],[\"a\",[\"href\",\"https://linkerd.io/\"]],[\"a\",[\"href\",\"https://traefik.io/\"]],[\"a\",[\"href\",\"https://github.com/mholt/caddy\"]],[\"a\",[\"href\",\"https://www.envoyproxy.io/\"]],[\"code\"],[\"em\"],[\"a\",[\"href\",\"https://kubernetes.io/docs/concepts/services-networking/service/#headless-services\"]],[\"a\"],[\"a\",[\"href\",\"https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/load_balancing.html\"]],[\"a\",[\"href\",\"https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/load_balancing.html#weighted-least-request\"]],[\"a\",[\"href\",\"https://www.envoyproxy.io/docs/envoy/latest/api-v2/api\"]],[\"a\",[\"href\",\"https://github.com/transferwise/prometheus-envoy-dashboards\"]],[\"a\",[\"href\",\"https://github.com/markvincze/PrimeCalcApi/\"]],[\"a\",[\"href\",\"https://github.com/markvincze/envoy/pull/1\"]],[\"a\",[\"href\",\"https://github.com/envoyproxy/envoy/issues/4481\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/concepts/services-networking/ingress/\"]],[\"a\",[\"href\",\"https://github.com/datawire/ambassador\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"In today's highly distributed word, where monolithic architectures are increasingly replaced with multiple, smaller, interconnected services (for better or worse), proxy and load balancing technologies seem to have a renaissance. Beside the older players, there are several new proxy technologies popping up in recent years, implemented in various technologies, popularizing themselves with diferent features, such as easy integration to certain cloud providers (\\\"cloud-native\\\"), high performance and low memory footprint, or dynamic configuration.\"]]],[1,\"p\",[[0,[],0,\"Arguably the two most popular \\\"classic\\\" proxy technologies are \"],[0,[0],1,\"NGINX\"],[0,[],0,\" (C) and \"],[0,[1],1,\"HAProxy\"],[0,[],0,\" (C), while some of the new kids on the block are \"],[0,[2],1,\"Zuul\"],[0,[],0,\" (Java), \"],[0,[3],1,\"Linkerd\"],[0,[],0,\" (Rust), \"],[0,[4],1,\"Traefik\"],[0,[],0,\" (Go), \"],[0,[5],1,\"Caddy\"],[0,[],0,\" (Go) and \"],[0,[6],1,\"Envoy\"],[0,[],0,\" (C++).\"]]],[1,\"p\",[[0,[],0,\"All of these technologies have different feature sets, and are targeting some specific scenarios or hosting environments (for example Linkerd is fine-tuned for being used in Kubernetes).\"],[1,[],0,0],[0,[],0,\"In this post I'm not going to do a comparison of these, but rather just focus on one specific scenario: how to use Envoy as a load balancer for a service running in Kubernetes.\"]]],[1,\"p\",[[0,[6],1,\"Envoy\"],[0,[],0,\" is a \\\"high performance C++ distributed proxy\\\", originally implemented at Lyft, but since then have gained a wide adoption. It's high-performant, has a low resource footprint, it supports dynamic configuration managed by a \\\"control plane\\\" API, and provides some advanced features such as various load balancing algorithms, rate limiting, circuit braking, and shadow mirroring.\"]]],[1,\"p\",[[0,[],0,\"I chose Envoy as a load balancer proxy for a number of reasons.\"]]],[3,\"ul\",[[[0,[],0,\"Besides being able to be controlled dynamically with a control plane API, it also supports a simple, hard-coded YAML-based configuration, which was convenient for my purposes, and made it easy to get started.\"]],[[0,[],0,\"It has built-in support for a service discovery technique it calls \"],[0,[7],1,\"STRICT_DNS\"],[0,[],0,\", which builds on querying a DNS record, and expecting seeing an A record with an IP address for every node of the upstream cluster. This made it easy to use with a headless service in Kubernetes.\"]],[[0,[],0,\"It supports various load balancing algorithms, among others \\\"Least Request\\\".\"]]]],[1,\"p\",[[0,[],0,\"Before starting to use Envoy, I was accessing my service in Kubernetes through a \"],[0,[7],1,\"service\"],[0,[],0,\" object of the type \"],[0,[7],1,\"LoadBalancer\"],[0,[],0,\", which is a pretty typical way to access services from the outside in Kubernetes. The exact way a load balancer service works depends on the hosting environmentif it supports it in the first place. I was using the Google Kubernetes Engine, where every load balancer service is mapped to a TCP-level Google Cloud load balancer, which only supports a \"],[0,[8],1,\"round robin\"],[0,[],0,\" load balancing algorithm.\"]]],[1,\"p\",[[0,[],0,\"In my case this was an issue, because my service had the following characteristics.\"]]],[3,\"ul\",[[[0,[],0,\"The requests were long running with varied response times, ranging from 100ms to seconds.\"]],[[0,[],0,\"The processing of the requests was CPU-intensive, practically the processing of one request used 100% of one CPU core.\"]],[[0,[],0,\"Processing many requests in parallel degraded the response time. (This was due to the internals of how this service worked, it couldn't efficiently run more than a handful requests in parallel.)\"]]]],[1,\"p\",[[0,[],0,\"Due to the above characteristics, the round robin load balancing algorithm was not a good fit, because oftenby chancemultiple requests ended up on the same node, which made the average response times much worse than what the cluster would've been capable achieving, given a more uniformly spred out load.\"]]],[1,\"p\",[[0,[],0,\"In the remainder of this post I will describe the steps necessary to deploy Envoy to be used as a load balancer in front of a service running in Kubernetes.\"]]],[1,\"h2\",[[0,[],0,\"1. Create the headless service for our application\"]]],[1,\"p\",[[0,[],0,\"In Kubernetes there is a specific kind of service called a \"],[0,[9],1,\"headless service\"],[0,[10],1,\",\"],[0,[],0,\" which happens to be very convenient to be used together with Envoy's \"],[0,[7],1,\"STRICT_DNS\"],[0,[],0,\" service discovery mode.\"]]],[1,\"p\",[[0,[],0,\"A headless service doesn't provide a single IP and load balancing to the underlying pods, but rather it just has DNS configuration which gives us an A record with the pod's IP address for all the pods matching the label selector.  \"],[1,[],0,1],[0,[],0,\"This service type is intended to be used in scenarios when we want to implement load balancing, and maintaining the connections to the upstream pods ourselves, which is exactly what we can do with Envoy.\"]]],[1,\"p\",[[0,[],0,\"We can create a headless service by setting the \"],[0,[7],1,\".spec.clusterIP\"],[0,[],0,\" field to \"],[0,[7],1,\"\\\"None\\\"\"],[0,[],0,\". So assuming that our application pods have the label \"],[0,[7],1,\"app\"],[0,[],0,\" with the value \"],[0,[7],1,\"myapp\"],[0,[],0,\", we can create the headless service with the following yaml.\"]]],[10,0],[1,\"p\",[[0,[],0,\"(The name of the \"],[0,[7],1,\"Service\"],[0,[],0,\" does not have to be equal to the name of our application nor the \"],[0,[7],1,\"app\"],[0,[],0,\" label, but it's a good convention to follow.)\"]]],[1,\"p\",[[0,[],0,\"Now if we check the DNS records for our service inside the Kubernetes cluster, we'll see the separate A records with the IP addresses. If we have 3 pods, we'll see a DNS summary similar to this.\"]]],[10,1],[1,\"p\",[[0,[],0,\"And the way the \"],[0,[7],1,\"STRICT_DNS\"],[0,[],0,\" service discovery of Envoy works is that it maintains the IP address of all the A records returned by the DNS, and it refreshes the set of IPs every couple of seconds.\"]]],[1,\"h2\",[[0,[],0,\"2. Create the Envoy image\"]]],[1,\"p\",[[0,[],0,\"The simplest way to use Envoy without providing the control plane in the form of a dynamic API is to add the hardcoded configuration to a static yaml file.\"]]],[1,\"p\",[[0,[],0,\"The following is a basic configuration that load balances to the IP addresses given by the domain name \"],[0,[7],1,\"myapp\"],[0,[],0,\".\"]]],[10,2],[1,\"p\",[[0,[],0,\"Note the following parts.\"]]],[3,\"ul\",[[[0,[7],1,\"type: STRICT_DNS\"],[0,[],0,\": Here we're specifying the service discovery type. It's important to set it to \"],[0,[7],1,\"STRICT_DNS\"],[0,[],0,\", because this works well together with the headless service we've set up.\"]],[[0,[7],1,\"lb_policy: LEAST_REQUEST\"],[0,[],0,\": We can choose from various \"],[0,[11],1,\"load balancing algorithms\"],[0,[],0,\", probably \"],[0,[7],1,\"ROUND_ROBIN\"],[0,[],0,\" and \"],[0,[7],1,\"LEAST_REQUEST\"],[0,[],0,\" are the most common. (Keep in mind that \"],[0,[7],1,\"LEAST_REQUEST\"],[0,[],0,\" is not checking all upstream nodes, but it only chooses from \"],[0,[12],1,\"2 randomly picked options\"],[0,[],0,\".)\"]],[[0,[7],1,\"hosts: [{ socket_address: { address: myapp, port_value: 80 }}]\"],[0,[],0,\": This is the part where we specify with the \"],[0,[7],1,\"address\"],[0,[],0,\" field the domain name from which Envoy will have to get the A records to route to.\"]]]],[1,\"p\",[[0,[],0,\"You can find more information about the various config parameters in the \"],[0,[13],1,\"docs\"],[0,[10],1,\".\"]]],[1,\"p\",[[0,[],0,\"Now we have to put the following \"],[0,[7],1,\"Dockerfile\"],[0,[],0,\" next to the \"],[0,[7],1,\"envoy.yaml\"],[0,[],0,\" config file.\"]]],[10,3],[1,\"p\",[[0,[],0,\"The last step is building the image, and pushing it somewhere (like the Docker hub, or the container registry of a cloud provider) to be able to use it from Kubernetes.\"],[1,[],0,2],[0,[],0,\"Assuming I'd like to push this to my personal Docker hub account, I could do it with the following commands.\"]]],[10,4],[1,\"h2\",[[0,[],0,\"(3. Optional: make the Envoy image parameterizable)\"]]],[1,\"p\",[[0,[],0,\"If we'd like to be able to customize some parts of the Envoy configuration with environment variables without rebuilding the Docker image, we can do some env var substitution in the yaml config. Let's say we'd like to be able to customize the name of the headless service we're proxying to, and the load balancer algorithm, then we'd have to modify the yaml config the following way.\"]]],[10,5],[1,\"p\",[[0,[],0,\"Then implement a little shell script (\"],[0,[7],1,\"docker-entrypoint.sh\"],[0,[],0,\") in which we do the env var substitution.\"]]],[10,6],[1,\"p\",[[0,[],0,\"And change our Dockerfile to run this script instead of directly starting Envoy.\"]]],[10,7],[1,\"p\",[[0,[],0,\"Keep in mind that if you use this approach, you have to specify these env vars in the Kubernetes deployment, otherwise they will be empty.\"]]],[1,\"h2\",[[0,[],0,\"4. Create the Envoy deployment\"]]],[1,\"p\",[[0,[],0,\"Finally we have to create a deployment for Envoy itself.\"]]],[10,8],[1,\"p\",[[0,[],0,\"(We only need the env variables if we made our Envoy Docker image parameterizable.)\"]]],[1,\"p\",[[0,[],0,\"After applying this yaml, the Envoy proxy should be operational, and you can access the underlying service by sending the requests to the main port of the Envoy service.\"]]],[1,\"p\",[[0,[],0,\"In this example I only added a service of type ClusterIP, but you can also use a LoadBalancer service, or an Ingress object if you want to access the proxy from outside the cluster.\"]]],[1,\"p\",[[0,[],0,\"The following diagram illustrates the architecture of the whole setup.\"]]],[10,9],[1,\"p\",[[0,[],0,\"The image only shows one single Envoy pod, but you can scale it up to have more instances if necessary. And of course you can use a Horizontal Pod Autoscaler to automatically create more replicas as needed. (All instances will be autonomous and independent of each other.)\"],[1,[],0,3],[0,[],0,\"In practice you'll probably need much fewer instances for the proxy than for the underlying service. In the current production application we're using Envoy with, we're serving ~1000 requests/seconds on ~400 upstream pods, but we only have 3 instances of Envoy running, with ~10% CPU load.\"]]],[1,\"h2\",[[0,[],0,\"Troubleshooting and Monitoring\"]]],[1,\"p\",[[0,[],0,\"In the Envoy configuration file you can see an \"],[0,[7],1,\"admin:\"],[0,[],0,\" section, which configures Envoy's admin endpoint. That can be used for checking various diagnostic information about the proxy.\"]]],[1,\"p\",[[0,[],0,\"(If you don't have a service publishing the admin port9901 by defaultyou can still access it by port-forwarding to a pod with \"],[0,[7],1,\"kubectl\"],[0,[],0,\". Assuming that one of the Envoy pods is called \"],[0,[7],1,\"myapp-envoy-656c8d5fff-mwff8\"],[0,[],0,\", then you can start port-fowarding with the command \"],[0,[7],1,\"kubectl port-forward myapp-envoy-656c8d5fff-mwff8 9901\"],[0,[],0,\". Then you can access the page at \"],[0,[7],1,\"http://localhost:9901\"],[0,[],0,\".)\"]]],[1,\"p\",[[0,[],0,\"Some of the useful endpoints:\"]]],[3,\"ul\",[[[0,[7],1,\"/config_dump\"],[0,[],0,\": Prints the complete configuration of the proxy, which is useful to verify if the correct config ended up on the pod.\"]],[[0,[7],1,\"/clusters\"],[0,[],0,\": Shows all the upstream nodes that Envoy discovered, and the number of requests processed for each of them. This is useful for example for checking if the load balancing algorithm works properly.\"]]]],[1,\"p\",[[0,[],0,\"One way to do monitoring is to use Prometheus to scrape the stats from the proxy pods. Envoy has built-in support for this, the Prometheus stats are published on the admin port at the route \"],[0,[7],1,\"/stats/prometheus\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"You can download a Grafana dashboard visualizing these metrics from \"],[0,[14],1,\"this repository\"],[0,[],0,\", which will give you a set of graphs to the following.\"]]],[10,10],[1,\"h2\",[[0,[],0,\"A word on the load balancing algorithms\"]]],[1,\"p\",[[0,[],0,\"The load balancing algorithm can have significant effect on the overall performance of our cluster. Using a least request algorithm can be beneficial with services for which an even spread of the load is necessary, for example when a service is CPU-intensive, and gets easily overloaded.\"],[1,[],0,4],[0,[],0,\"On the other hand, least request has the problem that if one of the nodes start failing for some reason, and the failure response times are quick, then the load balancer will send a disproportionately large portion of the requests to the failing nodesomething which wouldn't be a problem with round robin.\"]]],[1,\"p\",[[0,[],0,\"I made some benchmarks with a \"],[0,[15],1,\"dummy API\"],[0,[],0,\" and compared the round robin and the least request LB algorithms. It turned out that least request can bring a significant improvement in the overall performance.\"]]],[1,\"p\",[[0,[],0,\"I was benchmarking the API for ~40 minutes with a continuously increasing input traffic. Throughout the benchmark I collected the following metrics:\"]]],[3,\"ul\",[[[0,[],0,\"Number of requests being executed (\\\"requests in flight\\\"), broken down by servers\"]],[[0,[],0,\"Number of requests being executed, average per server\"]],[[0,[],0,\"Incoming request rate (this was increasing every 5 minutes)\"]],[[0,[],0,\"Error rate (normally there weren't any, but when things started to slow down, this started to show some timeouts)\"]],[[0,[],0,\"Response time percentiles (0.50, 0.90 and 0.99) recorded on the servers\"]]]],[1,\"p\",[[0,[],0,\"The stats looked like this with \"],[0,[7],1,\"ROUND_ROBIN\"],[0,[],0,\".\"]]],[10,11],[1,\"p\",[[0,[],0,\"These were the results with \"],[0,[7],1,\"LEAST_REQUEST\"],[0,[],0,\":\"]]],[10,12],[1,\"p\",[[0,[],0,\"You can see on the results that \"],[0,[7],1,\"LEAST_REQUEST\"],[0,[],0,\" can lead to a much smoother distribution of the traffic across the nodes, and thus a lower average response time at high load. \"]]],[1,\"p\",[[0,[],0,\"The exact improvement will depend on the actual API, so I definitely recommend you to do the benchmark with your own service too to be make a decision.\"]]],[1,\"p\",[[0,[],0,\"(Earlier I mentioned that when using \"],[0,[7],1,\"LEAST_REQUEST\"],[0,[],0,\", Envoy will not look at every node in the cluster when selecting the one to send a certain request to, but rather just looks at \"],[0,[12],1,\"2 randomly picked options\"],[0,[],0,\", and sends the request to the one that has the fewer pending requests.\"],[1,[],0,5],[0,[],0,\"I was curious if actually doing a full O(n) scan and picking among all the nodes the one with the least amount of requests would bring a further improvement, so I implemented this feature in a \"],[0,[16],1,\"fork of Envoy\"],[0,[10],1,\",\"],[0,[],0,\" and did the same benchmark.\"],[1,[],0,6],[0,[],0,\"It turns out that this can indeed further improve the uniformity of the load, and the resource utilization, thereby decreasing the response times at high load. There is an ongoing discussion about this topic in \"],[0,[17],1,\"this issue\"],[0,[],0,\".)\"]]],[1,\"h2\",[[0,[],0,\"Summary\"]]],[1,\"p\",[[0,[],0,\"I hope this introduction will be helpful for getting started with using Envoy in Kubernetes. By the way, this is not the only way to achieve least request load balancing on Kubernetes. There are various \"],[0,[18],1,\"ingress controllers\"],[0,[10],1,\" \"],[0,[],0,\"(one of which is \"],[0,[19],1,\"Ambassador\"],[0,[],0,\", which builds on top of Envoy) that can do the same.\"],[1,[],0,7],[0,[],0,\"If you have any other suggestions, feedback is welcome!\"]]]]}",
        "html": "<p>In today's highly distributed word, where monolithic architectures are increasingly replaced with multiple, smaller, interconnected services (for better or worse), proxy and load balancing technologies seem to have a renaissance. Beside the older players, there are several new proxy technologies popping up in recent years, implemented in various technologies, popularizing themselves with diferent features, such as easy integration to certain cloud providers (\"cloud-native\"), high performance and low memory footprint, or dynamic configuration.</p><p>Arguably the two most popular \"classic\" proxy technologies are <a href=\"https://www.nginx.com/\">NGINX</a> (C) and <a href=\"http://www.haproxy.org/\">HAProxy</a> (C), while some of the new kids on the block are <a href=\"https://github.com/Netflix/zuul\">Zuul</a> (Java), <a href=\"https://linkerd.io/\">Linkerd</a> (Rust), <a href=\"https://traefik.io/\">Traefik</a> (Go), <a href=\"https://github.com/mholt/caddy\">Caddy</a> (Go) and <a href=\"https://www.envoyproxy.io/\">Envoy</a> (C++).</p><p>All of these technologies have different feature sets, and are targeting some specific scenarios or hosting environments (for example Linkerd is fine-tuned for being used in Kubernetes).<br>In this post I'm not going to do a comparison of these, but rather just focus on one specific scenario: how to use Envoy as a load balancer for a service running in Kubernetes.</p><p><a href=\"https://www.envoyproxy.io/\">Envoy</a> is a \"high performance C++ distributed proxy\", originally implemented at Lyft, but since then have gained a wide adoption. It's high-performant, has a low resource footprint, it supports dynamic configuration managed by a \"control plane\" API, and provides some advanced features such as various load balancing algorithms, rate limiting, circuit braking, and shadow mirroring.</p><p>I chose Envoy as a load balancer proxy for a number of reasons.</p><ul><li>Besides being able to be controlled dynamically with a control plane API, it also supports a simple, hard-coded YAML-based configuration, which was convenient for my purposes, and made it easy to get started.</li><li>It has built-in support for a service discovery technique it calls <code>STRICT_DNS</code>, which builds on querying a DNS record, and expecting seeing an A record with an IP address for every node of the upstream cluster. This made it easy to use with a headless service in Kubernetes.</li><li>It supports various load balancing algorithms, among others \"Least Request\".</li></ul><p>Before starting to use Envoy, I was accessing my service in Kubernetes through a <code>service</code> object of the type <code>LoadBalancer</code>, which is a pretty typical way to access services from the outside in Kubernetes. The exact way a load balancer service works depends on the hosting environmentif it supports it in the first place. I was using the Google Kubernetes Engine, where every load balancer service is mapped to a TCP-level Google Cloud load balancer, which only supports a <em>round robin</em> load balancing algorithm.</p><p>In my case this was an issue, because my service had the following characteristics.</p><ul><li>The requests were long running with varied response times, ranging from 100ms to seconds.</li><li>The processing of the requests was CPU-intensive, practically the processing of one request used 100% of one CPU core.</li><li>Processing many requests in parallel degraded the response time. (This was due to the internals of how this service worked, it couldn't efficiently run more than a handful requests in parallel.)</li></ul><p>Due to the above characteristics, the round robin load balancing algorithm was not a good fit, because oftenby chancemultiple requests ended up on the same node, which made the average response times much worse than what the cluster would've been capable achieving, given a more uniformly spred out load.</p><p>In the remainder of this post I will describe the steps necessary to deploy Envoy to be used as a load balancer in front of a service running in Kubernetes.</p><h2 id=\"1-create-the-headless-service-for-our-application\">1. Create the headless service for our application</h2><p>In Kubernetes there is a specific kind of service called a <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/#headless-services\">headless service</a><a>,</a> which happens to be very convenient to be used together with Envoy's <code>STRICT_DNS</code> service discovery mode.</p><p>A headless service doesn't provide a single IP and load balancing to the underlying pods, but rather it just has DNS configuration which gives us an A record with the pod's IP address for all the pods matching the label selector. <br>This service type is intended to be used in scenarios when we want to implement load balancing, and maintaining the connections to the upstream pods ourselves, which is exactly what we can do with Envoy.</p><p>We can create a headless service by setting the <code>.spec.clusterIP</code> field to <code>\"None\"</code>. So assuming that our application pods have the label <code>app</code> with the value <code>myapp</code>, we can create the headless service with the following yaml.</p><pre><code class=\"language-yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  name: myapp\nspec:\n  clusterIP: None\n  ports:\n  - name: http\n    port: 80\n    targetPort: http\n    protocol: TCP\n  selector:\n    app: myapp\n</code></pre>\n<p>(The name of the <code>Service</code> does not have to be equal to the name of our application nor the <code>app</code> label, but it's a good convention to follow.)</p><p>Now if we check the DNS records for our service inside the Kubernetes cluster, we'll see the separate A records with the IP addresses. If we have 3 pods, we'll see a DNS summary similar to this.</p><pre><code class=\"language-bash\">$ nslookup myapp\nServer:         10.40.0.10\nAddress:        10.40.0.10#53\n\nNon-authoritative answer:\nName:   myapp.namespace.svc.cluster.local\nAddress: 10.36.224.5\nName:   myapp.namespace.svc.cluster.local\nAddress: 10.38.187.17\nName:   myapp.namespace.svc.cluster.local\nAddress: 10.38.1.8\n</code></pre>\n<p>And the way the <code>STRICT_DNS</code> service discovery of Envoy works is that it maintains the IP address of all the A records returned by the DNS, and it refreshes the set of IPs every couple of seconds.</p><h2 id=\"2-create-the-envoy-image\">2. Create the Envoy image</h2><p>The simplest way to use Envoy without providing the control plane in the form of a dynamic API is to add the hardcoded configuration to a static yaml file.</p><p>The following is a basic configuration that load balances to the IP addresses given by the domain name <code>myapp</code>.</p><pre><code class=\"language-yaml\">admin:\n  access_log_path: /tmp/admin_access.log\n  address:\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\n\nstatic_resources:\n  listeners:\n  - name: listener_0\n    address:\n      socket_address: { address: 0.0.0.0, port_value: 80 }\n    filter_chains:\n    - filters:\n      - name: envoy.http_connection_manager\n        config:\n          stat_prefix: ingress_http\n          route_config:\n            name: local_route\n            virtual_hosts:\n            - name: local_service\n              domains: [&quot;*&quot;]\n              routes:\n              - match: { prefix: &quot;/&quot; }\n                route: { host_rewrite: myapp, cluster: myapp_cluster, timeout: 60s }\n          http_filters:\n          - name: envoy.router\n  clusters:\n  - name: myapp_cluster\n    connect_timeout: 0.25s\n    type: STRICT_DNS\n    dns_lookup_family: V4_ONLY\n    lb_policy: LEAST_REQUEST\n    hosts: [{ socket_address: { address: myapp, port_value: 80 }}]\n</code></pre>\n<p>Note the following parts.</p><ul><li><code>type: STRICT_DNS</code>: Here we're specifying the service discovery type. It's important to set it to <code>STRICT_DNS</code>, because this works well together with the headless service we've set up.</li><li><code>lb_policy: LEAST_REQUEST</code>: We can choose from various <a href=\"https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/load_balancing.html\">load balancing algorithms</a>, probably <code>ROUND_ROBIN</code> and <code>LEAST_REQUEST</code> are the most common. (Keep in mind that <code>LEAST_REQUEST</code> is not checking all upstream nodes, but it only chooses from <a href=\"https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/load_balancing.html#weighted-least-request\">2 randomly picked options</a>.)</li><li><code>hosts: [{ socket_address: { address: myapp, port_value: 80 }}]</code>: This is the part where we specify with the <code>address</code> field the domain name from which Envoy will have to get the A records to route to.</li></ul><p>You can find more information about the various config parameters in the <a href=\"https://www.envoyproxy.io/docs/envoy/latest/api-v2/api\">docs</a><a>.</a></p><p>Now we have to put the following <code>Dockerfile</code> next to the <code>envoy.yaml</code> config file.</p><pre><code class=\"language-dockerfile\">FROM envoyproxy/envoy:latest\nCOPY envoy.yaml /etc/envoy.yaml\nCMD /usr/local/bin/envoy -c /etc/envoy.yaml\n</code></pre>\n<p>The last step is building the image, and pushing it somewhere (like the Docker hub, or the container registry of a cloud provider) to be able to use it from Kubernetes.<br>Assuming I'd like to push this to my personal Docker hub account, I could do it with the following commands.</p><pre><code>$ docker build -t markvincze/myapp-envoy:1 .\n$ docker push markvincze/myapp-envoy:1</code></pre><h2 id=\"-3-optional-make-the-envoy-image-parameterizable-\">(3. Optional: make the Envoy image parameterizable)</h2><p>If we'd like to be able to customize some parts of the Envoy configuration with environment variables without rebuilding the Docker image, we can do some env var substitution in the yaml config. Let's say we'd like to be able to customize the name of the headless service we're proxying to, and the load balancer algorithm, then we'd have to modify the yaml config the following way.</p><pre><code class=\"language-yaml\">admin:\n  access_log_path: /tmp/admin_access.log\n  address:\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\n\nstatic_resources:\n  listeners:\n  - name: listener_0\n    address:\n      socket_address: { address: 0.0.0.0, port_value: 80 }\n    filter_chains:\n    - filters:\n      - name: envoy.http_connection_manager\n        config:\n          stat_prefix: ingress_http\n          route_config:\n            name: local_route\n            virtual_hosts:\n            - name: local_service\n              domains: [&quot;*&quot;]\n              routes:\n              - match: { prefix: &quot;/&quot; }\n                route: { host_rewrite: myapp, cluster: myapp_cluster, timeout: 60s }\n          http_filters:\n          - name: envoy.router\n  clusters:\n  - name: myapp_cluster\n    connect_timeout: 0.25s\n    type: STRICT_DNS\n    dns_lookup_family: V4_ONLY\n    lb_policy: ${ENVOY_LB_ALG}\n    hosts: [{ socket_address: { address: ${SERVICE_NAME}, port_value: 80 }}]\n</code></pre>\n<p>Then implement a little shell script (<code>docker-entrypoint.sh</code>) in which we do the env var substitution.</p><pre><code class=\"language-bash\">#!/bin/sh\nset -e\n\necho &quot;Generating envoy.yaml config file...&quot;\ncat /tmpl/envoy.yaml.tmpl | envsubst \\$ENVOY_LB_ALG,\\$SERVICE_NAME &gt; /etc/envoy.yaml\n\necho &quot;Starting Envoy...&quot;\n/usr/local/bin/envoy -c /etc/envoy.yaml\n</code></pre>\n<p>And change our Dockerfile to run this script instead of directly starting Envoy.</p><pre><code>FROM envoyproxy/envoy:latest\n\nCOPY envoy.yaml /tmpl/envoy.yaml.tmpl\nCOPY docker-entrypoint.sh /\n\nRUN chmod 500 /docker-entrypoint.sh\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install gettext -y\n\nENTRYPOINT [\"/docker-entrypoint.sh\"]</code></pre><p>Keep in mind that if you use this approach, you have to specify these env vars in the Kubernetes deployment, otherwise they will be empty.</p><h2 id=\"4-create-the-envoy-deployment\">4. Create the Envoy deployment</h2><p>Finally we have to create a deployment for Envoy itself.</p><pre><code class=\"language-yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  name: myapp-envoy\n  labels:\n    app: myapp-envoy\nspec:\n  type: ClusterIP\n  ports:\n  - name: http\n    port: 80\n    targetPort: http\n    protocol: TCP\n  selector:\n    app: myapp-envoy\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: myapp-envoy\n  labels:\n    app: myapp-envoy\nspec:\n  selector:\n    matchLabels:\n      app: myapp-envoy\n  template:\n    metadata:\n      labels:\n        app: myapp-envoy\n    spec:\n      containers:\n      - name: myapp-envoy\n        image: mydockerhub/mycustomenvoy\n        imagePullPolicy: Always\n        env:\n        - name: &quot;ENVOY_LB_ALG&quot;\n          value: &quot;LEAST_REQUEST&quot;\n        - name: &quot;SERVICE_NAME&quot;\n          value: &quot;myapp&quot;\n        ports:\n        - name: http\n          containerPort: 80\n        - name: envoy-admin\n          containerPort: 9901\n</code></pre>\n<p>(We only need the env variables if we made our Envoy Docker image parameterizable.)</p><p>After applying this yaml, the Envoy proxy should be operational, and you can access the underlying service by sending the requests to the main port of the Envoy service.</p><p>In this example I only added a service of type ClusterIP, but you can also use a LoadBalancer service, or an Ingress object if you want to access the proxy from outside the cluster.</p><p>The following diagram illustrates the architecture of the whole setup.</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2018/10/architecture.png\" class=\"kg-image\"></figure><p>The image only shows one single Envoy pod, but you can scale it up to have more instances if necessary. And of course you can use a Horizontal Pod Autoscaler to automatically create more replicas as needed. (All instances will be autonomous and independent of each other.)<br>In practice you'll probably need much fewer instances for the proxy than for the underlying service. In the current production application we're using Envoy with, we're serving ~1000 requests/seconds on ~400 upstream pods, but we only have 3 instances of Envoy running, with ~10% CPU load.</p><h2 id=\"troubleshooting-and-monitoring\">Troubleshooting and Monitoring</h2><p>In the Envoy configuration file you can see an <code>admin:</code> section, which configures Envoy's admin endpoint. That can be used for checking various diagnostic information about the proxy.</p><p>(If you don't have a service publishing the admin port9901 by defaultyou can still access it by port-forwarding to a pod with <code>kubectl</code>. Assuming that one of the Envoy pods is called <code>myapp-envoy-656c8d5fff-mwff8</code>, then you can start port-fowarding with the command <code>kubectl port-forward myapp-envoy-656c8d5fff-mwff8 9901</code>. Then you can access the page at <code>http://localhost:9901</code>.)</p><p>Some of the useful endpoints:</p><ul><li><code>/config_dump</code>: Prints the complete configuration of the proxy, which is useful to verify if the correct config ended up on the pod.</li><li><code>/clusters</code>: Shows all the upstream nodes that Envoy discovered, and the number of requests processed for each of them. This is useful for example for checking if the load balancing algorithm works properly.</li></ul><p>One way to do monitoring is to use Prometheus to scrape the stats from the proxy pods. Envoy has built-in support for this, the Prometheus stats are published on the admin port at the route <code>/stats/prometheus</code>.</p><p>You can download a Grafana dashboard visualizing these metrics from <a href=\"https://github.com/transferwise/prometheus-envoy-dashboards\">this repository</a>, which will give you a set of graphs to the following.</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2018/10/image.png\" class=\"kg-image\"></figure><h2 id=\"a-word-on-the-load-balancing-algorithms\">A word on the load balancing algorithms</h2><p>The load balancing algorithm can have significant effect on the overall performance of our cluster. Using a least request algorithm can be beneficial with services for which an even spread of the load is necessary, for example when a service is CPU-intensive, and gets easily overloaded.<br>On the other hand, least request has the problem that if one of the nodes start failing for some reason, and the failure response times are quick, then the load balancer will send a disproportionately large portion of the requests to the failing nodesomething which wouldn't be a problem with round robin.</p><p>I made some benchmarks with a <a href=\"https://github.com/markvincze/PrimeCalcApi/\">dummy API</a> and compared the round robin and the least request LB algorithms. It turned out that least request can bring a significant improvement in the overall performance.</p><p>I was benchmarking the API for ~40 minutes with a continuously increasing input traffic. Throughout the benchmark I collected the following metrics:</p><ul><li>Number of requests being executed (\"requests in flight\"), broken down by servers</li><li>Number of requests being executed, average per server</li><li>Incoming request rate (this was increasing every 5 minutes)</li><li>Error rate (normally there weren't any, but when things started to slow down, this started to show some timeouts)</li><li>Response time percentiles (0.50, 0.90 and 0.99) recorded on the servers</li></ul><p>The stats looked like this with <code>ROUND_ROBIN</code>.</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2018/10/image-1.png\" class=\"kg-image\"></figure><p>These were the results with <code>LEAST_REQUEST</code>:</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2018/10/image-2.png\" class=\"kg-image\"></figure><p>You can see on the results that <code>LEAST_REQUEST</code> can lead to a much smoother distribution of the traffic across the nodes, and thus a lower average response time at high load. </p><p>The exact improvement will depend on the actual API, so I definitely recommend you to do the benchmark with your own service too to be make a decision.</p><p>(Earlier I mentioned that when using <code>LEAST_REQUEST</code>, Envoy will not look at every node in the cluster when selecting the one to send a certain request to, but rather just looks at <a href=\"https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/load_balancing.html#weighted-least-request\">2 randomly picked options</a>, and sends the request to the one that has the fewer pending requests.<br>I was curious if actually doing a full O(n) scan and picking among all the nodes the one with the least amount of requests would bring a further improvement, so I implemented this feature in a <a href=\"https://github.com/markvincze/envoy/pull/1\">fork of Envoy</a><a>,</a> and did the same benchmark.<br>It turns out that this can indeed further improve the uniformity of the load, and the resource utilization, thereby decreasing the response times at high load. There is an ongoing discussion about this topic in <a href=\"https://github.com/envoyproxy/envoy/issues/4481\">this issue</a>.)</p><h2 id=\"summary\">Summary</h2><p>I hope this introduction will be helpful for getting started with using Envoy in Kubernetes. By the way, this is not the only way to achieve least request load balancing on Kubernetes. There are various <a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress/\">ingress controllers</a><a> </a>(one of which is <a href=\"https://github.com/datawire/ambassador\">Ambassador</a>, which builds on top of Envoy) that can do the same.<br>If you have any other suggestions, feedback is welcome!</p>",
        "comment_id": "5bb0a47f63bf8a66478d78d6",
        "plaintext": "In today's highly distributed word, where monolithic architectures are\nincreasingly replaced with multiple, smaller, interconnected services (for\nbetter or worse), proxy and load balancing technologies seem to have a\nrenaissance. Beside the older players, there are several new proxy technologies\npopping up in recent years, implemented in various technologies, popularizing\nthemselves with diferent features, such as easy integration to certain cloud\nproviders (\"cloud-native\"), high performance and low memory footprint, or\ndynamic configuration.\n\nArguably the two most popular \"classic\" proxy technologies are NGINX\n[https://www.nginx.com/]  (C) and HAProxy [http://www.haproxy.org/]  (C), while\nsome of the new kids on the block are Zuul [https://github.com/Netflix/zuul] \n(Java), Linkerd [https://linkerd.io/]  (Rust), Traefik [https://traefik.io/] \n(Go), Caddy [https://github.com/mholt/caddy]  (Go) and Envoy\n[https://www.envoyproxy.io/]  (C++).\n\nAll of these technologies have different feature sets, and are targeting some\nspecific scenarios or hosting environments (for example Linkerd is fine-tuned\nfor being used in Kubernetes).\nIn this post I'm not going to do a comparison of these, but rather just focus on\none specific scenario: how to use Envoy as a load balancer for a service running\nin Kubernetes.\n\nEnvoy [https://www.envoyproxy.io/]  is a \"high performance C++ distributed\nproxy\", originally implemented at Lyft, but since then have gained a wide\nadoption. It's high-performant, has a low resource footprint, it supports\ndynamic configuration managed by a \"control plane\" API, and provides some\nadvanced features such as various load balancing algorithms, rate limiting,\ncircuit braking, and shadow mirroring.\n\nI chose Envoy as a load balancer proxy for a number of reasons.\n\n * Besides being able to be controlled dynamically with a control plane API, it\n   also supports a simple, hard-coded YAML-based configuration, which was\n   convenient for my purposes, and made it easy to get started.\n * It has built-in support for a service discovery technique it calls STRICT_DNS\n   , which builds on querying a DNS record, and expecting seeing an A record\n   with an IP address for every node of the upstream cluster. This made it easy\n   to use with a headless service in Kubernetes.\n * It supports various load balancing algorithms, among others \"Least Request\".\n\nBefore starting to use Envoy, I was accessing my service in Kubernetes through a\n service  object of the type LoadBalancer, which is a pretty typical way to\naccess services from the outside in Kubernetes. The exact way a load balancer\nservice works depends on the hosting environmentif it supports it in the first\nplace. I was using the Google Kubernetes Engine, where every load balancer\nservice is mapped to a TCP-level Google Cloud load balancer, which only supports\na round robin  load balancing algorithm.\n\nIn my case this was an issue, because my service had the following\ncharacteristics.\n\n * The requests were long running with varied response times, ranging from 100ms\n   to seconds.\n * The processing of the requests was CPU-intensive, practically the processing\n   of one request used 100% of one CPU core.\n * Processing many requests in parallel degraded the response time. (This was\n   due to the internals of how this service worked, it couldn't efficiently run\n   more than a handful requests in parallel.)\n\nDue to the above characteristics, the round robin load balancing algorithm was\nnot a good fit, because oftenby chancemultiple requests ended up on the same\nnode, which made the average response times much worse than what the cluster\nwould've been capable achieving, given a more uniformly spred out load.\n\nIn the remainder of this post I will describe the steps necessary to deploy\nEnvoy to be used as a load balancer in front of a service running in Kubernetes.\n\n1. Create the headless service for our application\nIn Kubernetes there is a specific kind of service called a headless service, \nwhich happens to be very convenient to be used together with Envoy's STRICT_DNS \nservice discovery mode.\n\nA headless service doesn't provide a single IP and load balancing to the\nunderlying pods, but rather it just has DNS configuration which gives us an A\nrecord with the pod's IP address for all the pods matching the label selector.\nThis service type is intended to be used in scenarios when we want to implement\nload balancing, and maintaining the connections to the upstream pods ourselves,\nwhich is exactly what we can do with Envoy.\n\nWe can create a headless service by setting the .spec.clusterIP  field to \"None\"\n. So assuming that our application pods have the label app  with the value myapp\n, we can create the headless service with the following yaml.\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapp\nspec:\n  clusterIP: None\n  ports:\n  - name: http\n    port: 80\n    targetPort: http\n    protocol: TCP\n  selector:\n    app: myapp\n\n\n(The name of the Service  does not have to be equal to the name of our\napplication nor the app  label, but it's a good convention to follow.)\n\nNow if we check the DNS records for our service inside the Kubernetes cluster,\nwe'll see the separate A records with the IP addresses. If we have 3 pods, we'll\nsee a DNS summary similar to this.\n\n$ nslookup myapp\nServer:         10.40.0.10\nAddress:        10.40.0.10#53\n\nNon-authoritative answer:\nName:   myapp.namespace.svc.cluster.local\nAddress: 10.36.224.5\nName:   myapp.namespace.svc.cluster.local\nAddress: 10.38.187.17\nName:   myapp.namespace.svc.cluster.local\nAddress: 10.38.1.8\n\n\nAnd the way the STRICT_DNS  service discovery of Envoy works is that it\nmaintains the IP address of all the A records returned by the DNS, and it\nrefreshes the set of IPs every couple of seconds.\n\n2. Create the Envoy image\nThe simplest way to use Envoy without providing the control plane in the form of\na dynamic API is to add the hardcoded configuration to a static yaml file.\n\nThe following is a basic configuration that load balances to the IP addresses\ngiven by the domain name myapp.\n\nadmin:\n  access_log_path: /tmp/admin_access.log\n  address:\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\n\nstatic_resources:\n  listeners:\n  - name: listener_0\n    address:\n      socket_address: { address: 0.0.0.0, port_value: 80 }\n    filter_chains:\n    - filters:\n      - name: envoy.http_connection_manager\n        config:\n          stat_prefix: ingress_http\n          route_config:\n            name: local_route\n            virtual_hosts:\n            - name: local_service\n              domains: [\"*\"]\n              routes:\n              - match: { prefix: \"/\" }\n                route: { host_rewrite: myapp, cluster: myapp_cluster, timeout: 60s }\n          http_filters:\n          - name: envoy.router\n  clusters:\n  - name: myapp_cluster\n    connect_timeout: 0.25s\n    type: STRICT_DNS\n    dns_lookup_family: V4_ONLY\n    lb_policy: LEAST_REQUEST\n    hosts: [{ socket_address: { address: myapp, port_value: 80 }}]\n\n\nNote the following parts.\n\n * type: STRICT_DNS: Here we're specifying the service discovery type. It's\n   important to set it to STRICT_DNS, because this works well together with the\n   headless service we've set up.\n * lb_policy: LEAST_REQUEST: We can choose from various load balancing\n   algorithms\n   [https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/load_balancing.html]\n   , probably ROUND_ROBIN  and LEAST_REQUEST  are the most common. (Keep in mind\n   that LEAST_REQUEST  is not checking all upstream nodes, but it only chooses\n   from 2 randomly picked options.)\n * hosts: [{ socket_address: { address: myapp, port_value: 80 }}]: This is the\n   part where we specify with the address  field the domain name from which\n   Envoy will have to get the A records to route to.\n\nYou can find more information about the various config parameters in the docs\n[https://www.envoyproxy.io/docs/envoy/latest/api-v2/api].\n\nNow we have to put the following Dockerfile  next to the envoy.yaml  config\nfile.\n\nFROM envoyproxy/envoy:latest\nCOPY envoy.yaml /etc/envoy.yaml\nCMD /usr/local/bin/envoy -c /etc/envoy.yaml\n\n\nThe last step is building the image, and pushing it somewhere (like the Docker\nhub, or the container registry of a cloud provider) to be able to use it from\nKubernetes.\nAssuming I'd like to push this to my personal Docker hub account, I could do it\nwith the following commands.\n\n$ docker build -t markvincze/myapp-envoy:1 .\n$ docker push markvincze/myapp-envoy:1\n\n(3. Optional: make the Envoy image parameterizable)\nIf we'd like to be able to customize some parts of the Envoy configuration with\nenvironment variables without rebuilding the Docker image, we can do some env\nvar substitution in the yaml config. Let's say we'd like to be able to customize\nthe name of the headless service we're proxying to, and the load balancer\nalgorithm, then we'd have to modify the yaml config the following way.\n\nadmin:\n  access_log_path: /tmp/admin_access.log\n  address:\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\n\nstatic_resources:\n  listeners:\n  - name: listener_0\n    address:\n      socket_address: { address: 0.0.0.0, port_value: 80 }\n    filter_chains:\n    - filters:\n      - name: envoy.http_connection_manager\n        config:\n          stat_prefix: ingress_http\n          route_config:\n            name: local_route\n            virtual_hosts:\n            - name: local_service\n              domains: [\"*\"]\n              routes:\n              - match: { prefix: \"/\" }\n                route: { host_rewrite: myapp, cluster: myapp_cluster, timeout: 60s }\n          http_filters:\n          - name: envoy.router\n  clusters:\n  - name: myapp_cluster\n    connect_timeout: 0.25s\n    type: STRICT_DNS\n    dns_lookup_family: V4_ONLY\n    lb_policy: ${ENVOY_LB_ALG}\n    hosts: [{ socket_address: { address: ${SERVICE_NAME}, port_value: 80 }}]\n\n\nThen implement a little shell script (docker-entrypoint.sh) in which we do the\nenv var substitution.\n\n#!/bin/sh\nset -e\n\necho \"Generating envoy.yaml config file...\"\ncat /tmpl/envoy.yaml.tmpl | envsubst \\$ENVOY_LB_ALG,\\$SERVICE_NAME > /etc/envoy.yaml\n\necho \"Starting Envoy...\"\n/usr/local/bin/envoy -c /etc/envoy.yaml\n\n\nAnd change our Dockerfile to run this script instead of directly starting Envoy.\n\nFROM envoyproxy/envoy:latest\n\nCOPY envoy.yaml /tmpl/envoy.yaml.tmpl\nCOPY docker-entrypoint.sh /\n\nRUN chmod 500 /docker-entrypoint.sh\n\nRUN apt-get update && \\\n    apt-get install gettext -y\n\nENTRYPOINT [\"/docker-entrypoint.sh\"]\n\nKeep in mind that if you use this approach, you have to specify these env vars\nin the Kubernetes deployment, otherwise they will be empty.\n\n4. Create the Envoy deployment\nFinally we have to create a deployment for Envoy itself.\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapp-envoy\n  labels:\n    app: myapp-envoy\nspec:\n  type: ClusterIP\n  ports:\n  - name: http\n    port: 80\n    targetPort: http\n    protocol: TCP\n  selector:\n    app: myapp-envoy\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: myapp-envoy\n  labels:\n    app: myapp-envoy\nspec:\n  selector:\n    matchLabels:\n      app: myapp-envoy\n  template:\n    metadata:\n      labels:\n        app: myapp-envoy\n    spec:\n      containers:\n      - name: myapp-envoy\n        image: mydockerhub/mycustomenvoy\n        imagePullPolicy: Always\n        env:\n        - name: \"ENVOY_LB_ALG\"\n          value: \"LEAST_REQUEST\"\n        - name: \"SERVICE_NAME\"\n          value: \"myapp\"\n        ports:\n        - name: http\n          containerPort: 80\n        - name: envoy-admin\n          containerPort: 9901\n\n\n(We only need the env variables if we made our Envoy Docker image\nparameterizable.)\n\nAfter applying this yaml, the Envoy proxy should be operational, and you can\naccess the underlying service by sending the requests to the main port of the\nEnvoy service.\n\nIn this example I only added a service of type ClusterIP, but you can also use a\nLoadBalancer service, or an Ingress object if you want to access the proxy from\noutside the cluster.\n\nThe following diagram illustrates the architecture of the whole setup.\n\nThe image only shows one single Envoy pod, but you can scale it up to have more\ninstances if necessary. And of course you can use a Horizontal Pod Autoscaler to\nautomatically create more replicas as needed. (All instances will be autonomous\nand independent of each other.)\nIn practice you'll probably need much fewer instances for the proxy than for the\nunderlying service. In the current production application we're using Envoy\nwith, we're serving ~1000 requests/seconds on ~400 upstream pods, but we only\nhave 3 instances of Envoy running, with ~10% CPU load.\n\nTroubleshooting and Monitoring\nIn the Envoy configuration file you can see an admin:  section, which configures\nEnvoy's admin endpoint. That can be used for checking various diagnostic\ninformation about the proxy.\n\n(If you don't have a service publishing the admin port9901 by defaultyou can\nstill access it by port-forwarding to a pod with kubectl. Assuming that one of\nthe Envoy pods is called myapp-envoy-656c8d5fff-mwff8, then you can start\nport-fowarding with the command kubectl port-forward\nmyapp-envoy-656c8d5fff-mwff8 9901. Then you can access the page at \nhttp://localhost:9901.)\n\nSome of the useful endpoints:\n\n * /config_dump: Prints the complete configuration of the proxy, which is useful\n   to verify if the correct config ended up on the pod.\n * /clusters: Shows all the upstream nodes that Envoy discovered, and the number\n   of requests processed for each of them. This is useful for example for\n   checking if the load balancing algorithm works properly.\n\nOne way to do monitoring is to use Prometheus to scrape the stats from the proxy\npods. Envoy has built-in support for this, the Prometheus stats are published on\nthe admin port at the route /stats/prometheus.\n\nYou can download a Grafana dashboard visualizing these metrics from this\nrepository [https://github.com/transferwise/prometheus-envoy-dashboards], which\nwill give you a set of graphs to the following.\n\nA word on the load balancing algorithms\nThe load balancing algorithm can have significant effect on the overall\nperformance of our cluster. Using a least request algorithm can be beneficial\nwith services for which an even spread of the load is necessary, for example\nwhen a service is CPU-intensive, and gets easily overloaded.\nOn the other hand, least request has the problem that if one of the nodes start\nfailing for some reason, and the failure response times are quick, then the load\nbalancer will send a disproportionately large portion of the requests to the\nfailing nodesomething which wouldn't be a problem with round robin.\n\nI made some benchmarks with a dummy API\n[https://github.com/markvincze/PrimeCalcApi/]  and compared the round robin and\nthe least request LB algorithms. It turned out that least request can bring a\nsignificant improvement in the overall performance.\n\nI was benchmarking the API for ~40 minutes with a continuously increasing input\ntraffic. Throughout the benchmark I collected the following metrics:\n\n * Number of requests being executed (\"requests in flight\"), broken down by\n   servers\n * Number of requests being executed, average per server\n * Incoming request rate (this was increasing every 5 minutes)\n * Error rate (normally there weren't any, but when things started to slow down,\n   this started to show some timeouts)\n * Response time percentiles (0.50, 0.90 and 0.99) recorded on the servers\n\nThe stats looked like this with ROUND_ROBIN.\n\nThese were the results with LEAST_REQUEST:\n\nYou can see on the results that LEAST_REQUEST  can lead to a much smoother\ndistribution of the traffic across the nodes, and thus a lower average response\ntime at high load. \n\nThe exact improvement will depend on the actual API, so I definitely recommend\nyou to do the benchmark with your own service too to be make a decision.\n\n(Earlier I mentioned that when using LEAST_REQUEST, Envoy will not look at every\nnode in the cluster when selecting the one to send a certain request to, but\nrather just looks at 2 randomly picked options, and sends the request to the one\nthat has the fewer pending requests.\nI was curious if actually doing a full O(n) scan and picking among all the nodes\nthe one with the least amount of requests would bring a further improvement, so\nI implemented this feature in a fork of Envoy\n[https://github.com/markvincze/envoy/pull/1],  and did the same benchmark.\nIt turns out that this can indeed further improve the uniformity of the load,\nand the resource utilization, thereby decreasing the response times at high\nload. There is an ongoing discussion about this topic in this issue\n[https://github.com/envoyproxy/envoy/issues/4481].)\n\nSummary\nI hope this introduction will be helpful for getting started with using Envoy in\nKubernetes. By the way, this is not the only way to achieve least request load\nbalancing on Kubernetes. There are various ingress controllers\n[https://kubernetes.io/docs/concepts/services-networking/ingress/]  (one of\nwhich is Ambassador [https://github.com/datawire/ambassador], which builds on\ntop of Envoy) that can do the same.\nIf you have any other suggestions, feedback is welcome!",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "An introduction to using Envoy as a load balancer in Kubernetes, and configuring various load balancing algorithms.",
        "author_id": "1",
        "created_at": "2018-09-30 10:25:03",
        "created_by": "1",
        "updated_at": "2018-10-16 20:58:16",
        "updated_by": "1",
        "published_at": "2018-10-05 23:07:49",
        "published_by": "1",
        "custom_excerpt": "An introduction to using Envoy as a load balancer in Kubernetes, and configuring various load balancing algorithms.",
        "codeinjection_head": "",
        "codeinjection_foot": "",
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "5c32318aa26b5f5ce4856be0",
        "uuid": "851eb291-2356-40b4-8d10-c9d942cca925",
        "title": "Graceful termination in Kubernetes with ASP.NET Core",
        "slug": "graceful-termination-in-kubernetes-with-asp-net-core",
        "mobiledoc": "{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"markdown\":\"Using a container-orchestration technology like Kubernetes, running applications in small containers, and scaling out horizontally rather than scaling a single machine up has numerous benefits, such as flexible allocation of the raw resources among different services, being able to precisely adjust the number of instances we're running according to the volume of traffic we're receiving, and forcing us to run our applications in immutable containers, thereby making our releases repeatable, thus easier to reason about.\\n\\nOn the other hand, we also face several challenges inherent to this architectural style, due to our components being inevitably distributed, and our containers constantly being shuffled around on the \\\"physical\\\" (technically, most probably still virtual) nodes.\\n\\nOne of these challenges is that due to Kubernetes constantly scaling our services up and down, and possible evicting our pods from certain nodes and moving them to another, instances of our services will constantly be stopped and then started up.  \\nThis is something we have to worry about much less if we have a fixed number of server machines to which we're directly deploying our application. In that case, our services will only be stopped when we're deploying, or in some exceptional cases like if they crash, or the server machines have to be restarted for some reason.\\n\\nThis requires a mental shift. When implementing a service, we always have to think it through how it will behave if it's suddenly stopped. Will it leave something in an inconsistent state that we have to clean up? Can it be doing a long-running operation that we need to cancel? Do we have to send a signal to notify some other component that this instance is stopping?\\n\\nIn this post I'd like to focus only on the simplest scenario: we have a REST API implemented in ASP.NET Core, which doesn't have any of the above issues. It's stateless, it doesn't run any background processes, it only works in the request-response model, through some HTTP endpoints.\\n\\nEven in this simplest scenario there is an important issue: at the point in time when our application is stopped, there might be some requests being processed. We have to ensure that ASP.NET Core (or Kubernetes) waits with killing our instance until they are completed.\\n\\nIn the rest of the post I'd like to describe the facilities Kubernetes provides to handle this scenario, and the way we can utilize them in ASP.NET Core.\\n\\n*I did all the testing on the Google Kubernetes Engine. Since this is related to networking, which to some extent depends on where and how we're hosting Kubernetes, I can imagine that this might work differently depending on where we run, let's say Amazon, Azure, etc.*\\n\\n# Graceful termination in Kubernetes\\n\\nKubernetes implements a specific order of events when it terminates any pod, so that we can set up our containers in a way that they have a chance to gracefully terminate.\\n\\nThe process is documented in [this section](https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods). I have to say that this part of the documentation has always been a bit confusing to me, and some parts I could figure out only through experimentation.\\n\\nThe basic idea is that when the pod is stopped, a grace period starts (which has the default length 30 seconds), during which the pod has a chance to do any necessary cleanup. If the containers terminate faster then that, then the pod is deleted as soon as all the containers exited, so Kubernetes doesn't necessarily waits for the full grace period. And if the containers don't terminate until the grace period passes, then Kubernetes forcefully stops all the containers, and deletes the pod.\\n\\nWe have two ways to run custom operations on shutdown, and prolong the termination of the container (but no longer than the full grace period):\\n\\n - We can specify a `preStop` hook for our container, in which we can run an arbitrary shell command. (Or execute a script file we included in our Docker image.)\\n - The container receives a `TERM` signal, which we can handle in our application code. This is something we'll have to figure out how to properly do in the technology of our choice, it'll be different in .NET, NodeJS, Go, etc. In this post we'll see the specificities of ASP.NET Core.\\n\\nWe can use either of these approaches, or even both of them. It's important to remember that they *don't* happen in parallel, but rather sequentially (starting with the `preStop` hook), and they are confined by the same grace period together, not individually. So if the `preStop` hook uses up all the time in the grace period, we might not have enough time left to properly handle the `TERM` signal. *(If the `preStop` hook uses up all the time, then the `TERM` handler gets an extra 2 seconds to run.)*\\n\\nAnd on top of all this, at some point the pod is removed from the list of Endpoints of the service objects.\\n\\nAn important detail is that the removal of the Endpoint is happening in parallel with `preStop` hook and the `TERM` signal, so there is no guarantee the load balancer won't send new requests to the pod being stopped, even after the `preStop` hook and the `TERM` signal was handled. (I couldn't find much official information on when this happens exactly, for example in [this tutorial video](https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-terminating-with-grace) it's not mentioned at all.)\\n\\nAll this being said, I think this description leaves a lot up to interpretation regarding how this works in practice, so I decided to try to test what actually happens in an ASP.NET Core application.\\n\\n# ASP.NET Core\\n\\nWhat I set out to do was to test what the proper way is to handle pod termination if our container is running an ASP.NET Core application.\\n\\nI particularly wanted to find an answer to these questions:\\n\\n - How can we handle the `TERM` signal in our C# code?\\n - If we don't implement a custom `TERM` handler, *and* there are requests in flight when Kubernetes sends the `TERM`, does our application stop immediately, or does ASP.NET Core do the \\\"right\\\" thing, and wait for all current requests to finish (draining)?\\n - In Kubernetes, can it happen that the load balancer still sends requests to our pod after we received `TERM`?\\n\\nTo be able to test these questions, I created a simple ASP.NET Core application, which simply logs a message to the console on every request, and has two endpoints.\\n\\n```csharp\\napp.Run(async (context) =>\\n{\\n    PrintRequestLog();\\n\\n    if (context.Request.Path.Value.Contains(\\\"slow\\\"))\\n    {\\n        await SleepAndPrintForSeconds(10);\\n    }\\n    else\\n    {\\n        await Task.Delay(100);\\n    }\\n\\n    await context.Response.WriteAsync(message);\\n});\\n```\\n\\n - The endpoint `/slow` responds after 10 seconds, and prints a log message every seconds while it's waiting. We can use this to test what happens to requests in flight when we receive the `TERM` signal.\\n - Any other route responds much quicker (after 100ms), and prints one message. By constantly polling this endpoint we can check if our pod still receives request after the `TERM` signal.\\n\\nIf we keep polling the \\\"slow\\\" endpoint, the output looks like this:\\n\\n```bash\\n1/6/19 1:50:22 PM: Incoming request at /slow, State: Running\\n1/6/19 1:50:22 PM: Sleeping (10 seconds left)\\n1/6/19 1:50:23 PM: Sleeping (9 seconds left)\\n1/6/19 1:50:24 PM: Sleeping (8 seconds left)\\n...\\n1/6/19 1:50:30 PM: Sleeping (2 seconds left)\\n1/6/19 1:50:31 PM: Sleeping (1 seconds left)\\n1/6/19 1:50:32 PM: Incoming request at /slow, State: Running\\n1/6/19 1:50:32 PM: Sleeping (10 seconds left)\\n1/6/19 1:50:33 PM: Sleeping (9 seconds left)\\n1/6/19 1:50:34 PM: Sleeping (8 seconds left)\\n...\\n1/6/19 1:50:40 PM: Sleeping (2 seconds left)\\n1/6/19 1:50:41 PM: Sleeping (1 seconds left)\\n1/6/19 1:50:42 PM: Incoming request at /slow, State: Running\\n1/6/19 1:50:42 PM: Sleeping (10 seconds left)\\n1/6/19 1:50:43 PM: Sleeping (9 seconds left)\\n...\\n```\\n\\nAnd if we poll the quick endpoint, we only get one log message per request.\\n\\n```bash\\n1/6/19 1:52:49 PM: Incoming request at /, State: Running\\n1/6/19 1:52:50 PM: Incoming request at /, State: Running\\n1/6/19 1:52:50 PM: Incoming request at /, State: Running\\n1/6/19 1:52:50 PM: Incoming request at /, State: Running\\n1/6/19 1:52:50 PM: Incoming request at /, State: Running\\n```\\n\\n## Handle `TERM` in ASP.NET Core\\n\\nIn order to handle the `TERM` signal with custom C# code, we have to use the [`IApplicationLifetime`](https://github.com/aspnet/AspNetCore/blob/master/src/Hosting/Abstractions/src/IApplicationLifetime.cs) interface, which we can inject in our `Startup.Configure()` method. It provides 2 events related to termination, that we can register to. The documentation specifies these as the following.\\n\\n - ApplicationStopping: \\\"Triggered when the application host is performing a graceful shutdown. Requests may still be in flight. Shutdown will block until this event completes.\\\"\\n - ApplicationStopped: \\\"Triggered when the application host is performing a graceful shutdown. All requests should be complete at this point. Shutdown will block until this event completes.\\\"\\n\\nTo test how these events work, I added a console print to both of them.\\n\\n```csharp\\nappLifetime.ApplicationStopping.Register(() => Console.WriteLine(\\\"ApplicationStopping called\\\"));\\nappLifetime.ApplicationStopped.Register(() => Console.WriteLine(\\\"ApplicationStopped called\\\"));\\n```\\n\\nThese are the events we can use to sort out any custom cleanup before the application terminates. The way they work is that the ASP.NET Core process does not terminate until these event handlers return. But they have a timeout, which we'll see in a second.\\n\\nIf we send a `TERM` signal by executing `kill PID` to a running ASP.NET Core applicationwhich is not executing any requests at the momentthis is the output we'll see.\\n\\n```bash\\nHosting environment: Production\\nContent root path: /home/mvincze/K8sGracefulShutdownTester/src/K8sGracefulShutdownTester\\nNow listening on: http://[::]:5000\\nApplication started. Press Ctrl+C to shut down.\\n...\\nApplication is shutting down...\\n1/6/19 1:58:51 PM: ApplicationStopping called\\n1/6/19 1:58:51 PM: ApplicationStopped called\\n```\\n\\nThe line `Application is shutting down...` is printed by the framework when it receives `TERM`, and we get the next two lines in short succession, which are coming from our custom handlers, and then the process exits immediately.\\n\\nThe next thing to see is what happens if we try the same when there is a request being processed.\\n\\n## Wait for requests in flight to finish\\n\\nThe first thing I tested was what happens if our application receives the `TERM` signal when a request is being processed. I kept polling the `/slow` endpoint, and sent the `TERM` signal by executing `kill PID` when the app was in the middle of processing on of the slow requests. This was the output.\\n\\n```bash\\n1/6/19 1:43:39 PM: Incoming request at /slow, State: Running\\n1/6/19 1:43:39 PM: Sleeping (10 seconds left)\\n1/6/19 1:43:40 PM: Sleeping (9 seconds left)\\n1/6/19 1:43:41 PM: Sleeping (8 seconds left)\\n1/6/19 1:43:42 PM: Sleeping (7 seconds left)\\n1/6/19 1:43:43 PM: Sleeping (6 seconds left)\\n1/6/19 1:43:44 PM: Sleeping (5 seconds left)\\n1/6/19 1:43:45 PM: Sleeping (4 seconds left)\\nApplication is shutting down...\\n1/6/19 1:43:45 PM: ApplicationStopping called\\n1/6/19 1:43:46 PM: Sleeping (3 seconds left)\\n1/6/19 1:43:47 PM: Sleeping (2 seconds left)\\n1/6/19 1:43:48 PM: Sleeping (1 seconds left)\\n1/6/19 1:43:49 PM: ApplicationStopped called\\n```\\n\\nThis illustrates that if the `TERM` comes in when there is a request being processed, the framework waits and doesn't let the process terminate until the requests in flight finish. So this is taken care of by ASP.NET Core (I believe it's implemented in [`KestrelServer`](https://github.com/aspnet/AspNetCore/blob/master/src/Servers/Kestrel/Core/src/KestrelServer.cs#L172)), we don't have to implement custom code to achieve this.\\n\\nOne important thing to consider is that there is a timeout period until the framework is willing to wait for the pending requests. If I send the `TERM` when there is still more than 5 seconds left from the current request, this is what happens.\\n\\n```bash\\n1/6/19 2:14:45 PM: Incoming request at /slow, State: Running\\n1/6/19 2:14:45 PM: Sleeping (10 seconds left)\\n1/6/19 2:14:46 PM: Sleeping (9 seconds left)\\nApplication is shutting down...\\n1/6/19 2:14:46 PM: ApplicationStopping called\\n1/6/19 2:14:47 PM: Sleeping (8 seconds left)\\n1/6/19 2:14:48 PM: Sleeping (7 seconds left)\\n1/6/19 2:14:49 PM: Sleeping (6 seconds left)\\n1/6/19 2:14:50 PM: Sleeping (5 seconds left)\\n1/6/19 2:14:51 PM: Sleeping (4 seconds left)\\n1/6/19 2:14:52 PM: Sleeping (3 seconds left)\\n1/6/19 2:14:52 PM: ApplicationStopped called\\n```\\n\\nSo ASP.NET Core is not willing to wait infinitely for the pending requests to finish, after some timeout period it forcefully terminates the application, regardless of the requests in flight. (And this causes a failed request, the `HttpClient` I was using to poll threw an exception.)\\n\\nThe default timeout is [5 seconds](https://github.com/aspnet/AspNetCore/blob/master/src/Hosting/Hosting/src/Internal/WebHostOptions.cs#L70), but we can increase it by calling the [`UseShutdownTimeout()`](https://github.com/aspnet/AspNetCore/blob/d852e10293c0fbd3dfbf82be455c2cde683ec0de/src/Hosting/Abstractions/src/HostingAbstractionsWebHostBuilderExtensions.cs#L179) extension method on the `WebHostBuilder` in our `Program.Main()` method.\\n\\n## Use this in Kubernetes\\n\\nBased on the above, it seems that we're good to go without implementing anything custom. Kubernetes sends a `TERM` signal before it kills our pods, and is willing to wait 30 seconds. And ASP.NET Core doesn't exit until the pending requests are finished. And if the 5 second default timeout is not enough, because we anticipate having slower requests (which is not commonplace in REST APIs anyway), then we can increase it in `Program.Main`.\\n\\nThere is an issue though. Kubernetes sending `TERM`, and removing the pod from the service Endpoint pool happens in parallel. So the following order of events can happen.\\n\\n1. Kubernetes sends the `TERM` signal.\\n2. Let's say there are no pending requests, so our container terminates immediately.\\n3. The pod has not been removed from the service endpoint pool yet, so it tries to send a request to the terminated pod, which will fail.\\n\\nFirst let's verify if we indeed receive more incoming requests after `TERM`. To test this, I added a 10 second sleep to our `ApplicationStopping` handler.\\n\\n```csharp\\nappLifetime.ApplicationStopping.Register(() =>\\n{\\n    Log(\\\"ApplicationStopping called, sleeping for 10s\\\");\\n    Thread.Sleep(10000);\\n});\\n```\\n\\nThen I deployed two pods of the application, and kept polling the quick endpoint through a LoadBalancer service, then at some point stopped one of the pods by executing `kubectl delete pod`.\\n\\nThis was the output of the pod:\\n\\n```bash\\n01/06/2019 14:40:44: Incoming request at /, State: Running\\n01/06/2019 14:40:45: Incoming request at /, State: Running\\nApplication is shutting down...\\n01/06/2019 14:40:45: ApplicationStopping called, sleeping for 10s\\n01/06/2019 14:40:46: Incoming request at /, State: AfterSigterm\\n01/06/2019 14:40:47: Incoming request at /, State: AfterSigterm\\n01/06/2019 14:40:48: Incoming request at /, State: AfterSigterm\\n01/06/2019 14:40:49: Incoming request at /, State: AfterSigterm\\n01/06/2019 14:40:51: Incoming request at /, State: AfterSigterm\\n01/06/2019 14:40:53: Incoming request at /, State: AfterSigterm\\n01/06/2019 14:40:55: ApplicationStopped called\\n```\\n\\nThis clearly shows that Kubernetes doesn't wait for the `LoadBalancer` endpoints to be updated before it sends the `TERM` signal (possibly causing the application to terminate), so it is very much possible that we will keep receiving some requests after `TERM`.\\n\\nIf we remove the 10 second sleep from the `TERM` handler, and execute the same test, we'll see the following output.\\n\\n```bash\\n01/06/2019 14:36:07: Incoming request at /, State: Running\\n01/06/2019 14:36:07: Incoming request at /, State: Running\\n01/06/2019 14:36:07: Incoming request at /, State: Running\\nApplication is shutting down...\\n01/06/2019 14:36:08: ApplicationStopped called\\n01/06/2019 14:36:08: ApplicationStopping called\\n```\\n\\nAt the time of receiving the `TERM`, there happened to be no pending requests, so the process exited immediately.  \\nAnd this is what I've seen in the little console tool doing the polling:\\n\\n```bash\\n2019-01-06 2:36:08 PM: Successful request\\n2019-01-06 2:36:09 PM: Successful request\\n2019-01-06 2:36:11 PM: Error! Exception: The operation was canceled.\\n2019-01-06 2:36:11 PM: Successful request\\n2019-01-06 2:36:13 PM: Error! Exception: The operation was canceled.\\n2019-01-06 2:36:13 PM: Successful request\\n```\\n\\nBy chance, there were 2 request which were still sent to the old pod, in which the container has already stopped, thus the requests failed.  \\nI repeated this test a couple of times, and this is happening quite randomly. Sometime there were no failed requests, other times there were a couple of them. I guess it depends on how quickly the Endpoints of the LoadBalancer gets updated. Sometimes that happens early enough to prevent failed requests, sometimes it doesn't.\\n\\nThis means that the default behavior we get when we run an ASP.NET Core container in Kubernetes is not the best, it can result in a couple of failed requests, not just when we deploy a new service version, but also every time a pod is stopped, for any reason.\\n\\n## How quickly does the LB get updated?\\n\\nTo see how quickly the endpoints of the LB get updated, let's prolong the grace period close to the maximum 30 seconds, and test how long the pod is still receiving requests.\\n\\nWe can keep the pod living for almost the default 30 second grace period by increasing the sleep duration in `ApplicationStopping` to 25 seconds (Not 30, because then the container might get forcefully killed, and we wouldn't observe the `ApplicationStopped` event). (Alternatively, we could add a `/bin/sleep 25` preStop hook, but the problem with that is that it doesn't show up in the container logs, so it's harder to follow what's happening.)\\n\\nThis was the result.\\n\\n```bash\\n01/06/2019 15:36:26: Incoming request at /, State: Running\\n01/06/2019 15:36:26: Incoming request at /, State: Running\\n01/06/2019 15:36:27: Incoming request at /, State: Running\\nApplication is shutting down...\\n01/06/2019 15:36:27: ApplicationStopping called, sleeping for 25s\\n01/06/2019 15:36:27: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:27: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:28: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:28: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:28: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:29: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:29: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:30: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:31: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:31: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:31: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:32: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:33: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:33: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:33: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:34: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:34: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:35: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:35: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:36: Incoming request at /, State: AfterSigterm\\n01/06/2019 15:36:52: ApplicationStopped called\\n```\\n\\nBased on this we can see that it takes approximately 10 seconds until the pod is removed from the endpoint pool, until then it keeps receiving requests.  \\nI repeated the test a couple of times, and the time needed until the requests stopped coming in was always around 10 seconds.\\n\\n# Conclusion and future work\\n\\nAfter learning all these details, the question is simple: what should we do as a maintainer of an ASP.NET Core application to avoid losing some requests on every pod termination?\\n\\nI haven't seen any official recommendation on this topic, so this is only my conclusion. Based on all the things I've described in this post, I don't think there is any \\\"sophisticated\\\" solution to this. We don't have to implement any smart logic which would wait until the pending requests are drained, that's handled by the framework out of the box.\\n\\nThe only thing we have to take care of is to make sure the container doesn't terminate until the Endpoints of our services are updated. And the only way I could find to do this was to sleep for an arbitrary amount of time.  \\nSince it seems that it takes around 10 seconds for the service Endpoints to get updated, we can sleep forlet's say20 seconds, and then with the 30 seconds default grace period that still leaves 10 seconds for the termination of the application process.\\n\\nWe can implement this sleeping either with a `preStop` hook:\\n\\n```yaml\\n  containers:\\n  - name: containername\\n    lifecycle:\\n      preStop:\\n        exec:\\n          command: [ \\\"/bin/sleep\\\", \\\"20\\\" ]\\n```\\n\\nOrin case of ASP.NET Corewe can do it in the `ApplicationStopping` event.\\n\\n```csharp\\n    appLifetime.ApplicationStopping.Register(() =>\\n    {\\n        Log(\\\"ApplicationStopping called, sleeping for 20s\\\");\\n        Thread.Sleep(20000);\\n    });\\n```\\n\\nDoing it in the `preStop` hook probably makes it easier to automate for every service we host.  \\nOn the other hand, doing it in our C# code gives us more options to do some custom logging or metric publishing to be able to collect some information about how the termination behaves in production. (For example it would be interesting to record the time of termination, and the time of the last request we receive, to have an estimate of how long it can take until production pods stop receiving requests.)\\n\\nI have uploaded the code of both the test service, and the console test tool I used for this experiment to [this repository](https://github.com/markvincze/K8sGracefulShutdownTester).\\n\\nWhat would still be interesting to test is how this behaves with other networking solutions. I've only did the tests with a `service` of type `LoadBalancer`, but I'd be interested to see how a `ClusterIP` service, or an `ingress` behaves. And doing the same experiment on different clouds, such as Amazon or Azure, or on a self-hosted Kubernetes cluster could also give valuable insights.\\n\\nIf you have additional information on this topic, or just any feedback, I'd love to hear about it in the comments.\"}]],\"markups\":[],\"sections\":[[10,0],[1,\"p\",[]]]}",
        "html": "<p>Using a container-orchestration technology like Kubernetes, running applications in small containers, and scaling out horizontally rather than scaling a single machine up has numerous benefits, such as flexible allocation of the raw resources among different services, being able to precisely adjust the number of instances we're running according to the volume of traffic we're receiving, and forcing us to run our applications in immutable containers, thereby making our releases repeatable, thus easier to reason about.</p>\n<p>On the other hand, we also face several challenges inherent to this architectural style, due to our components being inevitably distributed, and our containers constantly being shuffled around on the &quot;physical&quot; (technically, most probably still virtual) nodes.</p>\n<p>One of these challenges is that due to Kubernetes constantly scaling our services up and down, and possible evicting our pods from certain nodes and moving them to another, instances of our services will constantly be stopped and then started up.<br>\nThis is something we have to worry about much less if we have a fixed number of server machines to which we're directly deploying our application. In that case, our services will only be stopped when we're deploying, or in some exceptional cases like if they crash, or the server machines have to be restarted for some reason.</p>\n<p>This requires a mental shift. When implementing a service, we always have to think it through how it will behave if it's suddenly stopped. Will it leave something in an inconsistent state that we have to clean up? Can it be doing a long-running operation that we need to cancel? Do we have to send a signal to notify some other component that this instance is stopping?</p>\n<p>In this post I'd like to focus only on the simplest scenario: we have a REST API implemented in ASP.NET Core, which doesn't have any of the above issues. It's stateless, it doesn't run any background processes, it only works in the request-response model, through some HTTP endpoints.</p>\n<p>Even in this simplest scenario there is an important issue: at the point in time when our application is stopped, there might be some requests being processed. We have to ensure that ASP.NET Core (or Kubernetes) waits with killing our instance until they are completed.</p>\n<p>In the rest of the post I'd like to describe the facilities Kubernetes provides to handle this scenario, and the way we can utilize them in ASP.NET Core.</p>\n<p><em>I did all the testing on the Google Kubernetes Engine. Since this is related to networking, which to some extent depends on where and how we're hosting Kubernetes, I can imagine that this might work differently depending on where we run, let's say Amazon, Azure, etc.</em></p>\n<h1 id=\"gracefulterminationinkubernetes\">Graceful termination in Kubernetes</h1>\n<p>Kubernetes implements a specific order of events when it terminates any pod, so that we can set up our containers in a way that they have a chance to gracefully terminate.</p>\n<p>The process is documented in <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods\">this section</a>. I have to say that this part of the documentation has always been a bit confusing to me, and some parts I could figure out only through experimentation.</p>\n<p>The basic idea is that when the pod is stopped, a grace period starts (which has the default length 30 seconds), during which the pod has a chance to do any necessary cleanup. If the containers terminate faster then that, then the pod is deleted as soon as all the containers exited, so Kubernetes doesn't necessarily waits for the full grace period. And if the containers don't terminate until the grace period passes, then Kubernetes forcefully stops all the containers, and deletes the pod.</p>\n<p>We have two ways to run custom operations on shutdown, and prolong the termination of the container (but no longer than the full grace period):</p>\n<ul>\n<li>We can specify a <code>preStop</code> hook for our container, in which we can run an arbitrary shell command. (Or execute a script file we included in our Docker image.)</li>\n<li>The container receives a <code>TERM</code> signal, which we can handle in our application code. This is something we'll have to figure out how to properly do in the technology of our choice, it'll be different in .NET, NodeJS, Go, etc. In this post we'll see the specificities of ASP.NET Core.</li>\n</ul>\n<p>We can use either of these approaches, or even both of them. It's important to remember that they <em>don't</em> happen in parallel, but rather sequentially (starting with the <code>preStop</code> hook), and they are confined by the same grace period together, not individually. So if the <code>preStop</code> hook uses up all the time in the grace period, we might not have enough time left to properly handle the <code>TERM</code> signal. <em>(If the <code>preStop</code> hook uses up all the time, then the <code>TERM</code> handler gets an extra 2 seconds to run.)</em></p>\n<p>And on top of all this, at some point the pod is removed from the list of Endpoints of the service objects.</p>\n<p>An important detail is that the removal of the Endpoint is happening in parallel with <code>preStop</code> hook and the <code>TERM</code> signal, so there is no guarantee the load balancer won't send new requests to the pod being stopped, even after the <code>preStop</code> hook and the <code>TERM</code> signal was handled. (I couldn't find much official information on when this happens exactly, for example in <a href=\"https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-terminating-with-grace\">this tutorial video</a> it's not mentioned at all.)</p>\n<p>All this being said, I think this description leaves a lot up to interpretation regarding how this works in practice, so I decided to try to test what actually happens in an ASP.NET Core application.</p>\n<h1 id=\"aspnetcore\">ASP.NET Core</h1>\n<p>What I set out to do was to test what the proper way is to handle pod termination if our container is running an ASP.NET Core application.</p>\n<p>I particularly wanted to find an answer to these questions:</p>\n<ul>\n<li>How can we handle the <code>TERM</code> signal in our C# code?</li>\n<li>If we don't implement a custom <code>TERM</code> handler, <em>and</em> there are requests in flight when Kubernetes sends the <code>TERM</code>, does our application stop immediately, or does ASP.NET Core do the &quot;right&quot; thing, and wait for all current requests to finish (draining)?</li>\n<li>In Kubernetes, can it happen that the load balancer still sends requests to our pod after we received <code>TERM</code>?</li>\n</ul>\n<p>To be able to test these questions, I created a simple ASP.NET Core application, which simply logs a message to the console on every request, and has two endpoints.</p>\n<pre><code class=\"language-csharp\">app.Run(async (context) =&gt;\n{\n    PrintRequestLog();\n\n    if (context.Request.Path.Value.Contains(&quot;slow&quot;))\n    {\n        await SleepAndPrintForSeconds(10);\n    }\n    else\n    {\n        await Task.Delay(100);\n    }\n\n    await context.Response.WriteAsync(message);\n});\n</code></pre>\n<ul>\n<li>The endpoint <code>/slow</code> responds after 10 seconds, and prints a log message every seconds while it's waiting. We can use this to test what happens to requests in flight when we receive the <code>TERM</code> signal.</li>\n<li>Any other route responds much quicker (after 100ms), and prints one message. By constantly polling this endpoint we can check if our pod still receives request after the <code>TERM</code> signal.</li>\n</ul>\n<p>If we keep polling the &quot;slow&quot; endpoint, the output looks like this:</p>\n<pre><code class=\"language-bash\">1/6/19 1:50:22 PM: Incoming request at /slow, State: Running\n1/6/19 1:50:22 PM: Sleeping (10 seconds left)\n1/6/19 1:50:23 PM: Sleeping (9 seconds left)\n1/6/19 1:50:24 PM: Sleeping (8 seconds left)\n...\n1/6/19 1:50:30 PM: Sleeping (2 seconds left)\n1/6/19 1:50:31 PM: Sleeping (1 seconds left)\n1/6/19 1:50:32 PM: Incoming request at /slow, State: Running\n1/6/19 1:50:32 PM: Sleeping (10 seconds left)\n1/6/19 1:50:33 PM: Sleeping (9 seconds left)\n1/6/19 1:50:34 PM: Sleeping (8 seconds left)\n...\n1/6/19 1:50:40 PM: Sleeping (2 seconds left)\n1/6/19 1:50:41 PM: Sleeping (1 seconds left)\n1/6/19 1:50:42 PM: Incoming request at /slow, State: Running\n1/6/19 1:50:42 PM: Sleeping (10 seconds left)\n1/6/19 1:50:43 PM: Sleeping (9 seconds left)\n...\n</code></pre>\n<p>And if we poll the quick endpoint, we only get one log message per request.</p>\n<pre><code class=\"language-bash\">1/6/19 1:52:49 PM: Incoming request at /, State: Running\n1/6/19 1:52:50 PM: Incoming request at /, State: Running\n1/6/19 1:52:50 PM: Incoming request at /, State: Running\n1/6/19 1:52:50 PM: Incoming request at /, State: Running\n1/6/19 1:52:50 PM: Incoming request at /, State: Running\n</code></pre>\n<h2 id=\"handleterminaspnetcore\">Handle <code>TERM</code> in ASP.NET Core</h2>\n<p>In order to handle the <code>TERM</code> signal with custom C# code, we have to use the <a href=\"https://github.com/aspnet/AspNetCore/blob/master/src/Hosting/Abstractions/src/IApplicationLifetime.cs\"><code>IApplicationLifetime</code></a> interface, which we can inject in our <code>Startup.Configure()</code> method. It provides 2 events related to termination, that we can register to. The documentation specifies these as the following.</p>\n<ul>\n<li>ApplicationStopping: &quot;Triggered when the application host is performing a graceful shutdown. Requests may still be in flight. Shutdown will block until this event completes.&quot;</li>\n<li>ApplicationStopped: &quot;Triggered when the application host is performing a graceful shutdown. All requests should be complete at this point. Shutdown will block until this event completes.&quot;</li>\n</ul>\n<p>To test how these events work, I added a console print to both of them.</p>\n<pre><code class=\"language-csharp\">appLifetime.ApplicationStopping.Register(() =&gt; Console.WriteLine(&quot;ApplicationStopping called&quot;));\nappLifetime.ApplicationStopped.Register(() =&gt; Console.WriteLine(&quot;ApplicationStopped called&quot;));\n</code></pre>\n<p>These are the events we can use to sort out any custom cleanup before the application terminates. The way they work is that the ASP.NET Core process does not terminate until these event handlers return. But they have a timeout, which we'll see in a second.</p>\n<p>If we send a <code>TERM</code> signal by executing <code>kill PID</code> to a running ASP.NET Core applicationwhich is not executing any requests at the momentthis is the output we'll see.</p>\n<pre><code class=\"language-bash\">Hosting environment: Production\nContent root path: /home/mvincze/K8sGracefulShutdownTester/src/K8sGracefulShutdownTester\nNow listening on: http://[::]:5000\nApplication started. Press Ctrl+C to shut down.\n...\nApplication is shutting down...\n1/6/19 1:58:51 PM: ApplicationStopping called\n1/6/19 1:58:51 PM: ApplicationStopped called\n</code></pre>\n<p>The line <code>Application is shutting down...</code> is printed by the framework when it receives <code>TERM</code>, and we get the next two lines in short succession, which are coming from our custom handlers, and then the process exits immediately.</p>\n<p>The next thing to see is what happens if we try the same when there is a request being processed.</p>\n<h2 id=\"waitforrequestsinflighttofinish\">Wait for requests in flight to finish</h2>\n<p>The first thing I tested was what happens if our application receives the <code>TERM</code> signal when a request is being processed. I kept polling the <code>/slow</code> endpoint, and sent the <code>TERM</code> signal by executing <code>kill PID</code> when the app was in the middle of processing on of the slow requests. This was the output.</p>\n<pre><code class=\"language-bash\">1/6/19 1:43:39 PM: Incoming request at /slow, State: Running\n1/6/19 1:43:39 PM: Sleeping (10 seconds left)\n1/6/19 1:43:40 PM: Sleeping (9 seconds left)\n1/6/19 1:43:41 PM: Sleeping (8 seconds left)\n1/6/19 1:43:42 PM: Sleeping (7 seconds left)\n1/6/19 1:43:43 PM: Sleeping (6 seconds left)\n1/6/19 1:43:44 PM: Sleeping (5 seconds left)\n1/6/19 1:43:45 PM: Sleeping (4 seconds left)\nApplication is shutting down...\n1/6/19 1:43:45 PM: ApplicationStopping called\n1/6/19 1:43:46 PM: Sleeping (3 seconds left)\n1/6/19 1:43:47 PM: Sleeping (2 seconds left)\n1/6/19 1:43:48 PM: Sleeping (1 seconds left)\n1/6/19 1:43:49 PM: ApplicationStopped called\n</code></pre>\n<p>This illustrates that if the <code>TERM</code> comes in when there is a request being processed, the framework waits and doesn't let the process terminate until the requests in flight finish. So this is taken care of by ASP.NET Core (I believe it's implemented in <a href=\"https://github.com/aspnet/AspNetCore/blob/master/src/Servers/Kestrel/Core/src/KestrelServer.cs#L172\"><code>KestrelServer</code></a>), we don't have to implement custom code to achieve this.</p>\n<p>One important thing to consider is that there is a timeout period until the framework is willing to wait for the pending requests. If I send the <code>TERM</code> when there is still more than 5 seconds left from the current request, this is what happens.</p>\n<pre><code class=\"language-bash\">1/6/19 2:14:45 PM: Incoming request at /slow, State: Running\n1/6/19 2:14:45 PM: Sleeping (10 seconds left)\n1/6/19 2:14:46 PM: Sleeping (9 seconds left)\nApplication is shutting down...\n1/6/19 2:14:46 PM: ApplicationStopping called\n1/6/19 2:14:47 PM: Sleeping (8 seconds left)\n1/6/19 2:14:48 PM: Sleeping (7 seconds left)\n1/6/19 2:14:49 PM: Sleeping (6 seconds left)\n1/6/19 2:14:50 PM: Sleeping (5 seconds left)\n1/6/19 2:14:51 PM: Sleeping (4 seconds left)\n1/6/19 2:14:52 PM: Sleeping (3 seconds left)\n1/6/19 2:14:52 PM: ApplicationStopped called\n</code></pre>\n<p>So ASP.NET Core is not willing to wait infinitely for the pending requests to finish, after some timeout period it forcefully terminates the application, regardless of the requests in flight. (And this causes a failed request, the <code>HttpClient</code> I was using to poll threw an exception.)</p>\n<p>The default timeout is <a href=\"https://github.com/aspnet/AspNetCore/blob/master/src/Hosting/Hosting/src/Internal/WebHostOptions.cs#L70\">5 seconds</a>, but we can increase it by calling the <a href=\"https://github.com/aspnet/AspNetCore/blob/d852e10293c0fbd3dfbf82be455c2cde683ec0de/src/Hosting/Abstractions/src/HostingAbstractionsWebHostBuilderExtensions.cs#L179\"><code>UseShutdownTimeout()</code></a> extension method on the <code>WebHostBuilder</code> in our <code>Program.Main()</code> method.</p>\n<h2 id=\"usethisinkubernetes\">Use this in Kubernetes</h2>\n<p>Based on the above, it seems that we're good to go without implementing anything custom. Kubernetes sends a <code>TERM</code> signal before it kills our pods, and is willing to wait 30 seconds. And ASP.NET Core doesn't exit until the pending requests are finished. And if the 5 second default timeout is not enough, because we anticipate having slower requests (which is not commonplace in REST APIs anyway), then we can increase it in <code>Program.Main</code>.</p>\n<p>There is an issue though. Kubernetes sending <code>TERM</code>, and removing the pod from the service Endpoint pool happens in parallel. So the following order of events can happen.</p>\n<ol>\n<li>Kubernetes sends the <code>TERM</code> signal.</li>\n<li>Let's say there are no pending requests, so our container terminates immediately.</li>\n<li>The pod has not been removed from the service endpoint pool yet, so it tries to send a request to the terminated pod, which will fail.</li>\n</ol>\n<p>First let's verify if we indeed receive more incoming requests after <code>TERM</code>. To test this, I added a 10 second sleep to our <code>ApplicationStopping</code> handler.</p>\n<pre><code class=\"language-csharp\">appLifetime.ApplicationStopping.Register(() =&gt;\n{\n    Log(&quot;ApplicationStopping called, sleeping for 10s&quot;);\n    Thread.Sleep(10000);\n});\n</code></pre>\n<p>Then I deployed two pods of the application, and kept polling the quick endpoint through a LoadBalancer service, then at some point stopped one of the pods by executing <code>kubectl delete pod</code>.</p>\n<p>This was the output of the pod:</p>\n<pre><code class=\"language-bash\">01/06/2019 14:40:44: Incoming request at /, State: Running\n01/06/2019 14:40:45: Incoming request at /, State: Running\nApplication is shutting down...\n01/06/2019 14:40:45: ApplicationStopping called, sleeping for 10s\n01/06/2019 14:40:46: Incoming request at /, State: AfterSigterm\n01/06/2019 14:40:47: Incoming request at /, State: AfterSigterm\n01/06/2019 14:40:48: Incoming request at /, State: AfterSigterm\n01/06/2019 14:40:49: Incoming request at /, State: AfterSigterm\n01/06/2019 14:40:51: Incoming request at /, State: AfterSigterm\n01/06/2019 14:40:53: Incoming request at /, State: AfterSigterm\n01/06/2019 14:40:55: ApplicationStopped called\n</code></pre>\n<p>This clearly shows that Kubernetes doesn't wait for the <code>LoadBalancer</code> endpoints to be updated before it sends the <code>TERM</code> signal (possibly causing the application to terminate), so it is very much possible that we will keep receiving some requests after <code>TERM</code>.</p>\n<p>If we remove the 10 second sleep from the <code>TERM</code> handler, and execute the same test, we'll see the following output.</p>\n<pre><code class=\"language-bash\">01/06/2019 14:36:07: Incoming request at /, State: Running\n01/06/2019 14:36:07: Incoming request at /, State: Running\n01/06/2019 14:36:07: Incoming request at /, State: Running\nApplication is shutting down...\n01/06/2019 14:36:08: ApplicationStopped called\n01/06/2019 14:36:08: ApplicationStopping called\n</code></pre>\n<p>At the time of receiving the <code>TERM</code>, there happened to be no pending requests, so the process exited immediately.<br>\nAnd this is what I've seen in the little console tool doing the polling:</p>\n<pre><code class=\"language-bash\">2019-01-06 2:36:08 PM: Successful request\n2019-01-06 2:36:09 PM: Successful request\n2019-01-06 2:36:11 PM: Error! Exception: The operation was canceled.\n2019-01-06 2:36:11 PM: Successful request\n2019-01-06 2:36:13 PM: Error! Exception: The operation was canceled.\n2019-01-06 2:36:13 PM: Successful request\n</code></pre>\n<p>By chance, there were 2 request which were still sent to the old pod, in which the container has already stopped, thus the requests failed.<br>\nI repeated this test a couple of times, and this is happening quite randomly. Sometime there were no failed requests, other times there were a couple of them. I guess it depends on how quickly the Endpoints of the LoadBalancer gets updated. Sometimes that happens early enough to prevent failed requests, sometimes it doesn't.</p>\n<p>This means that the default behavior we get when we run an ASP.NET Core container in Kubernetes is not the best, it can result in a couple of failed requests, not just when we deploy a new service version, but also every time a pod is stopped, for any reason.</p>\n<h2 id=\"howquicklydoesthelbgetupdated\">How quickly does the LB get updated?</h2>\n<p>To see how quickly the endpoints of the LB get updated, let's prolong the grace period close to the maximum 30 seconds, and test how long the pod is still receiving requests.</p>\n<p>We can keep the pod living for almost the default 30 second grace period by increasing the sleep duration in <code>ApplicationStopping</code> to 25 seconds (Not 30, because then the container might get forcefully killed, and we wouldn't observe the <code>ApplicationStopped</code> event). (Alternatively, we could add a <code>/bin/sleep 25</code> preStop hook, but the problem with that is that it doesn't show up in the container logs, so it's harder to follow what's happening.)</p>\n<p>This was the result.</p>\n<pre><code class=\"language-bash\">01/06/2019 15:36:26: Incoming request at /, State: Running\n01/06/2019 15:36:26: Incoming request at /, State: Running\n01/06/2019 15:36:27: Incoming request at /, State: Running\nApplication is shutting down...\n01/06/2019 15:36:27: ApplicationStopping called, sleeping for 25s\n01/06/2019 15:36:27: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:27: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:28: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:28: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:28: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:29: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:29: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:30: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:31: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:31: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:31: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:32: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:33: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:33: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:33: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:34: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:34: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:35: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:35: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:36: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:52: ApplicationStopped called\n</code></pre>\n<p>Based on this we can see that it takes approximately 10 seconds until the pod is removed from the endpoint pool, until then it keeps receiving requests.<br>\nI repeated the test a couple of times, and the time needed until the requests stopped coming in was always around 10 seconds.</p>\n<h1 id=\"conclusionandfuturework\">Conclusion and future work</h1>\n<p>After learning all these details, the question is simple: what should we do as a maintainer of an ASP.NET Core application to avoid losing some requests on every pod termination?</p>\n<p>I haven't seen any official recommendation on this topic, so this is only my conclusion. Based on all the things I've described in this post, I don't think there is any &quot;sophisticated&quot; solution to this. We don't have to implement any smart logic which would wait until the pending requests are drained, that's handled by the framework out of the box.</p>\n<p>The only thing we have to take care of is to make sure the container doesn't terminate until the Endpoints of our services are updated. And the only way I could find to do this was to sleep for an arbitrary amount of time.<br>\nSince it seems that it takes around 10 seconds for the service Endpoints to get updated, we can sleep forlet's say20 seconds, and then with the 30 seconds default grace period that still leaves 10 seconds for the termination of the application process.</p>\n<p>We can implement this sleeping either with a <code>preStop</code> hook:</p>\n<pre><code class=\"language-yaml\">  containers:\n  - name: containername\n    lifecycle:\n      preStop:\n        exec:\n          command: [ &quot;/bin/sleep&quot;, &quot;20&quot; ]\n</code></pre>\n<p>Orin case of ASP.NET Corewe can do it in the <code>ApplicationStopping</code> event.</p>\n<pre><code class=\"language-csharp\">    appLifetime.ApplicationStopping.Register(() =&gt;\n    {\n        Log(&quot;ApplicationStopping called, sleeping for 20s&quot;);\n        Thread.Sleep(20000);\n    });\n</code></pre>\n<p>Doing it in the <code>preStop</code> hook probably makes it easier to automate for every service we host.<br>\nOn the other hand, doing it in our C# code gives us more options to do some custom logging or metric publishing to be able to collect some information about how the termination behaves in production. (For example it would be interesting to record the time of termination, and the time of the last request we receive, to have an estimate of how long it can take until production pods stop receiving requests.)</p>\n<p>I have uploaded the code of both the test service, and the console test tool I used for this experiment to <a href=\"https://github.com/markvincze/K8sGracefulShutdownTester\">this repository</a>.</p>\n<p>What would still be interesting to test is how this behaves with other networking solutions. I've only did the tests with a <code>service</code> of type <code>LoadBalancer</code>, but I'd be interested to see how a <code>ClusterIP</code> service, or an <code>ingress</code> behaves. And doing the same experiment on different clouds, such as Amazon or Azure, or on a self-hosted Kubernetes cluster could also give valuable insights.</p>\n<p>If you have additional information on this topic, or just any feedback, I'd love to hear about it in the comments.</p>\n",
        "comment_id": "5c32318aa26b5f5ce4856be0",
        "plaintext": "Using a container-orchestration technology like Kubernetes, running applications\nin small containers, and scaling out horizontally rather than scaling a single\nmachine up has numerous benefits, such as flexible allocation of the raw\nresources among different services, being able to precisely adjust the number of\ninstances we're running according to the volume of traffic we're receiving, and\nforcing us to run our applications in immutable containers, thereby making our\nreleases repeatable, thus easier to reason about.\n\nOn the other hand, we also face several challenges inherent to this\narchitectural style, due to our components being inevitably distributed, and our\ncontainers constantly being shuffled around on the \"physical\" (technically, most\nprobably still virtual) nodes.\n\nOne of these challenges is that due to Kubernetes constantly scaling our\nservices up and down, and possible evicting our pods from certain nodes and\nmoving them to another, instances of our services will constantly be stopped and\nthen started up.\nThis is something we have to worry about much less if we have a fixed number of\nserver machines to which we're directly deploying our application. In that case,\nour services will only be stopped when we're deploying, or in some exceptional\ncases like if they crash, or the server machines have to be restarted for some\nreason.\n\nThis requires a mental shift. When implementing a service, we always have to\nthink it through how it will behave if it's suddenly stopped. Will it leave\nsomething in an inconsistent state that we have to clean up? Can it be doing a\nlong-running operation that we need to cancel? Do we have to send a signal to\nnotify some other component that this instance is stopping?\n\nIn this post I'd like to focus only on the simplest scenario: we have a REST API\nimplemented in ASP.NET Core, which doesn't have any of the above issues. It's\nstateless, it doesn't run any background processes, it only works in the\nrequest-response model, through some HTTP endpoints.\n\nEven in this simplest scenario there is an important issue: at the point in time\nwhen our application is stopped, there might be some requests being processed.\nWe have to ensure that ASP.NET Core (or Kubernetes) waits with killing our\ninstance until they are completed.\n\nIn the rest of the post I'd like to describe the facilities Kubernetes provides\nto handle this scenario, and the way we can utilize them in ASP.NET Core.\n\nI did all the testing on the Google Kubernetes Engine. Since this is related to\nnetworking, which to some extent depends on where and how we're hosting\nKubernetes, I can imagine that this might work differently depending on where we\nrun, let's say Amazon, Azure, etc.\n\nGraceful termination in Kubernetes\nKubernetes implements a specific order of events when it terminates any pod, so\nthat we can set up our containers in a way that they have a chance to gracefully\nterminate.\n\nThe process is documented in this section. I have to say that this part of the\ndocumentation has always been a bit confusing to me, and some parts I could\nfigure out only through experimentation.\n\nThe basic idea is that when the pod is stopped, a grace period starts (which has\nthe default length 30 seconds), during which the pod has a chance to do any\nnecessary cleanup. If the containers terminate faster then that, then the pod is\ndeleted as soon as all the containers exited, so Kubernetes doesn't necessarily\nwaits for the full grace period. And if the containers don't terminate until the\ngrace period passes, then Kubernetes forcefully stops all the containers, and\ndeletes the pod.\n\nWe have two ways to run custom operations on shutdown, and prolong the\ntermination of the container (but no longer than the full grace period):\n\n * We can specify a preStop  hook for our container, in which we can run an\n   arbitrary shell command. (Or execute a script file we included in our Docker\n   image.)\n * The container receives a TERM  signal, which we can handle in our application\n   code. This is something we'll have to figure out how to properly do in the\n   technology of our choice, it'll be different in .NET, NodeJS, Go, etc. In\n   this post we'll see the specificities of ASP.NET Core.\n\nWe can use either of these approaches, or even both of them. It's important to\nremember that they don't  happen in parallel, but rather sequentially (starting\nwith the preStop  hook), and they are confined by the same grace period\ntogether, not individually. So if the preStop  hook uses up all the time in the\ngrace period, we might not have enough time left to properly handle the TERM \nsignal. (If the preStop  hook uses up all the time, then the TERM  handler gets\nan extra 2 seconds to run.)\n\nAnd on top of all this, at some point the pod is removed from the list of\nEndpoints of the service objects.\n\nAn important detail is that the removal of the Endpoint is happening in parallel\nwith preStop  hook and the TERM  signal, so there is no guarantee the load\nbalancer won't send new requests to the pod being stopped, even after the \npreStop  hook and the TERM  signal was handled. (I couldn't find much official\ninformation on when this happens exactly, for example in this tutorial video\n[https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-terminating-with-grace] \n it's not mentioned at all.)\n\nAll this being said, I think this description leaves a lot up to interpretation\nregarding how this works in practice, so I decided to try to test what actually\nhappens in an ASP.NET Core application.\n\nASP.NET Core\nWhat I set out to do was to test what the proper way is to handle pod\ntermination if our container is running an ASP.NET Core application.\n\nI particularly wanted to find an answer to these questions:\n\n * How can we handle the TERM  signal in our C# code?\n * If we don't implement a custom TERM  handler, and  there are requests in\n   flight when Kubernetes sends the TERM, does our application stop immediately,\n   or does ASP.NET Core do the \"right\" thing, and wait for all current requests\n   to finish (draining)?\n * In Kubernetes, can it happen that the load balancer still sends requests to\n   our pod after we received TERM?\n\nTo be able to test these questions, I created a simple ASP.NET Core application,\nwhich simply logs a message to the console on every request, and has two\nendpoints.\n\napp.Run(async (context) =>\n{\n    PrintRequestLog();\n\n    if (context.Request.Path.Value.Contains(\"slow\"))\n    {\n        await SleepAndPrintForSeconds(10);\n    }\n    else\n    {\n        await Task.Delay(100);\n    }\n\n    await context.Response.WriteAsync(message);\n});\n\n\n * The endpoint /slow  responds after 10 seconds, and prints a log message every\n   seconds while it's waiting. We can use this to test what happens to requests\n   in flight when we receive the TERM  signal.\n * Any other route responds much quicker (after 100ms), and prints one message.\n   By constantly polling this endpoint we can check if our pod still receives\n   request after the TERM  signal.\n\nIf we keep polling the \"slow\" endpoint, the output looks like this:\n\n1/6/19 1:50:22 PM: Incoming request at /slow, State: Running\n1/6/19 1:50:22 PM: Sleeping (10 seconds left)\n1/6/19 1:50:23 PM: Sleeping (9 seconds left)\n1/6/19 1:50:24 PM: Sleeping (8 seconds left)\n...\n1/6/19 1:50:30 PM: Sleeping (2 seconds left)\n1/6/19 1:50:31 PM: Sleeping (1 seconds left)\n1/6/19 1:50:32 PM: Incoming request at /slow, State: Running\n1/6/19 1:50:32 PM: Sleeping (10 seconds left)\n1/6/19 1:50:33 PM: Sleeping (9 seconds left)\n1/6/19 1:50:34 PM: Sleeping (8 seconds left)\n...\n1/6/19 1:50:40 PM: Sleeping (2 seconds left)\n1/6/19 1:50:41 PM: Sleeping (1 seconds left)\n1/6/19 1:50:42 PM: Incoming request at /slow, State: Running\n1/6/19 1:50:42 PM: Sleeping (10 seconds left)\n1/6/19 1:50:43 PM: Sleeping (9 seconds left)\n...\n\n\nAnd if we poll the quick endpoint, we only get one log message per request.\n\n1/6/19 1:52:49 PM: Incoming request at /, State: Running\n1/6/19 1:52:50 PM: Incoming request at /, State: Running\n1/6/19 1:52:50 PM: Incoming request at /, State: Running\n1/6/19 1:52:50 PM: Incoming request at /, State: Running\n1/6/19 1:52:50 PM: Incoming request at /, State: Running\n\n\nHandle TERM  in ASP.NET Core\nIn order to handle the TERM  signal with custom C# code, we have to use the \nIApplicationLifetime\n[https://github.com/aspnet/AspNetCore/blob/master/src/Hosting/Abstractions/src/IApplicationLifetime.cs] \n interface, which we can inject in our Startup.Configure()  method. It provides\n2 events related to termination, that we can register to. The documentation\nspecifies these as the following.\n\n * ApplicationStopping: \"Triggered when the application host is performing a\n   graceful shutdown. Requests may still be in flight. Shutdown will block until\n   this event completes.\"\n * ApplicationStopped: \"Triggered when the application host is performing a\n   graceful shutdown. All requests should be complete at this point. Shutdown\n   will block until this event completes.\"\n\nTo test how these events work, I added a console print to both of them.\n\nappLifetime.ApplicationStopping.Register(() => Console.WriteLine(\"ApplicationStopping called\"));\nappLifetime.ApplicationStopped.Register(() => Console.WriteLine(\"ApplicationStopped called\"));\n\n\nThese are the events we can use to sort out any custom cleanup before the\napplication terminates. The way they work is that the ASP.NET Core process does\nnot terminate until these event handlers return. But they have a timeout, which\nwe'll see in a second.\n\nIf we send a TERM  signal by executing kill PID  to a running ASP.NET Core\napplicationwhich is not executing any requests at the momentthis is the output\nwe'll see.\n\nHosting environment: Production\nContent root path: /home/mvincze/K8sGracefulShutdownTester/src/K8sGracefulShutdownTester\nNow listening on: http://[::]:5000\nApplication started. Press Ctrl+C to shut down.\n...\nApplication is shutting down...\n1/6/19 1:58:51 PM: ApplicationStopping called\n1/6/19 1:58:51 PM: ApplicationStopped called\n\n\nThe line Application is shutting down...  is printed by the framework when it\nreceives TERM, and we get the next two lines in short succession, which are\ncoming from our custom handlers, and then the process exits immediately.\n\nThe next thing to see is what happens if we try the same when there is a request\nbeing processed.\n\nWait for requests in flight to finish\nThe first thing I tested was what happens if our application receives the TERM \nsignal when a request is being processed. I kept polling the /slow  endpoint,\nand sent the TERM  signal by executing kill PID  when the app was in the middle\nof processing on of the slow requests. This was the output.\n\n1/6/19 1:43:39 PM: Incoming request at /slow, State: Running\n1/6/19 1:43:39 PM: Sleeping (10 seconds left)\n1/6/19 1:43:40 PM: Sleeping (9 seconds left)\n1/6/19 1:43:41 PM: Sleeping (8 seconds left)\n1/6/19 1:43:42 PM: Sleeping (7 seconds left)\n1/6/19 1:43:43 PM: Sleeping (6 seconds left)\n1/6/19 1:43:44 PM: Sleeping (5 seconds left)\n1/6/19 1:43:45 PM: Sleeping (4 seconds left)\nApplication is shutting down...\n1/6/19 1:43:45 PM: ApplicationStopping called\n1/6/19 1:43:46 PM: Sleeping (3 seconds left)\n1/6/19 1:43:47 PM: Sleeping (2 seconds left)\n1/6/19 1:43:48 PM: Sleeping (1 seconds left)\n1/6/19 1:43:49 PM: ApplicationStopped called\n\n\nThis illustrates that if the TERM  comes in when there is a request being\nprocessed, the framework waits and doesn't let the process terminate until the\nrequests in flight finish. So this is taken care of by ASP.NET Core (I believe\nit's implemented in KestrelServer), we don't have to implement custom code to\nachieve this.\n\nOne important thing to consider is that there is a timeout period until the\nframework is willing to wait for the pending requests. If I send the TERM  when\nthere is still more than 5 seconds left from the current request, this is what\nhappens.\n\n1/6/19 2:14:45 PM: Incoming request at /slow, State: Running\n1/6/19 2:14:45 PM: Sleeping (10 seconds left)\n1/6/19 2:14:46 PM: Sleeping (9 seconds left)\nApplication is shutting down...\n1/6/19 2:14:46 PM: ApplicationStopping called\n1/6/19 2:14:47 PM: Sleeping (8 seconds left)\n1/6/19 2:14:48 PM: Sleeping (7 seconds left)\n1/6/19 2:14:49 PM: Sleeping (6 seconds left)\n1/6/19 2:14:50 PM: Sleeping (5 seconds left)\n1/6/19 2:14:51 PM: Sleeping (4 seconds left)\n1/6/19 2:14:52 PM: Sleeping (3 seconds left)\n1/6/19 2:14:52 PM: ApplicationStopped called\n\n\nSo ASP.NET Core is not willing to wait infinitely for the pending requests to\nfinish, after some timeout period it forcefully terminates the application,\nregardless of the requests in flight. (And this causes a failed request, the \nHttpClient  I was using to poll threw an exception.)\n\nThe default timeout is 5 seconds, but we can increase it by calling the \nUseShutdownTimeout()  extension method on the WebHostBuilder  in our \nProgram.Main()  method.\n\nUse this in Kubernetes\nBased on the above, it seems that we're good to go without implementing anything\ncustom. Kubernetes sends a TERM  signal before it kills our pods, and is willing\nto wait 30 seconds. And ASP.NET Core doesn't exit until the pending requests are\nfinished. And if the 5 second default timeout is not enough, because we\nanticipate having slower requests (which is not commonplace in REST APIs\nanyway), then we can increase it in Program.Main.\n\nThere is an issue though. Kubernetes sending TERM, and removing the pod from the\nservice Endpoint pool happens in parallel. So the following order of events can\nhappen.\n\n 1. Kubernetes sends the TERM  signal.\n 2. Let's say there are no pending requests, so our container terminates\n    immediately.\n 3. The pod has not been removed from the service endpoint pool yet, so it tries\n    to send a request to the terminated pod, which will fail.\n\nFirst let's verify if we indeed receive more incoming requests after TERM. To\ntest this, I added a 10 second sleep to our ApplicationStopping  handler.\n\nappLifetime.ApplicationStopping.Register(() =>\n{\n    Log(\"ApplicationStopping called, sleeping for 10s\");\n    Thread.Sleep(10000);\n});\n\n\nThen I deployed two pods of the application, and kept polling the quick endpoint\nthrough a LoadBalancer service, then at some point stopped one of the pods by\nexecuting kubectl delete pod.\n\nThis was the output of the pod:\n\n01/06/2019 14:40:44: Incoming request at /, State: Running\n01/06/2019 14:40:45: Incoming request at /, State: Running\nApplication is shutting down...\n01/06/2019 14:40:45: ApplicationStopping called, sleeping for 10s\n01/06/2019 14:40:46: Incoming request at /, State: AfterSigterm\n01/06/2019 14:40:47: Incoming request at /, State: AfterSigterm\n01/06/2019 14:40:48: Incoming request at /, State: AfterSigterm\n01/06/2019 14:40:49: Incoming request at /, State: AfterSigterm\n01/06/2019 14:40:51: Incoming request at /, State: AfterSigterm\n01/06/2019 14:40:53: Incoming request at /, State: AfterSigterm\n01/06/2019 14:40:55: ApplicationStopped called\n\n\nThis clearly shows that Kubernetes doesn't wait for the LoadBalancer  endpoints\nto be updated before it sends the TERM  signal (possibly causing the application\nto terminate), so it is very much possible that we will keep receiving some\nrequests after TERM.\n\nIf we remove the 10 second sleep from the TERM  handler, and execute the same\ntest, we'll see the following output.\n\n01/06/2019 14:36:07: Incoming request at /, State: Running\n01/06/2019 14:36:07: Incoming request at /, State: Running\n01/06/2019 14:36:07: Incoming request at /, State: Running\nApplication is shutting down...\n01/06/2019 14:36:08: ApplicationStopped called\n01/06/2019 14:36:08: ApplicationStopping called\n\n\nAt the time of receiving the TERM, there happened to be no pending requests, so\nthe process exited immediately.\nAnd this is what I've seen in the little console tool doing the polling:\n\n2019-01-06 2:36:08 PM: Successful request\n2019-01-06 2:36:09 PM: Successful request\n2019-01-06 2:36:11 PM: Error! Exception: The operation was canceled.\n2019-01-06 2:36:11 PM: Successful request\n2019-01-06 2:36:13 PM: Error! Exception: The operation was canceled.\n2019-01-06 2:36:13 PM: Successful request\n\n\nBy chance, there were 2 request which were still sent to the old pod, in which\nthe container has already stopped, thus the requests failed.\nI repeated this test a couple of times, and this is happening quite randomly.\nSometime there were no failed requests, other times there were a couple of them.\nI guess it depends on how quickly the Endpoints of the LoadBalancer gets\nupdated. Sometimes that happens early enough to prevent failed requests,\nsometimes it doesn't.\n\nThis means that the default behavior we get when we run an ASP.NET Core\ncontainer in Kubernetes is not the best, it can result in a couple of failed\nrequests, not just when we deploy a new service version, but also every time a\npod is stopped, for any reason.\n\nHow quickly does the LB get updated?\nTo see how quickly the endpoints of the LB get updated, let's prolong the grace\nperiod close to the maximum 30 seconds, and test how long the pod is still\nreceiving requests.\n\nWe can keep the pod living for almost the default 30 second grace period by\nincreasing the sleep duration in ApplicationStopping  to 25 seconds (Not 30,\nbecause then the container might get forcefully killed, and we wouldn't observe\nthe ApplicationStopped  event). (Alternatively, we could add a /bin/sleep 25 \npreStop hook, but the problem with that is that it doesn't show up in the\ncontainer logs, so it's harder to follow what's happening.)\n\nThis was the result.\n\n01/06/2019 15:36:26: Incoming request at /, State: Running\n01/06/2019 15:36:26: Incoming request at /, State: Running\n01/06/2019 15:36:27: Incoming request at /, State: Running\nApplication is shutting down...\n01/06/2019 15:36:27: ApplicationStopping called, sleeping for 25s\n01/06/2019 15:36:27: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:27: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:28: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:28: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:28: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:29: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:29: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:30: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:31: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:31: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:31: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:32: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:33: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:33: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:33: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:34: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:34: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:35: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:35: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:36: Incoming request at /, State: AfterSigterm\n01/06/2019 15:36:52: ApplicationStopped called\n\n\nBased on this we can see that it takes approximately 10 seconds until the pod is\nremoved from the endpoint pool, until then it keeps receiving requests.\nI repeated the test a couple of times, and the time needed until the requests\nstopped coming in was always around 10 seconds.\n\nConclusion and future work\nAfter learning all these details, the question is simple: what should we do as a\nmaintainer of an ASP.NET Core application to avoid losing some requests on every\npod termination?\n\nI haven't seen any official recommendation on this topic, so this is only my\nconclusion. Based on all the things I've described in this post, I don't think\nthere is any \"sophisticated\" solution to this. We don't have to implement any\nsmart logic which would wait until the pending requests are drained, that's\nhandled by the framework out of the box.\n\nThe only thing we have to take care of is to make sure the container doesn't\nterminate until the Endpoints of our services are updated. And the only way I\ncould find to do this was to sleep for an arbitrary amount of time.\nSince it seems that it takes around 10 seconds for the service Endpoints to get\nupdated, we can sleep forlet's say20 seconds, and then with the 30 seconds\ndefault grace period that still leaves 10 seconds for the termination of the\napplication process.\n\nWe can implement this sleeping either with a preStop  hook:\n\n  containers:\n  - name: containername\n    lifecycle:\n      preStop:\n        exec:\n          command: [ \"/bin/sleep\", \"20\" ]\n\n\nOrin case of ASP.NET Corewe can do it in the ApplicationStopping  event.\n\n    appLifetime.ApplicationStopping.Register(() =>\n    {\n        Log(\"ApplicationStopping called, sleeping for 20s\");\n        Thread.Sleep(20000);\n    });\n\n\nDoing it in the preStop  hook probably makes it easier to automate for every\nservice we host.\nOn the other hand, doing it in our C# code gives us more options to do some\ncustom logging or metric publishing to be able to collect some information about\nhow the termination behaves in production. (For example it would be interesting\nto record the time of termination, and the time of the last request we receive,\nto have an estimate of how long it can take until production pods stop receiving\nrequests.)\n\nI have uploaded the code of both the test service, and the console test tool I\nused for this experiment to this repository\n[https://github.com/markvincze/K8sGracefulShutdownTester].\n\nWhat would still be interesting to test is how this behaves with other\nnetworking solutions. I've only did the tests with a service  of type \nLoadBalancer, but I'd be interested to see how a ClusterIP  service, or an \ningress  behaves. And doing the same experiment on different clouds, such as\nAmazon or Azure, or on a self-hosted Kubernetes cluster could also give valuable\ninsights.\n\nIf you have additional information on this topic, or just any feedback, I'd love\nto hear about it in the comments.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": null,
        "meta_description": "An overview of the challenges and solutions for implementing graceful termination when using ASP.NET Core in Kubernetes.",
        "author_id": "1",
        "created_at": "2019-01-06 16:49:14",
        "created_by": "1",
        "updated_at": "2019-01-07 09:27:23",
        "updated_by": "1",
        "published_at": "2019-01-06 16:56:05",
        "published_by": "1",
        "custom_excerpt": "",
        "codeinjection_head": "",
        "codeinjection_foot": "",
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }, {
        "id": "5cafa655a26b5f5ce4856bf4",
        "uuid": "b3badabd-7981-41bf-843b-5a0ba7ba8938",
        "title": "Shadow mirroring with Envoy",
        "slug": "shadow-mirroring-with-envoy",
        "mobiledoc": "{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"markdown\":\"# Introduction\\n\\nShadow mirroring (also called shadow feeding, or just shadowing) is a technique when at some point in our infrastructure we duplicate the outgoing traffic to an additional destination, but we send the responses to the actual client coming from the main destination.  \\nThis is mainly used to be able to test a service with real production traffic without affecting the end clients in any way. It's particularly useful when we are rewriting an existing service, and we want to verify if the new version can process a real variety of incoming requests in an identical way, or when we want to do a comparative benchmark between two version of the same service. And it can also be used to do some extra, out of bounds processing of our requests, which can be done asynchronously, for example collect some additional statistics, or do some extended logging.\\n\\nThere are some existing technologies implemented specifically for this purpose. The one I have used before is [GoReplay](https://github.com/buger/goreplay), which is a small network monitoring tool capable ofamong other thingsshadowing the incoming traffic to a second source.\\n\\nAnd some of the general-purpose reverse proxies have this capability as well. In this post we'll go over how this can be implemented with [Envoy](https://www.envoyproxy.io/). First we'll see an introduction to what a basic proxy or load balancing setup with Envoy looks like, and then we'll focus on the request mirroring capabilities which allows us to implement shadowing.\\n\\n# A simple Envoy setup\\n\\nThe following diagram illustrates a very simple setup where we use an Envoy instance as a \\\"frontend\\\" or reverse proxy to route traffic to a single backend service. (Envoy can be used in much more complicated routing setups, but this is enough for illustrating request mirroring.)\\n\\n![A basic Envoy setup.](/content/images/2019/04/envoy-simple-setup.png)\\n\\nAn Envoy instance can be controlled by providing a static YAML configuration on startup. (Which is just one of the ways of configuring Envoy, it also supports having a dynamic configuration provider, but we won't discuss that in this post.)  \\nThe above simple setup can be achieved with the following YAML config.\\n\\n```yaml\\nadmin:\\n  access_log_path: /tmp/admin_access.log\\n  address:\\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\\n\\nstatic_resources:\\n  listeners:\\n  - name: listener_0\\n    address:\\n      socket_address: { address: 0.0.0.0, port_value: 80 }\\n    filter_chains:\\n    - filters:\\n      - name: envoy.http_connection_manager\\n        config:\\n          stat_prefix: ingress_http\\n          route_config:\\n            name: local_route\\n            virtual_hosts:\\n            - name: local_service\\n              domains: [\\\"*\\\"]\\n              routes:\\n              - match: { prefix: \\\"/\\\" }\\n                route:\\n                  host_rewrite: myservice-backend.mycompany.com\\n                  cluster: myservice_cluster\\n          http_filters:\\n          - name: envoy.router\\n  clusters:\\n  - name: myservice_cluster\\n    type: LOGICAL_DNS\\n    hosts: [{ socket_address: { address: myservice-backend.mycompany.com, port_value: 80 }}]\\n```\\n\\nThis specifies one single route, which matches all incoming requests, and proxies them to the `myservice_cluster` cluster.\\n\\n```yaml\\n            - name: local_service\\n              domains: [\\\"*\\\"]\\n              routes:\\n              - match: { prefix: \\\"/\\\" }\\n                route:\\n                  host_rewrite: myservice-backend.mycompany.com\\n                  cluster: myservice_cluster\\n```\\n\\nAnd the single cluster we configured will route all requests to the address `myservice-backend.mycompany.com`.\\n\\n```yaml\\n  - name: myservice_cluster\\n    type: LOGICAL_DNS\\n    hosts: [{ socket_address: { address: myservice-backend.mycompany.com, port_value: 80 }}]\\n```\\n\\n# Mirroring support in Envoy\\n\\nThere are two specific capabilities of Envoy which are particularly interesting to us.\\n\\nThe first is [Traffic Shifting/Splitting](https://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/traffic_splitting). This allows us to route certain percentages of the traffic to different hosts. Although this is useful in many situations, it won't help us with shadowing, since this just routes the traffic in various directions, but doesn't duplicate it.\\n\\nThe feature which will allow us to implement shadowing is the [request mirror policy](https://www.envoyproxy.io/docs/envoy/latest/api-v2/api/v2/route/route.proto#route-routeaction-requestmirrorpolicy). With this we can specify an extra backend for every route, and configure what percentage of the traffic should be mirrored to it.\\n\\nThis can be enabled by adding the `request_mirror_policy` field to our route, and configure the following two keys.\\n\\n - `cluster`: The name of the cluster to which we're mirroring.\\n - `runtime_fraction`: The portion of the traffic which should be mirrored. You can either configure this with a dynamic runtime value, or add a static value to your YAML config with this syntax: `runtime_fraction: { default_value: { numerator: 25 } }`.  \\n The field `default_value` should contain an object of the type [FractionalPercent](https://www.envoyproxy.io/docs/envoy/latest/api-v2/type/percent.proto#envoy-api-msg-type-fractionalpercent), in which you can specify the `numerator` and the `denominator`. The final portion of the mirrored traffic will be calculated as `numerator / denominator`. The default value of `denominator` is `100`, so if you only specify the `numerator`, it'll be interpreted as a percentage. If you want to configure fractional percentages, then you need to configure `denominator` too.  \\n For example this value specifies 0.03% (0.0003): `runtime_fraction: { default_value: { numerator: 3, denominator: 10000 } }`.\\n\\nAnd the way Envoy will behave is that it'll send back the response to the original client which comes from the main cluster, and the mirrored requests happens in a fire & forget fashion, so the response is discarded. This is exactly what we want if we'd like to test a different version of our service without affecting any end users.\\n\\n# The full setup\\n\\nLet's say we want to configure the following setup, in which 25% of the traffic is shadow mirrored to a different upstream, accessible at `myservice-test.mycompany.com`.\\n\\n![An Envoy setup with mirroring 25% of the traffic.](/content/images/2019/04/envoy-mirror-setup.png)\\n\\nWe can set this up with the following YAML configuration.\\n\\n```yaml\\nadmin:\\n  access_log_path: /tmp/admin_access.log\\n  address:\\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\\n\\nstatic_resources:\\n  listeners:\\n  - name: listener_0\\n    address:\\n      socket_address: { address: 0.0.0.0, port_value: 80 }\\n    filter_chains:\\n    - filters:\\n      - name: envoy.http_connection_manager\\n        config:\\n          stat_prefix: ingress_http\\n          route_config:\\n            name: local_route\\n            virtual_hosts:\\n            - name: local_service\\n              domains: [\\\"*\\\"]\\n              routes:\\n              - match: { prefix: \\\"/\\\" }\\n                route:\\n                  host_rewrite: myservice-backend.mycompany.com\\n                  cluster: myservice_cluster\\n                  request_mirror_policy:\\n                    cluster: myservice_test_cluster\\n                    runtime_fraction: { default_value: { numerator: 25 } }\\n          http_filters:\\n          - name: envoy.router\\n  clusters:\\n  - name: myservice_cluster\\n    type: LOGICAL_DNS\\n    hosts: [{ socket_address: { address: myservice-backend.mycompany.com, port_value: 80 }}]\\n  - name: myservice_test_cluster\\n    type: LOGICAL_DNS\\n    hosts: [{ socket_address: { address: myservice-test.mycompany.com, port_value: 80 }}]\\n```\\n\\nOne important thing to note is that the service discovery for the test cluster will happen with the DNS address `myservice-test.mycompany.com`, that's not going to be used as the host name (the value of the `Host` header) in the mirrored requests.  \\nWith a request mirror policy, the host name is always what it would normally be, plus the `-shadow` suffix. Thus in this example it will become `myservice-backend.company.com-shadow`. This is something that we have to prepare for if we're mirroring traffic to a component which takes the value of the `Host` header into account. (At some point [I asked](https://groups.google.com/forum/#!topic/envoy-users/sroT9ecsCDY) if there is any way to customize this, but at the moment it is not possible.)\\n\\nI hope this will be a useful introduction about request mirroring with Envoy, I found that this feature can be extremely valuable for safely testing new implementations, or carrying out comparative benchmarks.\"}]],\"markups\":[],\"sections\":[[10,0],[1,\"p\",[]]]}",
        "html": "<h1 id=\"introduction\">Introduction</h1>\n<p>Shadow mirroring (also called shadow feeding, or just shadowing) is a technique when at some point in our infrastructure we duplicate the outgoing traffic to an additional destination, but we send the responses to the actual client coming from the main destination.<br>\nThis is mainly used to be able to test a service with real production traffic without affecting the end clients in any way. It's particularly useful when we are rewriting an existing service, and we want to verify if the new version can process a real variety of incoming requests in an identical way, or when we want to do a comparative benchmark between two version of the same service. And it can also be used to do some extra, out of bounds processing of our requests, which can be done asynchronously, for example collect some additional statistics, or do some extended logging.</p>\n<p>There are some existing technologies implemented specifically for this purpose. The one I have used before is <a href=\"https://github.com/buger/goreplay\">GoReplay</a>, which is a small network monitoring tool capable ofamong other thingsshadowing the incoming traffic to a second source.</p>\n<p>And some of the general-purpose reverse proxies have this capability as well. In this post we'll go over how this can be implemented with <a href=\"https://www.envoyproxy.io/\">Envoy</a>. First we'll see an introduction to what a basic proxy or load balancing setup with Envoy looks like, and then we'll focus on the request mirroring capabilities which allows us to implement shadowing.</p>\n<h1 id=\"asimpleenvoysetup\">A simple Envoy setup</h1>\n<p>The following diagram illustrates a very simple setup where we use an Envoy instance as a &quot;frontend&quot; or reverse proxy to route traffic to a single backend service. (Envoy can be used in much more complicated routing setups, but this is enough for illustrating request mirroring.)</p>\n<p><img src=\"/content/images/2019/04/envoy-simple-setup.png\" alt=\"A basic Envoy setup.\"></p>\n<p>An Envoy instance can be controlled by providing a static YAML configuration on startup. (Which is just one of the ways of configuring Envoy, it also supports having a dynamic configuration provider, but we won't discuss that in this post.)<br>\nThe above simple setup can be achieved with the following YAML config.</p>\n<pre><code class=\"language-yaml\">admin:\n  access_log_path: /tmp/admin_access.log\n  address:\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\n\nstatic_resources:\n  listeners:\n  - name: listener_0\n    address:\n      socket_address: { address: 0.0.0.0, port_value: 80 }\n    filter_chains:\n    - filters:\n      - name: envoy.http_connection_manager\n        config:\n          stat_prefix: ingress_http\n          route_config:\n            name: local_route\n            virtual_hosts:\n            - name: local_service\n              domains: [&quot;*&quot;]\n              routes:\n              - match: { prefix: &quot;/&quot; }\n                route:\n                  host_rewrite: myservice-backend.mycompany.com\n                  cluster: myservice_cluster\n          http_filters:\n          - name: envoy.router\n  clusters:\n  - name: myservice_cluster\n    type: LOGICAL_DNS\n    hosts: [{ socket_address: { address: myservice-backend.mycompany.com, port_value: 80 }}]\n</code></pre>\n<p>This specifies one single route, which matches all incoming requests, and proxies them to the <code>myservice_cluster</code> cluster.</p>\n<pre><code class=\"language-yaml\">            - name: local_service\n              domains: [&quot;*&quot;]\n              routes:\n              - match: { prefix: &quot;/&quot; }\n                route:\n                  host_rewrite: myservice-backend.mycompany.com\n                  cluster: myservice_cluster\n</code></pre>\n<p>And the single cluster we configured will route all requests to the address <code>myservice-backend.mycompany.com</code>.</p>\n<pre><code class=\"language-yaml\">  - name: myservice_cluster\n    type: LOGICAL_DNS\n    hosts: [{ socket_address: { address: myservice-backend.mycompany.com, port_value: 80 }}]\n</code></pre>\n<h1 id=\"mirroringsupportinenvoy\">Mirroring support in Envoy</h1>\n<p>There are two specific capabilities of Envoy which are particularly interesting to us.</p>\n<p>The first is <a href=\"https://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/traffic_splitting\">Traffic Shifting/Splitting</a>. This allows us to route certain percentages of the traffic to different hosts. Although this is useful in many situations, it won't help us with shadowing, since this just routes the traffic in various directions, but doesn't duplicate it.</p>\n<p>The feature which will allow us to implement shadowing is the <a href=\"https://www.envoyproxy.io/docs/envoy/latest/api-v2/api/v2/route/route.proto#route-routeaction-requestmirrorpolicy\">request mirror policy</a>. With this we can specify an extra backend for every route, and configure what percentage of the traffic should be mirrored to it.</p>\n<p>This can be enabled by adding the <code>request_mirror_policy</code> field to our route, and configure the following two keys.</p>\n<ul>\n<li><code>cluster</code>: The name of the cluster to which we're mirroring.</li>\n<li><code>runtime_fraction</code>: The portion of the traffic which should be mirrored. You can either configure this with a dynamic runtime value, or add a static value to your YAML config with this syntax: <code>runtime_fraction: { default_value: { numerator: 25 } }</code>.<br>\nThe field <code>default_value</code> should contain an object of the type <a href=\"https://www.envoyproxy.io/docs/envoy/latest/api-v2/type/percent.proto#envoy-api-msg-type-fractionalpercent\">FractionalPercent</a>, in which you can specify the <code>numerator</code> and the <code>denominator</code>. The final portion of the mirrored traffic will be calculated as <code>numerator / denominator</code>. The default value of <code>denominator</code> is <code>100</code>, so if you only specify the <code>numerator</code>, it'll be interpreted as a percentage. If you want to configure fractional percentages, then you need to configure <code>denominator</code> too.<br>\nFor example this value specifies 0.03% (0.0003): <code>runtime_fraction: { default_value: { numerator: 3, denominator: 10000 } }</code>.</li>\n</ul>\n<p>And the way Envoy will behave is that it'll send back the response to the original client which comes from the main cluster, and the mirrored requests happens in a fire &amp; forget fashion, so the response is discarded. This is exactly what we want if we'd like to test a different version of our service without affecting any end users.</p>\n<h1 id=\"thefullsetup\">The full setup</h1>\n<p>Let's say we want to configure the following setup, in which 25% of the traffic is shadow mirrored to a different upstream, accessible at <code>myservice-test.mycompany.com</code>.</p>\n<p><img src=\"/content/images/2019/04/envoy-mirror-setup.png\" alt=\"An Envoy setup with mirroring 25% of the traffic.\"></p>\n<p>We can set this up with the following YAML configuration.</p>\n<pre><code class=\"language-yaml\">admin:\n  access_log_path: /tmp/admin_access.log\n  address:\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\n\nstatic_resources:\n  listeners:\n  - name: listener_0\n    address:\n      socket_address: { address: 0.0.0.0, port_value: 80 }\n    filter_chains:\n    - filters:\n      - name: envoy.http_connection_manager\n        config:\n          stat_prefix: ingress_http\n          route_config:\n            name: local_route\n            virtual_hosts:\n            - name: local_service\n              domains: [&quot;*&quot;]\n              routes:\n              - match: { prefix: &quot;/&quot; }\n                route:\n                  host_rewrite: myservice-backend.mycompany.com\n                  cluster: myservice_cluster\n                  request_mirror_policy:\n                    cluster: myservice_test_cluster\n                    runtime_fraction: { default_value: { numerator: 25 } }\n          http_filters:\n          - name: envoy.router\n  clusters:\n  - name: myservice_cluster\n    type: LOGICAL_DNS\n    hosts: [{ socket_address: { address: myservice-backend.mycompany.com, port_value: 80 }}]\n  - name: myservice_test_cluster\n    type: LOGICAL_DNS\n    hosts: [{ socket_address: { address: myservice-test.mycompany.com, port_value: 80 }}]\n</code></pre>\n<p>One important thing to note is that the service discovery for the test cluster will happen with the DNS address <code>myservice-test.mycompany.com</code>, that's not going to be used as the host name (the value of the <code>Host</code> header) in the mirrored requests.<br>\nWith a request mirror policy, the host name is always what it would normally be, plus the <code>-shadow</code> suffix. Thus in this example it will become <code>myservice-backend.company.com-shadow</code>. This is something that we have to prepare for if we're mirroring traffic to a component which takes the value of the <code>Host</code> header into account. (At some point <a href=\"https://groups.google.com/forum/#!topic/envoy-users/sroT9ecsCDY\">I asked</a> if there is any way to customize this, but at the moment it is not possible.)</p>\n<p>I hope this will be a useful introduction about request mirroring with Envoy, I found that this feature can be extremely valuable for safely testing new implementations, or carrying out comparative benchmarks.</p>\n",
        "comment_id": "5cafa655a26b5f5ce4856bf4",
        "plaintext": "Introduction\nShadow mirroring (also called shadow feeding, or just shadowing) is a technique\nwhen at some point in our infrastructure we duplicate the outgoing traffic to an\nadditional destination, but we send the responses to the actual client coming\nfrom the main destination.\nThis is mainly used to be able to test a service with real production traffic\nwithout affecting the end clients in any way. It's particularly useful when we\nare rewriting an existing service, and we want to verify if the new version can\nprocess a real variety of incoming requests in an identical way, or when we want\nto do a comparative benchmark between two version of the same service. And it\ncan also be used to do some extra, out of bounds processing of our requests,\nwhich can be done asynchronously, for example collect some additional\nstatistics, or do some extended logging.\n\nThere are some existing technologies implemented specifically for this purpose.\nThe one I have used before is GoReplay [https://github.com/buger/goreplay],\nwhich is a small network monitoring tool capable ofamong other thingsshadowing\nthe incoming traffic to a second source.\n\nAnd some of the general-purpose reverse proxies have this capability as well. In\nthis post we'll go over how this can be implemented with Envoy\n[https://www.envoyproxy.io/]. First we'll see an introduction to what a basic\nproxy or load balancing setup with Envoy looks like, and then we'll focus on the\nrequest mirroring capabilities which allows us to implement shadowing.\n\nA simple Envoy setup\nThe following diagram illustrates a very simple setup where we use an Envoy\ninstance as a \"frontend\" or reverse proxy to route traffic to a single backend\nservice. (Envoy can be used in much more complicated routing setups, but this is\nenough for illustrating request mirroring.)\n\n\n\nAn Envoy instance can be controlled by providing a static YAML configuration on\nstartup. (Which is just one of the ways of configuring Envoy, it also supports\nhaving a dynamic configuration provider, but we won't discuss that in this\npost.)\nThe above simple setup can be achieved with the following YAML config.\n\nadmin:\n  access_log_path: /tmp/admin_access.log\n  address:\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\n\nstatic_resources:\n  listeners:\n  - name: listener_0\n    address:\n      socket_address: { address: 0.0.0.0, port_value: 80 }\n    filter_chains:\n    - filters:\n      - name: envoy.http_connection_manager\n        config:\n          stat_prefix: ingress_http\n          route_config:\n            name: local_route\n            virtual_hosts:\n            - name: local_service\n              domains: [\"*\"]\n              routes:\n              - match: { prefix: \"/\" }\n                route:\n                  host_rewrite: myservice-backend.mycompany.com\n                  cluster: myservice_cluster\n          http_filters:\n          - name: envoy.router\n  clusters:\n  - name: myservice_cluster\n    type: LOGICAL_DNS\n    hosts: [{ socket_address: { address: myservice-backend.mycompany.com, port_value: 80 }}]\n\n\nThis specifies one single route, which matches all incoming requests, and\nproxies them to the myservice_cluster  cluster.\n\n            - name: local_service\n              domains: [\"*\"]\n              routes:\n              - match: { prefix: \"/\" }\n                route:\n                  host_rewrite: myservice-backend.mycompany.com\n                  cluster: myservice_cluster\n\n\nAnd the single cluster we configured will route all requests to the address \nmyservice-backend.mycompany.com.\n\n  - name: myservice_cluster\n    type: LOGICAL_DNS\n    hosts: [{ socket_address: { address: myservice-backend.mycompany.com, port_value: 80 }}]\n\n\nMirroring support in Envoy\nThere are two specific capabilities of Envoy which are particularly interesting\nto us.\n\nThe first is Traffic Shifting/Splitting\n[https://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/traffic_splitting]\n. This allows us to route certain percentages of the traffic to different hosts.\nAlthough this is useful in many situations, it won't help us with shadowing,\nsince this just routes the traffic in various directions, but doesn't duplicate\nit.\n\nThe feature which will allow us to implement shadowing is the request mirror\npolicy. With this we can specify an extra backend for every route, and configure\nwhat percentage of the traffic should be mirrored to it.\n\nThis can be enabled by adding the request_mirror_policy  field to our route, and\nconfigure the following two keys.\n\n * cluster: The name of the cluster to which we're mirroring.\n * runtime_fraction: The portion of the traffic which should be mirrored. You\n   can either configure this with a dynamic runtime value, or add a static value\n   to your YAML config with this syntax: runtime_fraction: { default_value: {\n   numerator: 25 } }.\n   The field default_value  should contain an object of the type \n   FractionalPercent, in which you can specify the numerator  and the \n   denominator. The final portion of the mirrored traffic will be calculated as \n   numerator / denominator. The default value of denominator  is 100, so if you\n   only specify the numerator, it'll be interpreted as a percentage. If you want\n   to configure fractional percentages, then you need to configure denominator \n   too.\n   For example this value specifies 0.03% (0.0003): runtime_fraction: {\n   default_value: { numerator: 3, denominator: 10000 } }.\n\nAnd the way Envoy will behave is that it'll send back the response to the\noriginal client which comes from the main cluster, and the mirrored requests\nhappens in a fire & forget fashion, so the response is discarded. This is\nexactly what we want if we'd like to test a different version of our service\nwithout affecting any end users.\n\nThe full setup\nLet's say we want to configure the following setup, in which 25% of the traffic\nis shadow mirrored to a different upstream, accessible at \nmyservice-test.mycompany.com.\n\n\n\nWe can set this up with the following YAML configuration.\n\nadmin:\n  access_log_path: /tmp/admin_access.log\n  address:\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\n\nstatic_resources:\n  listeners:\n  - name: listener_0\n    address:\n      socket_address: { address: 0.0.0.0, port_value: 80 }\n    filter_chains:\n    - filters:\n      - name: envoy.http_connection_manager\n        config:\n          stat_prefix: ingress_http\n          route_config:\n            name: local_route\n            virtual_hosts:\n            - name: local_service\n              domains: [\"*\"]\n              routes:\n              - match: { prefix: \"/\" }\n                route:\n                  host_rewrite: myservice-backend.mycompany.com\n                  cluster: myservice_cluster\n                  request_mirror_policy:\n                    cluster: myservice_test_cluster\n                    runtime_fraction: { default_value: { numerator: 25 } }\n          http_filters:\n          - name: envoy.router\n  clusters:\n  - name: myservice_cluster\n    type: LOGICAL_DNS\n    hosts: [{ socket_address: { address: myservice-backend.mycompany.com, port_value: 80 }}]\n  - name: myservice_test_cluster\n    type: LOGICAL_DNS\n    hosts: [{ socket_address: { address: myservice-test.mycompany.com, port_value: 80 }}]\n\n\nOne important thing to note is that the service discovery for the test cluster\nwill happen with the DNS address myservice-test.mycompany.com, that's not going\nto be used as the host name (the value of the Host  header) in the mirrored\nrequests.\nWith a request mirror policy, the host name is always what it would normally be,\nplus the -shadow  suffix. Thus in this example it will become \nmyservice-backend.company.com-shadow. This is something that we have to prepare\nfor if we're mirroring traffic to a component which takes the value of the Host \nheader into account. (At some point I asked  if there is any way to customize\nthis, but at the moment it is not possible.)\n\nI hope this will be a useful introduction about request mirroring with Envoy, I\nfound that this feature can be extremely valuable for safely testing new\nimplementations, or carrying out comparative benchmarks.",
        "feature_image": null,
        "featured": 0,
        "page": 0,
        "status": "published",
        "locale": null,
        "visibility": "public",
        "meta_title": "Shadow mirroring with Envoy",
        "meta_description": "An overview of implementing shadow mirroring with Envoy, to safely test a service with real production traffic without affecting the end clients.",
        "author_id": "1",
        "created_at": "2019-04-11 20:40:53",
        "created_by": "1",
        "updated_at": "2019-04-11 20:44:50",
        "updated_by": "1",
        "published_at": "2019-04-11 20:44:50",
        "published_by": "1",
        "custom_excerpt": "An overview of implementing shadow mirroring with Envoy, to safely test a different version of a service with real production traffic without affecting the end clients.",
        "codeinjection_head": "",
        "codeinjection_foot": "",
        "og_image": null,
        "og_title": null,
        "og_description": null,
        "twitter_image": null,
        "twitter_title": null,
        "twitter_description": null,
        "custom_template": null
      }]
    }
  }]
}